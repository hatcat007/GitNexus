### Quickstart: Basic memvid-sdk Usage

Source: https://docs.memvid.com/installation/python

A Python quickstart example demonstrating how to use the memvid-sdk to open a memory file, put a new memory, check statistics, verify the file integrity, and seal it. It covers basic import and usage patterns.

```python
from memvid_sdk import LockedError, Memvid, use

mv = use("basic", "notes.mv2")
mv.put(text="hello world", kind="text/plain")
print(mv.stats())

report = Memvid.verify("notes.mv2", deep=True)
print(report["overall_status"])
mv.seal()
```

--------------------------------

### Pinecone Setup and Initialization (Python)

Source: https://docs.memvid.com/comparisons/vector-databases

Guides through the setup process for Pinecone, a vector database. Involves signing up, creating a project, obtaining an API key, installing the client library, initializing the connection, creating an index, and waiting for it to become ready. Data must be embedded before insertion.

```python
# 1. Sign up at pinecone.io
# 2. Create a project
# 3. Get API key
# 4. Install SDK
pip install pinecone-client

# 5. Initialize
import pinecone
pinecone.init(api_key="your-api-key", environment="us-west1-gcp")

# 6. Create index (wait for provisioning...)
pinecone.create_index("my-index", dimension=1536, metric="cosine")

# 7. Wait for index to be ready
import time
while not pinecone.describe_index("my-index").status["ready"]:
    time.sleep(1)

# 8. Connect to index
index = pinecone.Index("my-index")

# 9. Now you need to embed your data before inserting...
```

--------------------------------

### Quick Start

Source: https://docs.memvid.com/python-sdk/overview

A concise guide to get started with the Memvid Python SDK, demonstrating memory creation, data ingestion, searching, and querying.

```APIDOC
## Quick Start

```python
from memvid_sdk import create

# Create a new memory
mem = create('knowledge.mv2')

# Enable lexical search (BM25)
mem.enable_lex()

# Add documents
mem.put(
    title='Meeting Notes',
    label='notes',
    metadata={'source': 'slack'},
    text='Alice mentioned she works at Anthropic...'
)

# Search
results = mem.find('who works at AI companies?', k=5)
print(results['hits'])

# Ask questions
answer = mem.ask('What does Alice do?')
print(answer['text'])

# Seal when done
mem.seal()
```

**Note:** No embeddings required for BM25 lexical search. Embeddings can be added later for semantic search.
```

--------------------------------

### Install Memvid Node.js SDK

Source: https://docs.memvid.com/quickstart/five-minute-guide

Installs the Memvid SDK for Node.js applications.

```bash
npm install @memvid/sdk
```

--------------------------------

### Installation and Quick Start (Python)

Source: https://docs.memvid.com/frameworks/langchain

Install the necessary packages and get started with Memvid in Python for LangChain applications.

```APIDOC
## Installation (Python)

```bash
pip install memvid-sdk langchain langchain-openai
```

## Quick Start (Python)

```python
from memvid_sdk import use

# Open with LangChain adapter
mem = use('langchain', 'knowledge.mv2')

# Access LangChain tools
tools = mem.tools  # Returns LangChain StructuredTool objects
```
```

--------------------------------

### Install Memvid Python SDK

Source: https://docs.memvid.com/quickstart/five-minute-guide

Installs the Memvid SDK for Python applications using pip.

```bash
pip install memvid-sdk
```

--------------------------------

### Memvid CLI Quick Start Commands

Source: https://docs.memvid.com/installation/cli

Demonstrates basic Memvid CLI operations including creating a memory, adding documents, performing immediate searches, and asking questions. These commands help you quickly start building AI memories.

```bash
# Create your first memory
memvid create my-memory.mv2

# Add some documents
memvid put my-memory.mv2 --input ./documents/

# Search immediately (no embedding wait!)
memvid find my-memory.mv2 --query "your search term"

# Ask questions
memvid ask my-memory.mv2 --question "What is this about?"
```

--------------------------------

### Installation and Quick Start (Node.js)

Source: https://docs.memvid.com/frameworks/langchain

Install the necessary packages and get started with Memvid in Node.js for LangChain applications.

```APIDOC
## Installation (Node.js)

```bash
npm install @memvid/sdk @langchain/core @langchain/openai @langchain/langgraph zod
```

## Quick Start (Node.js)

```typescript
import { use } from '@memvid/sdk';

// Open with LangChain adapter
const mem = await use('langchain', 'knowledge.mv2');

// Access LangChain tools (compatible with createReactAgent)
const tools = mem.tools;  // Array of tool() objects
```
```

--------------------------------

### ChromaDB Setup and Initialization (Python)

Source: https://docs.memvid.com/comparisons/vector-databases

Details the setup for ChromaDB, an open-source vector database. Covers installation, client initialization, creating a collection, and adding documents. Note that ChromaDB embeds documents automatically, which can take significant time for large datasets.

```python
# 1. Install
pip install chromadb

# 2. Initialize
import chromadb
client = chromadb.Client()

# 3. Create collection
collection = client.create_collection("my-collection")

# 4. Add documents (ChromaDB embeds automatically, but still takes time)
collection.add(
    documents=["doc1", "doc2", "doc3"],
    ids=["id1", "id2", "id3"]
)
# This step embeds all documents - can take minutes for large datasets
```

--------------------------------

### Memvid SDK: LangChain Integration Example

Source: https://docs.memvid.com/introduction/welcome

Demonstrates how to integrate Memvid with LangChain in Python. It shows how to open a memory file with the LangChain adapter, create a retriever, and set up a RetrievalQA chain for question answering.

```python
from memvid_sdk import use
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# Open with LangChain adapter
mem = use('langchain', 'my-knowledge.mv2', read_only=True)
retriever = mem.as_retriever(k=5)

# Create QA chain
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4o"),
    retriever=retriever
)

result = qa.run("What are the main concepts?")
print(result)
```

--------------------------------

### Memvid CLI Installation and Usage

Source: https://docs.memvid.com/introduction/welcome

Provides commands to install the Memvid CLI globally using npm and then create a new memory file, ingest documents with vector compression, and perform a basic search.

```bash
npm install -g memvid-cli

# Create a new memory file (1 GB capacity by default)
memvid create my-knowledge.mv2

# Ingest documents with vector compression
memvid put my-knowledge.mv2 --input ./documents/ --vector-compression

# Search your knowledge
memvid find my-knowledge.mv2 --query "your search query"
```

--------------------------------

### Install Memvid CLI

Source: https://docs.memvid.com/quickstart/five-minute-guide

Installs the Memvid command-line interface globally. Requires Node.js version 14 or higher and works on macOS, Linux, and Windows.

```bash
npm install -g memvid-cli
```

--------------------------------

### Install @memvid/sdk using npm or pnpm

Source: https://docs.memvid.com/installation/node

This snippet shows how to install the Node.js SDK using package managers npm or pnpm. Ensure you have Node.js version 18 or later installed. Prebuilt binaries are available for common platforms; otherwise, you may need to build from source.

```bash
npm install @memvid/sdk
# or: pnpm add @memvid/sdk
```

--------------------------------

### Install and Verify Memvid CLI

Source: https://docs.memvid.com/installation/cli

Installs the Memvid CLI globally using npm and then verifies the installation by checking the CLI version. Ensure Node.js and npm are installed and configured correctly.

```bash
npm install -g memvid-cli

memvid --version
```

--------------------------------

### Install Memvid CLI

Source: https://docs.memvid.com/quickstart/cli-to-dashboard

Installs the Memvid command-line interface using package managers like Homebrew for macOS, a script for Linux, Winget for Windows, or Cargo for Rust projects. After installation, verify the CLI is operational by checking its version.

```bash
brew install memvid/tap/memvid
```

```bash
curl -sSL https://get.memvid.com | sh
```

```powershell
winget install memvid
```

```bash
cargo install memvid-cli
```

```bash
memvid --version
```

--------------------------------

### Node.js SDK Quick Start: Create, Open, and Use Memory

Source: https://docs.memvid.com/installation/node

Demonstrates the basic usage of the Node.js SDK to create a new memory file, add data, commit changes, open an existing memory file for read-only access, perform searches, and integrate with framework adapters like Langchain. This code requires Node.js 18+.

```typescript
import { create, open, use } from "@memvid/sdk";

// Create new memory
const mem = await create("notes.mv2");
await mem.put({
  title: "Hello Memvid",
  label: "demo",
  text: "Hello from Node.js",
  enableEmbedding: true,
});
await mem.seal(); // commits writes

// Open existing memory (read-only)
const ro = await open("notes.mv2", "basic", { readOnly: true });
const results = await ro.find("hello", { k: 5, mode: "auto" });
console.log(results.hits.map((h: any) => h.title));

// Framework adapters (tools/functions)
const lc = await use("langchain", "notes.mv2", { readOnly: true });
console.log(Object.keys(lc.tools ?? {}));
```

--------------------------------

### Memvid SDK: Python Example

Source: https://docs.memvid.com/introduction/welcome

Illustrates the usage of the Memvid SDK in Python for opening a memory file, conducting searches, and performing AI-powered question answering. Includes a reminder to close the memory instance when finished.

```python
from memvid_sdk import use

# Open your memory
mem = use('basic', 'my-knowledge.mv2', read_only=True)

# Search
results = mem.find('machine learning', k=10)
for hit in results.get('hits', []):
    print(f"{hit['score']:.2f}: {hit['title']}")

# Ask questions with AI synthesis
answer = mem.ask('What are the key concepts?')
print(answer.get('answer'))

# Always close when done
mem.close()
```

--------------------------------

### Install Memvid CLI and Create Memory

Source: https://docs.memvid.com/introduction/the-memvid-approach

Provides instructions for installing the Memvid command-line interface globally using npm and then creating a new Memvid memory file. This is the initial setup step for using Memvid locally.

```bash
# Install
npm install -g memvid-cli

# Create memory
memvid create my-memory.mv2
```

--------------------------------

### Initial Memvid Setup Steps

Source: https://docs.memvid.com/cli/tickets-and-capacity

Guides through the initial setup process for a new Memvid memory, including creating a memory file, syncing it with the dashboard, and verifying the binding.

```bash
# 1. Create memory file
memvid create project.mv2

# 2. Sync with dashboard (binds and applies ticket)
MEMVID_API_KEY=mv_live_xxx memvid tickets sync project.mv2 --memory-id mem_abc123

# 3. Verify binding
memvid binding project.mv2
```

--------------------------------

### Install memvid-sdk Python Package

Source: https://docs.memvid.com/installation/python

Install the memvid-sdk package using pip. Optional adapters for integrations with libraries like LangChain and OpenAI can be installed using package extras.

```bash
pip install memvid-sdk
# optional adapters
pip install "memvid-sdk[langchain]" "memvid-sdk[openai]"
```

--------------------------------

### Memvid Python SDK Quick Start

Source: https://docs.memvid.com/python-sdk/overview

A quick start guide to using the Memvid Python SDK. Demonstrates creating a memory, enabling lexical search, adding documents, performing searches, asking questions, and sealing the memory.

```python
from memvid_sdk import create

# Create a new memory
mem = create('knowledge.mv2')

# Enable lexical search (BM25)
mem.enable_lex()

# Add documents (no embeddings needed!)
mem.put(
    title='Meeting Notes',
    label='notes',
    metadata={'source': 'slack'},
    text='Alice mentioned she works at Anthropic...'
)

# Search works immediately
results = mem.find('who works at AI companies?', k=5)
print(results['hits'])

# Ask questions
answer = mem.ask('What does Alice do?')
print(answer['text'])

# Seal when done
mem.seal()
```

--------------------------------

### Memvid SDK: Node.js Example

Source: https://docs.memvid.com/introduction/welcome

Shows how to use the Memvid SDK in Node.js to open a memory file, perform semantic search, and ask questions. It demonstrates opening a memory in read-only mode and iterating through search results.

```typescript
import { use } from '@memvid/sdk';

// Open your memory
const mem = await use('basic', 'my-knowledge.mv2', { readOnly: true });

// Search
const results = await mem.find('machine learning', { k: 10 });
results.hits.forEach(hit => {
  console.log(`${hit.score.toFixed(2)}: ${hit.title}`);
});

// Ask questions
const answer = await mem.ask('What are the key concepts?');
console.log(answer.answer);
```

--------------------------------

### MemVid Model Comparison Example

Source: https://docs.memvid.com/concepts/time-travel-replay

Shows how to use MemVid to compare different LLM models. The example involves starting a session, performing an 'ask' command with a specific model (e.g., GPT-4o), and ending the session. This allows for subsequent analysis or replaying with different models.

```bash
# Ask with GPT-4o
memvid session start knowledge.mv2 --name "Model Comparison"
memvid ask knowledge.mv2 --question "Summarize the key findings" --use-model openai:gpt-4o
memvid session end knowledge.mv2
```

--------------------------------

### Session Management: Start, Checkpoint, End, List, Replay, Delete

Source: https://docs.memvid.com/python-sdk/overview

Provides a comprehensive guide to managing user sessions, including starting a session with a name, performing operations, adding checkpoints, ending the session to get a summary, listing all existing sessions, replaying a session with modified parameters, and deleting a session.

```python
# Start recording
session_id = mem.session_start('qa-test')

# Perform operations
mem.find('test query')
mem.ask('What happened?')

# Add checkpoint
mem.session_checkpoint()

# End session
summary = mem.session_end()

# List sessions
sessions = mem.session_list()

# Replay session with different params
replay = mem.session_replay(session_id, top_k=10, adaptive=True)
print(replay['match_rate'])

# Delete session
mem.session_delete(session_id)
```

--------------------------------

### Memvid Node.js SDK Quick Start Example

Source: https://docs.memvid.com/sdks/node

A basic example demonstrating how to create a new memory file, add documents, perform searches, ask questions using an AI model, and close the memory file. This requires Node.js 18+.

```typescript
import { create, open } from '@memvid/sdk';

// Create a new memory file
const mem = await create('knowledge.mv2');

// Add documents
await mem.put({
  title: 'Meeting Notes',
  text: 'Alice mentioned she works at Anthropic...',
  enableEmbedding: true
});

// Search
const results = await mem.find('who works at AI companies?');
console.log(results.hits);

// Ask questions with AI
const answer = await mem.ask('What does Alice do?', {
  model: 'gpt-4o-mini',
  modelApiKey: process.env.OPENAI_API_KEY
});
console.log(answer.text);

// Close when done
await mem.close();
```

--------------------------------

### Python SDK: Interact with Memvid Memory Databases

Source: https://docs.memvid.com/quickstart/cli-to-dashboard

This Python code demonstrates how to use the memvid SDK to interact with memory databases. It shows how to install the SDK, open a database in read-only mode, perform searches using keywords, and ask natural language questions, specifying an OpenAI model for processing.

```bash
pip install memvid-sdk
```

```python
from memvid_sdk import use

# Open read-only (for queries)
mem = use('basic', 'docs.mv2', read_only=True)

# Search
results = mem.find('authentication', k=5)
for hit in results['hits']:
    print(f"{hit['score']:.2f}: {hit['title']}")

# Ask questions
answer = mem.ask('How do I configure OAuth?', model='openai:gpt-4o')
print(answer['answer'])
```

--------------------------------

### Quick Start: Use Memvid with LangChain Adapter (Python)

Source: https://docs.memvid.com/frameworks/langchain

Provides a Python example for initializing Memvid with the 'langchain' adapter. It illustrates how to load Memvid data and access the tools it exposes for LangChain integration.

```python
from memvid_sdk import use

# Open with LangChain adapter
mem = use('langchain', 'knowledge.mv2')

# Access LangChain tools
tools = mem.tools  # Returns LangChain StructuredTool objects
```

--------------------------------

### Run Memvid CLI without Global Installation

Source: https://docs.memvid.com/installation/cli

Executes Memvid CLI commands using npx, allowing you to run the tool without a global npm installation. This is useful for testing or when global installations are restricted.

```bash
npx memvid-cli --help
npx memvid-cli create test.mv2
```

--------------------------------

### Quick Start with Haystack Adapter

Source: https://docs.memvid.com/frameworks/haystack

Initializes Memvid with the Haystack adapter and accesses its retriever component. This is a basic setup to get started with Memvid's search capabilities within Haystack.

```python
from memvid_sdk import use

# Open with Haystack adapter
mem = use('haystack', 'knowledge.mv2')

# Access Haystack components
retriever = mem.as_retriever(top_k=5)
```

--------------------------------

### Node.js SDK: Interact with Memvid Memory Databases

Source: https://docs.memvid.com/quickstart/cli-to-dashboard

This Node.js code illustrates how to use the memvid SDK for interacting with memory databases. It covers installing the SDK via npm, opening a database in read-only mode, executing searches with a specified number of results (k), and posing natural language questions, including specifying different OpenAI models.

```bash
npm install @memvid/sdk
```

```typescript
import { use } from '@memvid/sdk';

// Open read-only
const mem = await use('basic', 'docs.mv2', { readOnly: true });

// Search
const results = await mem.find('authentication', { k: 5 });
results.hits.forEach(hit => {
  console.log(`${hit.score.toFixed(2)}: ${hit.title}`);
});

// Ask questions
const answer = await mem.ask('How do I configure OAuth?', {
  model: 'openai:gpt-4o-mini'
});
console.log(answer.answer);
```

--------------------------------

### Troubleshoot 'Command Not Found'

Source: https://docs.memvid.com/installation/cli

Provides commands to check your npm global bin directory and add it to your PATH environment variable if the Memvid CLI commands are not recognized. This ensures your system can locate the installed CLI.

```bash
# Check npm global bin location
npm root -g

# Add to PATH if needed
export PATH="$PATH:$(npm root -g)/../bin"
```

--------------------------------

### Create and Ingest Data with Memvid CLI

Source: https://docs.memvid.com/quickstart/five-minute-guide

Demonstrates creating a new memory file and adding documents to it using the Memvid CLI. Data can be ingested directly from standard input and is immediately searchable.

```bash
# Create a new memory
memvid create knowledge.mv2

# Add documents (no embeddings needed!)
echo "Alice works at Anthropic as a Senior Engineer in San Francisco." | \
  memvid put knowledge.mv2 --title "Team Info"

echo "Bob joined OpenAI last month as a Research Scientist." | \
  memvid put knowledge.mv2 --title "New Hires"

echo "Project Alpha has a budget of $500k and is led by Alice." | \
  memvid put knowledge.mv2 --title "Projects"
```

--------------------------------

### Agent Setup and Chat Initiation (Python)

Source: https://docs.memvid.com/frameworks/autogen

Initializes a UserProxyAgent, an AssistantAgent for research with tool access, and another AssistantAgent for writing. It then sets up a GroupChat and GroupChatManager to orchestrate a conversation between these agents, starting with a specific research and writing task.

```python
from autogen import UserProxyAgent, AssistantAgent, GroupChat, GroupChatManager

# Assume search_tool is defined elsewhere and imported
# from your_module import search_tool 

# Mock search_tool for demonstration purposes
class MockSearchTool:
    def __init__(self):
        self.schema = {"name": "search", "parameters": {"query": "string"}}
        self.name = "search"
        self.func = self.search_func

    def search_func(self, query: str):
        print(f"Searching for: {query}")
        return f"Mock results for {query}"

search_tool = MockSearchTool()


user_proxy = UserProxyAgent(name="user", human_input_mode="NEVER")

researcher = AssistantAgent(
    name="researcher",
    llm_config={
        "model": "gpt-4o",
        "functions": [search_tool.schema]
    },
    system_message="You research topics using the knowledge base."
)
researcher.register_function(function_map={search_tool.name: search_tool.func})

writer = AssistantAgent(
    name="writer",
    llm_config={"model": "gpt-4o"},
    system_message="You write summaries based on research findings."
)

group_chat = GroupChat(
    agents=[user_proxy, researcher, writer],
    messages=[],
    max_round=10
)

manager = GroupChatManager(groupchat=group_chat, llm_config={"model": "gpt-4o"})

user_proxy.initiate_chat(
    manager,
    message="Research deployment best practices and write a summary"
)
```

--------------------------------

### Ask Questions with Memvid CLI

Source: https://docs.memvid.com/quickstart/five-minute-guide

Demonstrates using LLM-powered Q&A with Memvid CLI. Requires an OpenAI API key to be set as an environment variable.

```bash
# Ask with LLM synthesis (requires OPENAI_API_KEY)
export OPENAI_API_KEY=sk-...
memvid ask knowledge.mv2 --question "What is Alice's role?" --use-model openai
```

--------------------------------

### Update Memvid CLI

Source: https://docs.memvid.com/installation/cli

Updates the globally installed Memvid CLI to the latest version using npm. Regular updates ensure you have the latest features and bug fixes.

```bash
npm update -g memvid-cli
```

--------------------------------

### Install Memvid Framework Dependencies

Source: https://docs.memvid.com/errors/troubleshooting

These bash commands show how to install necessary dependencies for different frameworks (LangChain, LlamaIndex, CrewAI) to ensure proper integration with Memvid. Make sure to install the correct package for your chosen framework.

```bash
# For LangChain
pip install langchain langchain-openai

# For LlamaIndex
pip install llama-index

# For CrewAI
pip install crewai
```

--------------------------------

### Create and Ingest Data with Memvid Python SDK

Source: https://docs.memvid.com/quickstart/five-minute-guide

Demonstrates creating a new memory and ingesting documents using the Memvid Python SDK. Documents require a title, label, metadata, and text.

```python
from memvid_sdk import create

mem = create('knowledge.mv2')
mem.enable_lex()

# Add documents (no embeddings needed!)
mem.put(
    title='Team Info',
    label='team',
    metadata={},
    text='Alice works at Anthropic as a Senior Engineer in San Francisco.'
)

mem.put(
    title='New Hires',
    label='team',
    metadata={},
    text='Bob joined OpenAI last month as a Research Scientist.'
)

mem.put(
    title='Projects',
    label='project',
    metadata={},
    text='Project Alpha has a budget of $500k and is led by Alice.'
)
```

--------------------------------

### OpenAI CLIP Provider Setup and Usage (Python)

Source: https://docs.memvid.com/concepts/visual-embeddings

Shows how to set up and use OpenAI's CLIP embedding models with the Memvid SDK. It includes setting the API key via an environment variable and provides examples of initializing the provider using a factory function with default or specific models, or direct instantiation.

--------------------------------

### Use Case Examples

Source: https://docs.memvid.com/concepts/time-travel-replay

Practical examples demonstrating how to use Memvid for various scenarios.

```APIDOC
## Use Case Examples

Memvid can be applied to several practical scenarios, including debugging, compliance audits, and model comparisons.

### 1. Debug Missing Results

This example shows how to record a failing scenario and replay it with adaptive retrieval to identify issues.

```bash
# Record the failing scenario
memvid session start knowledge.mv2 --name "Missing Results Debug"
memvid ask knowledge.mv2 --question "What did Databricks purchase?" --use-model openai
memvid session end knowledge.mv2

# Replay with adaptive retrieval
memvid session replay knowledge.mv2 --session <id> --adaptive --verbose
# Reveals: Document existed at rank 12, adaptive found it
```

### 2. Compliance Audit Trail

Use Memvid to create an immutable record of decisions for compliance purposes.

```bash
# Record all decisions for audit
memvid session start knowledge.mv2 --name "Compliance Review 2024-12"
memvid ask knowledge.mv2 --question "Is this transaction fraudulent?" --use-model openai
memvid session end knowledge.mv2

# Later: Replay with frozen context to verify decision
memvid session replay knowledge.mv2 --session <id> --audit
# Shows exact frames and answer - reproducible for auditors
```

### 3. Model Comparison

Compare the performance and output of different LLM models.

```bash
# Ask with GPT-4o
memvid session start knowledge.mv2 --name "Model Comparison"
memvid ask knowledge.mv2 --question "Summarize the key findings" --use-model openai:gpt-4o
memvid session end knowledge.mv2
```
```

--------------------------------

### Memvid Google ADK Integration

Source: https://docs.memvid.com/frameworks/google-adk

This section details the integration of Memvid with Google ADK, including installation, setup, and usage examples for building Gemini-powered agents.

```APIDOC
## Installation

```bash
npm install @memvid/sdk @google/generative-ai
```

## Quick Start

```typescript
import { use } from '@memvid/sdk';

// Open with Google ADK adapter
const mem = await use('google-adk', 'knowledge.mv2');

// Access ADK function declarations
const tools = mem.tools;       // FunctionDeclaration[] for Gemini API
const executors = mem.functions; // Function executors by name
```

## Available Functions

The Google ADK adapter provides three function declarations:

| Function      | Description                                           |
| ------------- | ----------------------------------------------------- |
| `memvid_put`  | Store documents in memory with title, label, and text |
| `memvid_find` | Search for relevant documents by query                |
| `memvid_ask`  | Ask questions with RAG-style answer synthesis         |

## Basic Usage with Gemini

```typescript
import { use } from '@memvid/sdk';
import { GoogleGenerativeAI } from '@google/generative-ai';

// Initialize Memvid with Google ADK adapter
const mem = await use('google-adk', 'knowledge.mv2');

// Get function declarations and executors
const tools = mem.tools as any[];
const executors = mem.functions as Record<string, (args: any) => Promise<string>>;

// Create Gemini client
const geminiKey = process.env.GEMINI_API_KEY ?? process.env.GOOGLE_API_KEY;
if (!geminiKey) throw new Error("Set GEMINI_API_KEY (or legacy GOOGLE_API_KEY)");
const genAI = new GoogleGenerativeAI(geminiKey);

// Create model with Memvid tools
const model = genAI.getGenerativeModel({
  model: 'gemini-2.0-flash',
  tools: [{ functionDeclarations: tools }],
});

// Start a chat
const chat = model.startChat();
const result = await chat.sendMessage('Search for authentication information');

// Handle function calls
const response = result.response;
const parts = response.candidates?.[0]?.content?.parts || [];

for (const part of parts) {
  if (part.functionCall) {
    const { name, args } = part.functionCall;
    console.log(`Function call: ${name}`);

    // Execute the function
    if (executors[name]) {
      const funcResult = await executors[name](args as Record<string, unknown>);
      console.log(`Result: ${funcResult}`);

      // Send result back to model
      const followUp = await chat.sendMessage([{ functionResponse: { name, response: { result: funcResult } } }]);
      console.log(`Model response: ${followUp.response.text()}`);
    }
  } else if (part.text) {
    console.log(`Response: ${part.text}`);
  }
}
```

## Direct Tool Execution

Use the function executors directly without Gemini:

```typescript
import { use } from '@memvid/sdk';

const mem = await use('google-adk', 'knowledge.mv2', { mode: 'create' });
const executors = mem.functions as Record<string, (args: any) => Promise<string>>;

// Store documents
const putResult = await executors.memvid_put({
  title: 'API Documentation',
  label: 'docs',
  text: 'Authentication uses JWT tokens with refresh capability.',
});
console.log(putResult);
// Output: Document stored with frame_id: 2

// Search documents
const findResult = await executors.memvid_find({
  query: 'authentication',
  top_k: 5,
});
console.log(findResult);
// Output: Found 1 results:
// 1. [API Documentation] (score: 2.34): Authentication uses JWT tokens...

// Ask questions
const askResult = await executors.memvid_ask({
  question: 'How does authentication work?',
  mode: 'auto',
});
console.log(askResult);
// Output: Answer: Authentication uses JWT tokens with refresh capability.
// Sources: API Documentation
```

## Complete Agentic Example

```typescript
import { use } from '@memvid/sdk';
import { GoogleGenerativeAI } from '@google/generative-ai';

async function runGeminiAgent() {
  // Initialize
  const mem = await use('google-adk', 'knowledge.mv2');
  const tools = mem.tools as any[];
  const executors = mem.functions as Record<string, (args: any) => Promise<string>>;

  // Store some knowledge first
  await executors.memvid_put({
    title: 'Gemini Overview',
    label: 'google-ai',
    text: 'Gemini is Google\'s most capable AI model family.',
  });
  // ... rest of the agent logic
}
```
```

--------------------------------

### Quick Start: Use Memvid with LangChain Adapter (Node.js)

Source: https://docs.memvid.com/frameworks/langchain

Demonstrates the basic setup for using Memvid within a LangChain application using the Node.js adapter. It shows how to initialize Memvid with the 'langchain' adapter and access its compatible tools.

```typescript
import { use } from '@memvid/sdk';

// Open with LangChain adapter
const mem = await use('langchain', 'knowledge.mv2');

// Access LangChain tools (compatible with createReactAgent)
const tools = mem.tools;  // Array of tool() objects
```

--------------------------------

### Ask Questions with Memvid Python SDK

Source: https://docs.memvid.com/quickstart/five-minute-guide

Illustrates using LLM-powered Q&A with the Memvid Python SDK. Requires an OpenAI API key to be set in the environment variables.

```python
# Ask with LLM synthesis
answer = mem.ask(
    "What is Alice's role?",
    model='gpt-4o-mini',
    api_key=os.environ['OPENAI_API_KEY']
)
print(answer['text'])
# "Alice is a Senior Engineer at Anthropic in San Francisco."
```

--------------------------------

### Install and Use NER Models for Logic-Mesh

Source: https://docs.memvid.com/installation/models

Guides on installing NER models and enabling Logic-Mesh during data ingestion with `memvid put`. Also demonstrates how to use `memvid follow` with extracted entities.

```bash
# Install NER model
memvid models install --ner distilbert-ner

# Enable Logic-Mesh during ingestion and traversal
memvid put graph.mv2 --input docs/ --logic-mesh
memvid follow graph.mv2 traverse --start "Microsoft" --hops 2
```

--------------------------------

### Memvid CLI: Create, Ingest, and Search Memory Databases

Source: https://docs.memvid.com/quickstart/cli-to-dashboard

This snippet demonstrates the core command-line interface (CLI) operations for the memvid tool. It covers creating a new memory file, ingesting data with vector compression, checking database statistics, performing keyword searches, asking natural language questions, and verifying data integrity. The `--vector-compression` flag is used for efficient data storage.

```bash
memvid create docs.mv2

memvid put docs.mv2 \
  --input ./docs/ \
  --vector-compression \
  --track "documentation"

memvid put docs.mv2 \
  --input ./api-reference/ \
  --vector-compression \
  --track "api"

memvid stats docs.mv2

memvid find docs.mv2 --query "authentication setup" --mode auto

export OPENAI_API_KEY=your-key
memvid ask docs.mv2 \
  --question "How do I configure OAuth?" \
  --use-model openai

memvid verify docs.mv2 --deep
```

--------------------------------

### Verify memvid Installation

Source: https://docs.memvid.com/sdks/cli

Checks if the memvid CLI has been installed correctly by displaying its current version. This command should be run after installation to confirm successful setup.

```bash
memvid --version
```

--------------------------------

### Installation

Source: https://docs.memvid.com/cli

Instructions for installing the Memvid CLI using npm and verifying the installation.

```APIDOC
## Installation

```bash
# npm (recommended)
npm install -g memvid-cli

# Verify installation
memvid --version
```

<Info>
  See [Installation Guide](/installation/cli) for platform-specific instructions and troubleshooting.
</Info>
```

--------------------------------

### Get CLI Version

Source: https://docs.memvid.com/cli/maintenance-and-tickets

Displays the current version of the Memvid CLI. This is a basic command for verifying the installed version.

```bash
memvid version
```

--------------------------------

### Memvid CLI: Best Practice - Starting with Defaults

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Recommends starting with Memvid's default settings, which typically use the 'combined' adaptive strategy, for most search scenarios.

```bash
memvid find memory.mv2 --query "your search"
```

--------------------------------

### Ask Questions with Memvid

Source: https://docs.memvid.com/quickstart/cli-to-dashboard

Utilize AI models to ask questions and receive synthesized answers based on the content of your Memvid memory. Supports local models like 'tinyllama' and external services like OpenAI and Anthropic by providing API keys.

```bash
# Using local model (tinyllama)
memvid ask my-knowledge.mv2 --question "What is Memvid and how does it work?"
```

```bash
# Using OpenAI (requires API key)
export OPENAI_API_KEY=your-key
memvid ask my-knowledge.mv2 --question "What is Memvid?" --use-model openai
```

```bash
# Using Anthropic
export ANTHROPIC_API_KEY=your-key
memvid ask my-knowledge.mv2 --question "What is Memvid?" --use-model claude
```

--------------------------------

### Installing and Using Memvid CLI

Source: https://docs.memvid.com/comparisons/vector-databases

Provides bash commands to install the Memvid command-line interface (CLI) using npm, create a new Memvid file, add content to it, and perform a search query. This demonstrates the immediate usability of Memvid without API keys or embedding delays.

```bash
# Install (10 seconds)
npm install -g memvid-cli

# Create and search (10 more seconds)
memvid create test.mv2
echo "The quick brown fox jumps over the lazy dog" | memvid put test.mv2
memvid find test.mv2 --query "quick fox"

# That's it. No API keys. No embedding wait. Just search.
```

--------------------------------

### Start Ollama Server

Source: https://docs.memvid.com/concepts/local-models

Starts the Ollama local inference server. The 'serve' command runs it in the foreground, showing logs. For macOS, 'brew services start ollama' can run it as a background service.

```bash
# Start in foreground (see logs)
ollama serve

# Or run as background service (macOS)
brew services start ollama
```

--------------------------------

### Select Synthesis Model for RAG with Memvid

Source: https://docs.memvid.com/concepts/performance-tuning

Choose the appropriate synthesis model for RAG tasks based on speed, quality, and cost requirements. Examples show how to specify models for both local and API-based synthesis.

```bash
# Fast local synthesis
memvid ask memory.mv2 --question "..." --use-model tinyllama

# Fast API synthesis
memvid ask memory.mv2 --question "..." --use-model groq
```

--------------------------------

### Manage Documents and Query via CLI

Source: https://docs.memvid.com/examples/document-qa

This example provides command-line interface commands for managing a document Q&A system. It covers creating a new document store, ingesting all documents from a directory, and asking questions against the store.

```bash
# Create and ingest
memvid create documents.mv2
memvid put documents.mv2 --input ./docs/

# Ask questions
memvid ask documents.mv2 --question "What is the refund policy?"
```

--------------------------------

### Python Function Calling Example

Source: https://docs.memvid.com/frameworks/openai

This Python example demonstrates how to use the Memvid SDK with OpenAI's function calling. It shows the process of initializing the Memvid adapter, creating an OpenAI client, setting up messages, making a chat completion request with function definitions, and handling the tool calls to execute Memvid functions.

```APIDOC
## Function Calling Example (Python)

### Description
This example shows how to integrate Memvid functions into an OpenAI chat completion flow using Python. It covers initialization, making a function call, and handling the results.

### Method
N/A (Client-side execution)

### Endpoint
N/A (Client-side execution)

### Request Example
```python
from memvid_sdk import use
import openai
import json

# Get Memvid functions
mem = use('openai', 'knowledge.mv2')
functions = mem.functions

# Create completion with function calling
client = openai.OpenAI()

messages = [
    {"role": "system", "content": "You are a helpful assistant with access to a knowledge base."},
    {"role": "user", "content": "Search for information about authentication"}
]

response = client.chat.completions.create(
    model="gpt-4o",
    messages=messages,
    tools=[{"type": "function", "function": f} for f in functions],
    tool_choice="auto"
)

# Handle function calls
message = response.choices[0].message
if message.tool_calls:
    for tool_call in message.tool_calls:
        func_name = tool_call.function.name
        func_args = json.loads(tool_call.function.arguments)

        # Execute the function
        if func_name == "memvid_find":
            result = mem.find(func_args["query"], k=func_args.get("top_k", 5))
        elif func_name == "memvid_put":
            result = mem.put(
                title=func_args["title"],
                label=func_args["label"],
                text=func_args["text"]
            )
        elif func_name == "memvid_ask":
            result = mem.ask(func_args["question"], mode=func_args.get("mode", "auto"))

        print(f"Function {func_name} result: {result}")
```

### Response Example
```json
{
  "function_name": "memvid_find",
  "result": [
    {
      "title": "Authentication Basics",
      "label": "security",
      "text": "Authentication is the process of verifying the identity of a user or device."
    }
  ]
}
```
```

--------------------------------

### Ask Questions with Memvid Node.js SDK

Source: https://docs.memvid.com/quickstart/five-minute-guide

Illustrates how to use LLM-powered Q&A with the Memvid Node.js SDK. Requires an OpenAI API key passed to the SDK.

```typescript
// Ask with LLM synthesis
const answer = await mem.ask("What is Alice's role?", {
  model: 'gpt-4o-mini',
  modelApiKey: process.env.OPENAI_API_KEY
});
console.log(answer.text);
// "Alice is a Senior Engineer at Anthropic in San Francisco."
```

--------------------------------

### Search Data with Memvid CLI

Source: https://docs.memvid.com/quickstart/five-minute-guide

Shows how to perform a lexical search on a Memvid memory using the CLI. This search is available immediately after data ingestion.

```bash
# Search works immediately (BM25 lexical search)
memvid find knowledge.mv2 --query "who works at AI companies"
```

--------------------------------

### Installation

Source: https://docs.memvid.com/python-sdk/overview

Instructions on how to install the Memvid Python SDK using pip. Requires Python 3.8+.

```APIDOC
## Installation

```bash
pip install memvid-sdk
```

**Requirements:** Python 3.8+, macOS/Linux/Windows. Native bindings included.
```

--------------------------------

### Create and Manage Memvid Memory

Source: https://docs.memvid.com/quickstart/cli-to-dashboard

Create a new Memvid memory file (`.mv2`) and add various types of content, including text, files, and directories, with options for metadata like tracks and tags. Vector compression can be enabled during document addition for optimized storage.

```bash
memvid create my-knowledge.mv2
memvid stats my-knowledge.mv2
```

```bash
echo "Memvid is a portable AI memory system. It stores embeddings, indices, and data in a single .mv2 file." | memvid put my-knowledge.mv2 --input - --title "What is Memvid"
```

```bash
# Add a single file with vector compression
memvid put my-knowledge.mv2 --input document.pdf --vector-compression

# Add all files in a directory
memvid put my-knowledge.mv2 --input ./documents/ --vector-compression
```

```bash
memvid put my-knowledge.mv2 \
  --input notes.md \
  --track "notes" \
  --tag "category=meeting" \
  --vector-compression
```

--------------------------------

### Get Help and Version Information

Source: https://docs.memvid.com/troubleshooting/cli

Retrieves help information for memvid commands and displays version details. This includes general help, command-specific help, and the memvid version.

```bash
# General help
memvid --help

# Command-specific help
memvid create --help
memvid put --help
memvid find --help
memvid doctor --help

# Version info
memvid version
```

--------------------------------

### Create and Ingest Data with Memvid Node.js SDK

Source: https://docs.memvid.com/quickstart/five-minute-guide

Shows how to create a new memory and ingest documents using the Memvid Node.js SDK. Documents are added with titles and labels.

```typescript
import { create } from '@memvid/sdk';

const mem = await create('knowledge.mv2');

// Add documents (no embeddings needed!)
await mem.put({
  title: 'Team Info',
  label: 'team',
  text: 'Alice works at Anthropic as a Senior Engineer in San Francisco.'
});

await mem.put({
  title: 'New Hires',
  label: 'team',
  text: 'Bob joined OpenAI last month as a Research Scientist.'
});

await mem.put({
  title: 'Projects',
  label: 'project',
  text: 'Project Alpha has a budget of $500k and is led by Alice.'
});
```

--------------------------------

### Memvid CLI: Create, Put, and Find Knowledge

Source: https://docs.memvid.com/comparisons/vector-databases

This snippet shows basic Memvid CLI commands for creating a knowledge base file, adding content to it, and searching for specific information. It requires the memvid-cli to be installed globally.

```bash
npm install -g memvid-cli
memvid create knowledge.mv2
echo "Your document content" | memvid put knowledge.mv2
memvid find knowledge.mv2 --query "document"
```

--------------------------------

### Install Memvid Node.js SDK

Source: https://docs.memvid.com/sdks/node

Instructions for installing the Memvid Node.js SDK using npm, pnpm, or yarn. Ensure you have Node.js version 18 or later installed.

```bash
npm install @memvid/sdk
# or
pnpm add @memvid/sdk
# or
yarn add @memvid/sdk
```

--------------------------------

### Python SDK Operations

Source: https://docs.memvid.com/quickstart/sdk-recipes

Code examples for Python demonstrating how to open/create a Memvid instance, ingest data, perform searches, manage capacity, and handle errors.

```APIDOC
## Python SDK Operations

### Open or Create Instance

```python
from memvid_sdk import use

mem = use("basic", "notes.mv2", mode="auto", enable_lex=True, enable_vec=False)
```

### Put (Required Signature)

Ingests a single item with required arguments.

```python
mem.put(
    "Title",           # title (required)
    "label",           # label (required)
    {},                # metadata dict (required, can be {})
    text="Content here",
    uri="mv2://docs/intro",
    tags=["api"],
    labels=["public"],
    kind="markdown",
    track="docs",
    enable_embedding=True,
    vector_compression=True,
)
```

### Batch Ingest

Ingests multiple items efficiently.

```python
docs = [
    {"title": "Doc 1", "label": "kb", "text": "First document"},
    {"title": "Doc 2", "label": "kb", "text": "Second document"},
]
mem.put_many(docs, opts={"compression_level": 3, "enable_embedding": True})
```

### Search and Ask

Performs hybrid search and asks questions based on the ingested data.

```python
hits = mem.find("hybrid search", k=5, scope="mv2://docs/")["hits"]
answer = mem.ask("What is hybrid search?", k=6, mode="auto", mask_pii=True)
```

### Timeline and Stats

Retrieves timeline information and statistics about the data.

```python
mem.timeline(limit=10, as_of_frame=200)
stats = mem.stats()
```

### Tickets and Capacity

Manages capacity by applying tickets and checking current capacity.

```python
mem.apply_ticket("base64-ticket")           # expands capacity
print(mem.get_capacity())
```

### Tables

Processes PDF tables and lists/retrieves table data.

```python
result = mem.put_pdf_tables("invoice.pdf", embed_rows=True)
tables = mem.list_tables()
table = mem.get_table(tables[0]["table_id"], format="dict")
```

### Error Handling

Demonstrates how to catch specific Memvid SDK errors.

```python
from memvid_sdk import CapacityExceededError, LockedError
try:
    mem.put("Big", "kb", {}, file="huge.bin", enable_embedding=False)
except CapacityExceededError:
    print("Upgrade or apply a ticket")
except LockedError:
    print("Another writer holds the lock")
```
```

--------------------------------

### Install and Use CLIP Models for Visual Search

Source: https://docs.memvid.com/installation/models

Instructions for installing CLIP models and using them with `memvid put` for image ingestion and `memvid find` for visual content search. Various CLIP model variants are available for installation.

```bash
# Install CLIP models
memvid models install --clip mobileclip-s2
memvid models install --clip mobileclip-s2-fp16
memvid models install --clip siglip-base

# Use CLIP during ingestion and search
memvid put photos.mv2 --input ./images --clip
memvid find photos.mv2 --query "sunset over ocean" --mode clip
```

--------------------------------

### Node.js Function Calling Example

Source: https://docs.memvid.com/frameworks/openai

This Node.js example demonstrates how to use the Memvid SDK with OpenAI's function calling. It shows the process of initializing the Memvid adapter, creating an OpenAI client, setting up messages, making a chat completion request with function definitions, and handling the tool calls to execute Memvid functions.

```APIDOC
## Function Calling Example (Node.js)

### Description
This example shows how to integrate Memvid functions into an OpenAI chat completion flow using Node.js. It covers initialization, making a function call, and handling the results.

### Method
N/A (Client-side execution)

### Endpoint
N/A (Client-side execution)

### Request Example
```typescript
import { use } from '@memvid/sdk';
import OpenAI from 'openai';

// Get Memvid functions
const mem = await use('openai', 'knowledge.mv2');
const functions = mem.functions;

// Create OpenAI client
const client = new OpenAI();

const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
  { role: 'system', content: 'You are a helpful assistant with access to a knowledge base.' },
  { role: 'user', content: 'Search for information about authentication' },
];

// Create completion with function calling
const response = await client.chat.completions.create({
  model: 'gpt-4o',
  messages,
  tools: functions.map((f: any) => ({ type: 'function' as const, function: f })),
  tool_choice: 'auto',
});

// Handle function calls
const message = response.choices[0].message;
if (message.tool_calls) {
  for (const toolCall of message.tool_calls) {
    const funcName = toolCall.function.name;
    const funcArgs = JSON.parse(toolCall.function.arguments);

    let result: any;
    if (funcName === 'memvid_find') {
      result = await mem.find(funcArgs.query, { k: funcArgs.top_k || 5 });
    } else if (funcName === 'memvid_put') {
      result = await mem.put({
        title: funcArgs.title,
        label: funcArgs.label,
        text: funcArgs.text,
      });
    } else if (funcName === 'memvid_ask') {
      result = await mem.ask(funcArgs.question, { mode: funcArgs.mode || 'auto' });
    }

    console.log(`Function ${funcName} result:`, result);
  }
}
```

### Response Example
```json
{
  "function_name": "memvid_find",
  "result": [
    {
      "title": "Authentication Basics",
      "label": "security",
      "text": "Authentication is the process of verifying the identity of a user or device."
    }
  ]
}
```
```

--------------------------------

### Extract Facts with Memvid CLI

Source: https://docs.memvid.com/quickstart/five-minute-guide

Illustrates extracting structured facts from a Memvid memory using a 'rules' engine and then querying the state of an entity.

```bash
# Extract structured facts
memvid enrich knowledge.mv2 --engine rules

# Query entity state (O(1) lookup)
memvid state knowledge.mv2 "Alice"
```

--------------------------------

### Memvid CLI Hybrid Search Modes

Source: https://docs.memvid.com/introduction/welcome

Demonstrates how to perform hybrid, lexical, and semantic searches using the Memvid CLI. The 'auto' mode is recommended for combining lexical and semantic search capabilities.

```bash
# Hybrid search (recommended)
memvid find knowledge.mv2 --query "user authentication" --mode auto

# Lexical search - exact keyword matching
memvid find knowledge.mv2 --query "authentication" --mode lex

# Semantic search - conceptual understanding
memvid find knowledge.mv2 --query "how do users log in" --mode sem
```

--------------------------------

### Use Correct Memvid Framework Adapter Names

Source: https://docs.memvid.com/errors/troubleshooting

This Python snippet illustrates the correct and incorrect ways to specify framework adapter names when initializing Memvid. Using the correct name is crucial for the adapter to function properly. Examples for 'langchain', 'llamaindex', and 'crewai' are provided.

```python
# Correct
mem = use('langchain', 'knowledge.mv2')
mem = use('llamaindex', 'knowledge.mv2')
mem = use('crewai', 'knowledge.mv2')

# Incorrect
mem = use('lang-chain', 'knowledge.mv2')  # Wrong!
```

--------------------------------

### Framework Adapters

Source: https://docs.memvid.com/sdks/node

Code examples demonstrating how to integrate Memvid SDK with various popular frameworks.

```APIDOC
## Integrating with Frameworks

### Vercel AI SDK
```typescript
import { use } from '@memvid/sdk';

const vercel = await use('vercel-ai', 'knowledge.mv2');
const tools = vercel.tools;
```

### LangChain.js
```typescript
import { use } from '@memvid/sdk';

const langchain = await use('langchain', 'knowledge.mv2');
const retriever = langchain.asRetriever();
```

### LlamaIndex
```typescript
import { use } from '@memvid/sdk';

const llamaindex = await use('llamaindex', 'knowledge.mv2');
```

### OpenAI Function Calling
```typescript
import { use } from '@memvid/sdk';

const openai = await use('openai', 'knowledge.mv2');
const functions = openai.functions;
```

### Google ADK
```typescript
import { use } from '@memvid/sdk';

const googleAdk = await use('google-adk', 'knowledge.mv2');
```

### Semantic Kernel
```typescript
import { use } from '@memvid/sdk';

const sk = await use('semantic-kernel', 'knowledge.mv2');
```
```

--------------------------------

### Node.js SDK Operations

Source: https://docs.memvid.com/quickstart/sdk-recipes

Code examples for Node.js demonstrating how to open/create a Memvid instance, ingest data, perform searches, manage capacity, and handle errors.

```APIDOC
## Node.js SDK Operations

### Open or Create Instance

```typescript
import { use } from "@memvid/sdk";

const mem = await use("basic", "notes.mv2", { mode: "auto", enableLex: true });
```

### Put (Required Fields)

Ingests a single item with required fields.

```typescript
await mem.put({
  title: "Title",          // required
  label: "label",          // required
  text: "Content here",
  uri: "mv2://docs/intro",
  tags: ["api"],
  labels: ["public"],
  kind: "markdown",
  track: "docs",
  enableEmbedding: true,
  vectorCompression: true,
});
```

### Batch Ingest

Ingests multiple items efficiently.

```typescript
const docs = [
  { title: "Doc 1", label: "kb", text: "First document" },
  { title: "Doc 2", label: "kb", text: "Second document" },
];
await mem.putMany(docs, { compressionLevel: 3 });
```

### Search and Ask

Performs hybrid search and asks questions based on the ingested data.

```typescript
const results = await mem.find("hybrid search", { k: 5, scope: "mv2://docs/" });
const answer = await mem.ask("What is hybrid search?", { k: 6, mode: "auto", maskPii: true });
```

### Timeline and Stats

Retrieves timeline information and statistics about the data.

```typescript
await mem.timeline({ limit: 10, asOfFrame: 200 });
const stats = await mem.stats();
```

### Tickets and Capacity

Manages capacity by applying tickets and checking current capacity.

```typescript
await mem.applyTicket("base64-ticket");
console.log(await mem.getCapacity());
```

### Tables

Processes PDF tables and lists/retrieves table data.

```typescript
await mem.putPdfTables("invoice.pdf", true);
const tables = await mem.listTables();
const table = await mem.getTable(tables[0].tableId, "dict");
```

### Error Handling

Demonstrates how to catch specific Memvid SDK errors.

```typescript
import { CapacityExceededError, LockedError } from "@memvid/sdk";

try {
  await mem.put({ title: "Big", label: "kb", file: "huge.bin", enableEmbedding: false });
} catch (err) {
  if (err instanceof CapacityExceededError) console.log("Capacity exceeded");
  else if (err instanceof LockedError) console.log("File is locked");
}
```
```

--------------------------------

### Memvid SDK: Python for Knowledge Management

Source: https://docs.memvid.com/comparisons/vector-databases

This Python code demonstrates how to use the memvid_sdk to create a knowledge base, put data with metadata, and perform searches. It requires the memvid_sdk package to be installed.

```python
from memvid_sdk import create

mem = create('knowledge.mv2')
mem.put(title="Doc", label="docs", metadata={}, text="Your document content")
results = mem.find("document", k=5)
```

--------------------------------

### Search Data with Memvid Python SDK

Source: https://docs.memvid.com/quickstart/five-minute-guide

Shows how to perform a lexical search on a Memvid memory using the Python SDK. The results are a dictionary containing 'hits' with document titles.

```python
# Search works immediately (BM25 lexical search)
results = mem.find('who works at AI companies', k=5)
print([h['title'] for h in results['hits']])
# ['Team Info', 'New Hires']
```

--------------------------------

### Semantic Search in Node.js and Python

Source: https://docs.memvid.com/examples

This example demonstrates how to perform semantic search using Memvid. It covers adding knowledge to a memory store and then querying it semantically. The code is provided for both Node.js and Python, requiring the respective Memvid SDKs.

```typescript
import { use } from '@memvid/sdk';

// Open or create memory
const mem = await use('basic', 'knowledge.mv2', { mode: 'auto' });

// Add some knowledge
await mem.put({ title: 'AI Basics', label: 'docs', text: 'Artificial intelligence is...' });
await mem.put({ title: 'ML Guide', label: 'docs', text: 'Machine learning enables...' });

// Search semantically
const results = await mem.find('how do computers learn?', { k: 5 });
results.hits.forEach(hit => console.log(`${hit.title}: ${hit.snippet}`));
```

```python
from memvid_sdk import use

# Open or create memory
mem = use('basic', 'knowledge.mv2', mode='auto')

# Add some knowledge
mem.put({"title": "AI Basics", "label": "docs", "text": "Artificial intelligence is..."})
mem.put({"title": "ML Guide", "label": "docs", "text": "Machine learning enables..."})

# Search semantically
results = mem.find("how do computers learn?", k=5)
for hit in results.hits:
    print(f"{hit.title}: {hit.snippet}")
```

--------------------------------

### Install Memvid SDK and AI Packages

Source: https://docs.memvid.com/frameworks/vercel-ai

Installs the necessary Memvid SDK, the core AI SDK, and the OpenAI adapter for Vercel AI.

```bash
npm install @memvid/sdk ai @ai-sdk/openai
```

--------------------------------

### Install Ollama on Linux

Source: https://docs.memvid.com/concepts/local-models

Installs Ollama on Linux by downloading and executing the official installation script. This method is suitable for most Linux distributions.

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

--------------------------------

### Get Memory Statistics and List Entities

Source: https://docs.memvid.com/python-sdk/overview

Provides examples for retrieving statistics about memories, such as the count of entities and cards, and for listing all available entities within the system.

```python
stats = mem.memories_stats()
print(stats['entityCount'], stats['cardCount'])

entities = mem.memory_entities()
```

--------------------------------

### LangChain RAG Pipeline in Python

Source: https://docs.memvid.com/examples

This Python example shows how to integrate Memvid with LangChain to create a Retrieval Augmented Generation (RAG) pipeline. It initializes Memvid with the LangChain adapter, sets up a retrieval chain using `RetrievalQA`, and demonstrates how to invoke it with a question. Dependencies include `memvid_sdk`, `langchain_openai`, and `langchain`.

```python
from memvid_sdk import use
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# Initialize with LangChain adapter
mem = use('langchain', 'knowledge.mv2')

# Create RAG chain
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4o"),
    retriever=mem.as_retriever(k=5),
    return_source_documents=True
)

# Ask questions
result = qa.invoke("What are the main features?")
print(result['result'])
```

--------------------------------

### Memvid CLI: Basic Usage and Installation

Source: https://docs.memvid.com/concepts/no-vec-mode

This snippet demonstrates the fundamental commands for installing the Memvid CLI, creating a new memory file, adding documents, performing immediate searches, and asking questions using the local TinyLlama model. It highlights the out-of-the-box functionality without requiring API keys or embedding wait times.

```bash
# Install
npm install -g memvid-cli

# Create memory (no embeddings by default)
memvid create my-memory.mv2

# Add documents
memvid put my-memory.mv2 --input ./documents/

# Search immediately
memvid find my-memory.mv2 --query "your search"

# Ask questions (uses local TinyLlama)
memvid ask my-memory.mv2 --question "What is this about?"
```

--------------------------------

### Document Q&A with RAG using Memvid SDK (Python)

Source: https://docs.memvid.com/resources/examples

Builds a Retrieval-Augmented Generation (RAG) system for document querying using the Memvid SDK. This example shows ingesting PDF documents and then querying them to get answers. It depends on the memvid_sdk and Path from pathlib.

```python
from memvid_sdk import create
from pathlib import Path

# Ingest documents
mem = create('docs.mv2')
for pdf in Path('documents/').glob('*.pdf'):
    mem.put({
        'title': pdf.stem,
        'file': str(pdf),
        'label': 'document'
    })
mem.seal()

# Query with RAG
answer = mem.ask(
    'What are the key findings?',
    model='openai:gpt-4o',
    k=10
)
print(answer['answer'])
```

--------------------------------

### Check Framework Version Compatibility for Memvid

Source: https://docs.memvid.com/errors/troubleshooting

This bash command helps you check the installed version of a framework, such as LangChain, to ensure it's compatible with Memvid. Compatibility issues can arise from outdated or unsupported framework versions.

```bash
pip show langchain  # Check version
```

--------------------------------

### Bash: Install Sentence Transformers for HuggingFace

Source: https://docs.memvid.com/concepts/embedding-models

Command to install the necessary Python package for using HuggingFace sentence-transformer models locally.

```bash
pip install sentence-transformers

```

--------------------------------

### Generate Strong Password Example

Source: https://docs.memvid.com/concepts/encryption

Provides examples of generating strong passwords using different methods. It includes generating random characters with `openssl`, using passphrases, and an example of a randomly generated string suitable for a password manager.

```bash
# Random characters (use password manager)
Kj#9mP$2xL@nQ5vR

# Passphrase (easier to remember)
correct-horse-battery-staple-42

# Generated (most secure)
openssl rand -base64 24
#  "X7kP2mN9qR3sT6vY8wA1bC4d"
```

--------------------------------

### Ask Command Examples with memvid CLI

Source: https://docs.memvid.com/cli/search-and-ask

Demonstrates various ways to use the 'memvid ask' command to query memory files. This includes specifying models, adjusting retrieval parameters like top-k, filtering by date ranges, and obtaining context-only responses. It highlights options for masking PII and using different LLM providers.

```bash
# Ask with local Ollama model (recommended)
memvid ask knowledge.mv2 \
  --question "How do I configure authentication?" \
  --use-model "ollama:qwen2.5:1.5b"

# Ask with more context
memvid ask knowledge.mv2 \
  --question "Explain the architecture in detail" \
  --top-k 15 \
  --use-model "ollama:qwen2.5:3b"

# Get just the context without LLM synthesis
memvid ask knowledge.mv2 \
  --question "What is the architecture?" \
  --context-only

# Mask sensitive data before sending to cloud LLM
memvid ask knowledge.mv2 \
  --question "What are the contact details?" \
  --use-model openai \
  --mask-pii

# Filter to specific date range
memvid ask knowledge.mv2 \
  --question "What happened in Q4?" \
  --start "2024-10-01" \
  --end "2024-12-31" \
  --use-model "ollama:qwen2.5:1.5b"

# JSON output with Gemini
memvid ask knowledge.mv2 \
  --question "Summarize the API" \
  --use-model "gemini-2.0-flash" \
  --json
```

--------------------------------

### Memvid SDK Installation for Python

Source: https://docs.memvid.com/frameworks/overview

Lists pip commands for installing the Memvid core SDK and various framework integrations in Python, including an option to install all dependencies.

```bash
# Core SDK
pip install memvid-sdk

# Framework dependencies (install what you need)
pip install "memvid-sdk[langchain]"        # LangChain
pip install "memvid-sdk[llamaindex]"       # LlamaIndex
pip install "memvid-sdk[openai]"           # OpenAI
pip install google-genai                   # Google ADK
pip install "memvid-sdk[autogen]"          # AutoGen
pip install "memvid-sdk[crewai]"           # CrewAI
pip install "memvid-sdk[semantic-kernel]"  # Semantic Kernel
pip install "memvid-sdk[haystack]"         # Haystack

# Or install all integrations:
pip install "memvid-sdk[full]"
```

--------------------------------

### Install CLIP and NER Models

Source: https://docs.memvid.com/cli/create-and-put

Installs the CLIP model for visual embeddings and the NER model for entity extraction. These models are automatically enabled by the CLI when installed.

```bash
memvid models install --clip mobileclip-s2
memvid models install --ner distilbert-ner
```

--------------------------------

### Function Calling Example with Memvid (Python)

Source: https://docs.memvid.com/frameworks/openai

Illustrates using Memvid functions with OpenAI's chat completions in Python. This example shows how to set up the OpenAI client, prepare messages, and process the response to execute Memvid's `find`, `put`, and `ask` functions based on the generated tool calls.

```python
from memvid_sdk import use
import openai
import json

# Get Memvid functions
mem = use('openai', 'knowledge.mv2')
functions = mem.functions

# Create completion with function calling
client = openai.OpenAI()

messages = [
    {"role": "system", "content": "You are a helpful assistant with access to a knowledge base."},
    {"role": "user", "content": "Search for information about authentication"}
]

response = client.chat.completions.create(
    model="gpt-4o",
    messages=messages,
    tools=[{"type": "function", "function": f} for f in functions],
    tool_choice="auto"
)

# Handle function calls
message = response.choices[0].message
if message.tool_calls:
    for tool_call in message.tool_calls:
        func_name = tool_call.function.name
        func_args = json.loads(tool_call.function.arguments)

        # Execute the function
        if func_name == "memvid_find":
            result = mem.find(func_args["query"], k=func_args.get("top_k", 5))
        elif func_name == "memvid_put":
            result = mem.put(
                title=func_args["title"],
                label=func_args["label"],
                text=func_args["text"]
            )
        elif func_name == "memvid_ask":
            result = mem.ask(func_args["question"], mode=func_args.get("mode", "auto"))

        print(f"Function {func_name} result: {result}")
```

--------------------------------

### Session Recording and Replay

Source: https://docs.memvid.com/node-sdk/overview

This snippet covers session management, including starting a session with a name, performing operations within the session, adding checkpoints, ending the session to get a summary, listing existing sessions, replaying a session with specified parameters, and deleting a session.

```typescript
const sessionId = await mem.sessionStart('qa-test');

await mem.find('test query');
await mem.ask('What happened?');

await mem.sessionCheckpoint();

const summary = await mem.sessionEnd();

const sessions = await mem.sessionList();

const replay = await mem.sessionReplay(sessionId, 10, true);
console.log(replay.match_rate);

await mem.sessionDelete(sessionId);
```

--------------------------------

### Diagnose Python Import Errors with Bash

Source: https://docs.memvid.com/errors/troubleshooting

This snippet provides bash commands to diagnose Python import errors related to the memvid SDK. It includes checking the installation, Python version, and installed packages.

```bash
pip show memvid-sdk
python --version
pip list | grep memvid
```

--------------------------------

### Install Memvid CLI

Source: https://docs.memvid.com/cli/cheat-sheet

Installs the Memvid CLI globally using npm or from source.

```bash
# npm (recommended)
npm install -g memvid-cli

# From source
cargo install memvid-cli
```

--------------------------------

### Configure Indexes in Memvid

Source: https://docs.memvid.com/concepts/performance-tuning

Control index creation during Memvid setup to optimize storage and search performance. Disable vector or lexical indexes as needed based on the intended use case.

```bash
# No vector index (lexical only)
memvid create code.mv2 --no-vec

# No lexical index (semantic only)
memvid create semantic.mv2 --no-lex
```

--------------------------------

### Run Benchmarks Script

Source: https://docs.memvid.com/introduction/benchmarks

This bash script clones the Memvid benchmark repository, installs dependencies, and runs the benchmarks. It's designed for local execution on user hardware to reproduce the results. Ensure you have Git, Python, and pip installed.

```bash
git clone https://github.com/memvid/memvid
cd memvid/benchmarks/python
pip install -r requirements.txt
python run_benchmarks.py
```

--------------------------------

### Search Data with Memvid Node.js SDK

Source: https://docs.memvid.com/quickstart/five-minute-guide

Demonstrates performing a lexical search on a Memvid memory using the Node.js SDK. The results include the titles of matching documents.

```typescript
// Search works immediately (BM25 lexical search)
const results = await mem.find('who works at AI companies', { k: 5 });
console.log(results.hits.map(h => h.title));
// ['Team Info', 'New Hires']
```

--------------------------------

### Memvid Smart Frames Search Examples (Python)

Source: https://docs.memvid.com/comparisons/vector-databases

Demonstrates various search functionalities within Memvid using Python. Includes exact lexical search, temporal queries based on timelines, entity state retrieval using a knowledge graph, and semantic/hybrid search modes. Requires the 'mem' object to be initialized.

```python
# Exact lexical search (instant, no embeddings needed)
results = mem.find("handleAuthentication", k=5)

# Temporal queries (unique to Memvid)
results = mem.timeline("2024-01-01", "2024-01-31")

# Entity state (knowledge graph)
alice = mem.state("Alice")  # {employer: "Anthropic", role: "Engineer"}

# Semantic search (when you need it)
results = mem.find("cost reduction strategies", mode="vec")

# Hybrid search (best of both)
results = mem.find("budget optimization", mode="auto")
```

--------------------------------

### Start Session Recording with Memvid

Source: https://docs.memvid.com/cli/advanced-commands

Commands to start recording an agent session using `memvid session start`. Sessions can be named or unnamed, and record subsequent operations for debugging. The session must be explicitly ended.

```bash
# Start a named recording session
memvid session start knowledge.mv2 "Debug Session"

# Start an unnamed session
memvid session start knowledge.mv2
```

--------------------------------

### CLI: PDF Ingestion with Table Extraction

Source: https://docs.memvid.com/quickstart/five-minute-guide

These CLI commands demonstrate ingesting a PDF file, specifying a title, and enabling table extraction. It also shows how to list available tables within the ingested PDF and export a specific table to a CSV file.

```bash
memvid put project.mv2 --input report.pdf --title "Q4 Report" --tables
memvid tables list project.mv2
memvid tables export project.mv2 --table-id tbl_001 -o table.csv
```

--------------------------------

### Python: PDF Ingestion and Table Extraction

Source: https://docs.memvid.com/quickstart/five-minute-guide

This Python code demonstrates ingesting a PDF file with associated metadata and enabling the extraction of tables. It also shows how to list all tables found within the PDF.

```python
mem.put(title='Q4 Report', label='report', metadata={}, file='report.pdf')
mem.put_pdf_tables('report.pdf', embed_rows=True)
tables = mem.list_tables()
```

--------------------------------

### Creating Encrypted Backups with Memvid

Source: https://docs.memvid.com/concepts/encryption

This example demonstrates how to create secure, long-term encrypted backups of Memvid data. It includes creating an encrypted backup with a timestamp and a note about securely storing the password separately.

```bash
# Create encrypted backup
memvid lock knowledge.mv2 --out backups/knowledge-$(date +%Y%m%d).mv2e --keep-original

# Store password securely (password manager, vault, etc.)
```

--------------------------------

### Install and Verify Memvid CLI

Source: https://docs.memvid.com/cli/index

Installs the Memvid CLI globally using npm and verifies the installation by checking the CLI version. Assumes npm is installed and configured.

```bash
# npm (recommended)
npm install -g memvid-cli

# Verify installation
memvid --version
```

--------------------------------

### Memvid SDK Installation for Node.js

Source: https://docs.memvid.com/frameworks/overview

Provides commands for installing the Memvid core SDK and optional framework-specific dependencies for Node.js projects using npm.

```bash
# Core SDK
npm install @memvid/sdk

# Framework dependencies (install what you need)
npm install @langchain/core @langchain/openai   # LangChain
npm install llamaindex                          # LlamaIndex
npm install ai @ai-sdk/openai                   # Vercel AI
npm install openai                              # OpenAI
npm install @google/generative-ai              # Google ADK
```

--------------------------------

### Install Memvid SDK and OpenAI Libraries (Node.js)

Source: https://docs.memvid.com/frameworks/autogen

Installs the Memvid SDK and the OpenAI Node.js client, necessary for integrating with OpenAI's API and AutoGen.

```bash
npm install @memvid/sdk openai
```

--------------------------------

### MemVid Ask Command JSON Output Example

Source: https://docs.memvid.com/concepts/time-travel-replay

An example of the JSON output from the 'memvid ask' command, showing the generated answer, detailed token usage (input, output, total), estimated cost in USD, and grounding metrics including score, label, sentence counts, and warning status.

```json
{
  "answer": "The report covers...",
  "usage": {
    "input_tokens": 3648,
    "output_tokens": 36,
    "total_tokens": 3684,
    "cost_usd": 0.000569
  },
  "grounding": {
    "score": 1.0,
    "label": "HIGH",
    "sentence_count": 1,
    "grounded_sentences": 1,
    "has_warning": false
  },
  "cached": false
}
```

--------------------------------

### Install Memvid SDK and Google Generative AI (Python)

Source: https://docs.memvid.com/frameworks/google-adk

This Python command installs the necessary libraries, `memvid-sdk` and `google-genai`, for interacting with the Memvid knowledge base and Google's Gemini models.

```bash
pip install memvid-sdk google-genai
```

--------------------------------

### Research Paper Analysis Example (Python)

Source: https://docs.memvid.com/examples/document-qa

This Python example illustrates using the DocumentQA class for analyzing research papers. It covers initializing the system, ingesting multiple research papers recursively, asking a question that requires synthesizing information across documents, and then printing the summarized answer along with the titles of key source papers. This showcases the system's ability to draw insights from a collection of documents.

```python
qa = DocumentQA("research-papers.mv2")
qa.ingest_folder("./papers/", recursive=True)

# Synthesize across papers
result = qa.ask(
    "What are the main approaches to transformer optimization "
    "mentioned across these papers?"
)

print("Summary:", result["answer"])
print("\nKey papers:")
for source in result["sources"]:
    print(f"  - {source['title']}")

```

--------------------------------

### Install Ollama on macOS

Source: https://docs.memvid.com/concepts/local-models

Installs Ollama using Homebrew on macOS. Ensure Homebrew is installed before running this command.

```bash
brew install ollama
```

--------------------------------

### Complete Agentic Example with Memvid and Gemini (Node.js)

Source: https://docs.memvid.com/frameworks/google-adk

A comprehensive example showcasing the creation of a Gemini agent that utilizes Memvid for persistent memory. It includes initialization, storing initial knowledge, and implies further interaction logic.

```typescript
import { use } from '@memvid/sdk';
import { GoogleGenerativeAI } from '@google/generative-ai';

async function runGeminiAgent() {
  // Initialize
  const mem = await use('google-adk', 'knowledge.mv2');
  const tools = mem.tools as any[];
  const executors = mem.functions as Record<string, (args: any) => Promise<string>>;

  // Store some knowledge first
  await executors.memvid_put({
    title: 'Gemini Overview',
    label: 'google-ai',
    text: 'Gemini is Google\'s most capable AI model family.',
  });

```

--------------------------------

### Quick Start: Initialize Memvid with AutoGen Adapter (Python)

Source: https://docs.memvid.com/frameworks/autogen

Initializes the Memvid SDK using the 'autogen' adapter to connect to a knowledge base. It shows how to access AutoGen-compatible function definitions.

```python
from memvid_sdk import use

# Open with AutoGen adapter
mem = use('autogen', 'knowledge.mv2')

# Access AutoGen tools
tools = mem.tools  # Returns AutoGen function definitions
```

--------------------------------

### Install Dependencies with pip

Source: https://docs.memvid.com/examples/chatbot-memory

Installs the necessary Python packages for the chatbot, including memvid-sdk, openai, and langchain components. These libraries are essential for interacting with the Memvid memory store and OpenAI's language models.

```bash
pip install memvid-sdk openai langchain langchain-openai
```

--------------------------------

### Install Memvid SDK and Semantic Kernel

Source: https://docs.memvid.com/frameworks/semantic-kernel

Installs the necessary Python packages for Memvid SDK and Microsoft Semantic Kernel. This is a prerequisite for using the Memvid adapter with Semantic Kernel.

```bash
pip install memvid-sdk semantic-kernel
```

--------------------------------

### MemVid Session Debugging with Adaptive Retrieval

Source: https://docs.memvid.com/concepts/time-travel-replay

Example of using MemVid for debugging missing results. It involves starting a session, recording a failing 'ask' command, ending the session, and then replaying it with the '--adaptive' and '--verbose' flags to identify why results were missed.

```bash
# Record the failing scenario
memvid session start knowledge.mv2 --name "Missing Results Debug"
memvid ask knowledge.mv2 --question "What did Databricks purchase?" --use-model openai
memvid session end knowledge.mv2

# Replay with adaptive retrieval
memvid session replay knowledge.mv2 --session <id> --adaptive --verbose
# Reveals: Document existed at rank 12, adaptive found it
```

--------------------------------

### Initialize Memvid for Multi-Agent Setup (Python)

Source: https://docs.memvid.com/frameworks/autogen

Initializes the Memvid SDK with the 'autogen' adapter in read-only mode and retrieves the search tool. This setup is a prerequisite for configuring multi-agent conversations using AutoGen and Memvid.

```python
from autogen import GroupChat, GroupChatManager, AssistantAgent, UserProxyAgent
from memvid_sdk import use

# Initialize
mem = use('autogen', 'knowledge.mv2', read_only=True)
search_tool = mem.get_search_tool()
```

--------------------------------

### Extracting Tables from PDFs with Memvid SDK

Source: https://docs.memvid.com/node-sdk/examples

Demonstrates how to extract tables from PDF files using the Memvid SDK's `putPdfTables` function. This example requires the '@memvid/sdk' package. It covers creating a memory file, enabling lexical search, extracting tables, listing them, retrieving table data in different formats (dictionary, CSV), and sealing the file.

```typescript
import { create, Memvid } from '@memvid/sdk';

async function tableExample() {
  const mv: Memvid = await create('invoices.mv2');
  await mv.enableLex();

  // Extract tables from PDF
  const result = await mv.putPdfTables('invoice.pdf', true);
  console.log(`Extracted ${result.tables_count} tables`);

  // List tables
  const tables = await mv.listTables();
  for (const table of tables) {
    console.log(`  ${table.tableId}: ${table.nRows} x ${table.nCols}`);
  }

  // Get table data
  const data = await mv.getTable('pdf_table_1_page1', 'dict') as {
    headers?: string[];
    rows?: unknown[][];
  };
  console.log('Headers:', data.headers);

  // Export to CSV
  const csv = await mv.getTable('pdf_table_1_page1', 'csv');
  console.log(csv);

  await mv.seal();
}
```

--------------------------------

### Deduplication Example in Python

Source: https://docs.memvid.com/concepts/memory-cards

Demonstrates how Memvid handles deduplication of facts. Multiple calls with similar information result in a single, high-confidence card.

```python
# These produce one card, not three
mem.put("John works as an engineer")      # Extracts: job_title = engineer
mem.put("John is an engineer at Acme")    # Same fact, different source
mem.put("John Smith - Engineer")          # Same fact, skipped
```

--------------------------------

### Extract Facts with Memvid Node.js SDK

Source: https://docs.memvid.com/quickstart/five-minute-guide

Shows how to extract structured facts using a 'rules' engine and query entity states with the Memvid Node.js SDK.

```typescript
// Extract structured facts
await mem.enrich('rules');

// Query entity state (O(1) lookup)
const alice = await mem.state('Alice');
console.log(alice.slots);
// { employer: 'Anthropic', role: 'Senior Engineer', location: 'San Francisco' }
```

--------------------------------

### Ingest Documents and Perform Search/Ask Questions

Source: https://docs.memvid.com/introduction/the-memvid-approach

Shows how to add local documents to a Memvid memory file and then perform searches using a hybrid lexical and semantic approach. It also demonstrates asking natural language questions against the memory.

```bash
# Add your documents (uses local embeddings by default)
memvid put my-memory.mv2 --input ./documents/

# Search with hybrid lexical + semantic
memvid find my-memory.mv2 --query "quarterly report"

# Ask questions
memvid ask my-memory.mv2 --question "What were the Q4 results?"
```

--------------------------------

### Multi-Agent Research Team in Python

Source: https://docs.memvid.com/examples

This Python example showcases how to build a multi-agent system for research using Memvid and the AutoGen framework. It sets up a researcher agent with access to Memvid tools for knowledge retrieval and a writer agent for summarization, orchestrated by a user proxy agent. Dependencies include `memvid_sdk`, `autogen`, and `langchain`.

```python
from memvid_sdk import use
from autogen import AssistantAgent, UserProxyAgent

mem = use('autogen', 'research-papers.mv2')

# Create researcher agent with Memvid tools
researcher = AssistantAgent(
    name="researcher",
    llm_config={"tools": mem.tools},
    system_message="You research topics using the knowledge base."
)

# Create writer agent
writer = AssistantAgent(
    name="writer",
    system_message="You write summaries based on research findings."
)

# Create user proxy
user = UserProxyAgent(name="user", human_input_mode="NEVER")

# Start research task
user.initiate_chat(
    researcher,
    message="Research the latest advances in transformer architectures"
)
```

--------------------------------

### Install Memvid CLI with GPU Support (macOS/Linux/Windows)

Source: https://docs.memvid.com/concepts/audio-video

Install the memvid CLI with GPU acceleration for faster transcription. On macOS, use 'cargo install' with the 'metal' feature or Homebrew. On Linux/Windows, use 'cargo install' with the 'cuda' feature, ensuring CUDA toolkit and cuDNN are installed.

```bash
# macOS (Apple Silicon)
cargo install memvid-cli --features metal

# Or via Homebrew (includes Metal)
brew install memvid/tap/memvid

# Linux/Windows (NVIDIA CUDA)
cargo install memvid-cli --features cuda

# Requires CUDA toolkit and cuDNN
```

--------------------------------

### Memvid JSON Output Example

Source: https://docs.memvid.com/cli/tickets-and-capacity

An example of the JSON output format for Memvid plan details, showcasing the structure for limits, features, and billing.

```json
{
  "plan": "developer",
  "status": "active",
  "limits": {
    "memories": {"used": 10, "limit": 25},
    "capacity_bytes": {"used": 2684354560, "limit": 10737418240},
    "api_calls": {"used": 45231, "limit": 100000, "period": "monthly"}
  },
  "features": {
    "semantic_search": true,
    "llm_enrichment": true,
    "table_extraction": true,
    "encryption": true,
    "priority_support": false
  },
  "billing": {
    "next_renewal": "2024-02-01",
    "amount_cents": 2900,
    "currency": "usd"
  }
}
```

--------------------------------

### Quick Start: Memvid Node.js SDK

Source: https://docs.memvid.com/node-sdk/overview

Demonstrates basic usage of the Memvid Node.js SDK, including creating a memory file, adding documents, performing lexical searches, asking questions, and sealing the memory. No embeddings are needed for initial lexical search.

```typescript
import { create } from '@memvid/sdk';

// Create a new memory
const mem = await create('knowledge.mv2');

// Add documents (no embeddings needed!)
await mem.put({
  title: 'Meeting Notes',
  label: 'notes',
  text: 'Alice mentioned she works at Anthropic...'
});

// Search works immediately
const results = await mem.find('who works at AI companies?', { k: 5 });
console.log(results.hits);

// Ask questions
const answer = await mem.ask('What does Alice do?');
console.log(answer.text);

// Seal when done
await mem.seal();
```

--------------------------------

### Troubleshoot Ollama Not Running

Source: https://docs.memvid.com/concepts/local-models

Provides commands to start the Ollama server when it's not running. This addresses the 'Failed to contact LLM provider' error.

```bash
# Start Ollama server
ollama serve

# Or as background service (macOS)
brew services start ollama
```

--------------------------------

### Install Memvid SDK and PyAutoGen (Python)

Source: https://docs.memvid.com/frameworks/autogen

Installs the Memvid SDK and the PyAutoGen library, required for building multi-agent applications with Memvid.

```bash
pip install memvid-sdk pyautogen
```

--------------------------------

### Resolve Node.js Native Binding Errors with Bash

Source: https://docs.memvid.com/errors/troubleshooting

This snippet shows bash commands to resolve Node.js native binding errors by reinstalling with rebuild, checking Node.js version compatibility (18+), and installing necessary build tools.

```bash
rm -rf node_modules package-lock.json
npm install
nvm use 18
npm rebuild
# macOS
xcode-select --install
# Ubuntu
sudo apt install build-essential
# Windows
npm install -g windows-build-tools
```

--------------------------------

### Memvid Hybrid Search Examples

Source: https://docs.memvid.com/cli/search-and-ask

Shows examples of using the hybrid search mode ('auto'), which combines lexical and semantic search for potentially the best overall results. This mode is recommended for general-purpose queries.

```bash
# General queries
memvid find knowledge.mv2 --query "authentication best practices" --mode auto

# Technical with context
memvid find knowledge.mv2 --query "OAuth2 implementation patterns" --mode auto
```

--------------------------------

### Legal Document Analysis Example (Python)

Source: https://docs.memvid.com/examples/document-qa

This Python code snippet demonstrates how to use the DocumentQA class for legal document analysis. It shows initializing the system with a specific memory file, ingesting documents from a folder, asking targeted questions about contract clauses, and searching for specific legal terms within the documents. The output includes answers and relevant document snippets.

```python
qa = DocumentQA("legal-docs.mv2")

# Ingest contracts
qa.ingest_folder("./contracts/")

# Ask specific questions
result = qa.ask("What are the termination clauses in the vendor contracts?")
print(result["answer"])

# Search for specific terms
matches = qa.search("indemnification liability")
for m in matches:
    print(f"{m['title']}: {m['snippet']}")

```

--------------------------------

### Extract Facts with Memvid Python SDK

Source: https://docs.memvid.com/quickstart/five-minute-guide

Demonstrates extracting structured facts using a 'rules' engine and querying entity states with the Memvid Python SDK. Entity states are returned as dictionaries.

```python
# Extract structured facts
mem.enrich(engine='rules')

# Query entity state (O(1) lookup)
alice = mem.state('Alice')
print(alice['slots'])
```

--------------------------------

### Store Correction Example with memvid CLI

Source: https://docs.memvid.com/cli/search-and-ask

Provides an example of how to use the 'memvid correct' command to add a specific statement as a correction to a memory file. It demonstrates the basic usage without additional options.

```bash
# Store a correction
memvid correct knowledge.mv2 "Ben Koenig reported to Chloe Nguyen before 2025"
```

--------------------------------

### Python: Batch Ingestion of Documents

Source: https://docs.memvid.com/quickstart/five-minute-guide

This Python code illustrates batch ingestion of multiple documents into Memvid. Prepare a list of dictionaries, each containing 'title', 'label', and 'text'. The `put_many` method is used for efficient addition of these documents.

```python
docs = [
    {'title': 'Doc 1', 'label': 'kb', 'text': 'Content 1'},
    {'title': 'Doc 2', 'label': 'kb', 'text': 'Content 2'},
    {'title': 'Doc 3', 'label': 'kb', 'text': 'Content 3'}
]

mem.put_many(docs)
```

--------------------------------

### Verify Memvid Memory Integrity

Source: https://docs.memvid.com/quickstart/cli-to-dashboard

Ensure the integrity and self-contained nature of your Memvid memory file. Commands perform basic checksum verification, deep index checks, and confirm that the file does not rely on external data.

```bash
# Basic verification
memvid verify my-knowledge.mv2
```

```bash
# Deep verification
memvid verify my-knowledge.mv2 --deep
```

```bash
# Confirm single-file integrity
memvid verify-single-file my-knowledge.mv2
```

--------------------------------

### Browse and View Memvid Timeline

Source: https://docs.memvid.com/quickstart/cli-to-dashboard

Explore documents within your Memvid memory chronologically using the timeline command. You can limit the number of results, filter by time ranges (Unix timestamps), or view a specific document by its frame ID.

```bash
# Recent documents
memvid timeline my-knowledge.mv2 --limit 10
```

```bash
# Filter by time range
memvid timeline my-knowledge.mv2 --since 1704067200 --until 1706745600
```

```bash
# View a specific document
memvid view my-knowledge.mv2 --frame-id 1
```

--------------------------------

### Memvid CLI Diagnostic Commands for File Health

Source: https://docs.memvid.com/troubleshooting/cli

This snippet details essential diagnostic commands for checking the health and status of Memvid files. It includes commands for getting basic statistics, JSON output for scripting, and performing both basic and deep integrity verifications.

```bash
# Basic stats
memvid stats myfile.mv2

# JSON output for scripting
memvid stats myfile.mv2 --json

# Verify integrity
memvid verify myfile.mv2

# Deep verification
memvid verify myfile.mv2 --deep
```

--------------------------------

### Install Memvid and OpenAI SDK (Python)

Source: https://docs.memvid.com/frameworks/openai

Installs the required Memvid SDK and OpenAI Python packages. This step is necessary before utilizing Memvid with OpenAI's function calling features in Python.

```bash
pip install memvid-sdk openai
```

--------------------------------

### Start Session Recording (CLI & Python)

Source: https://docs.memvid.com/concepts/time-travel-replay

Initiates a new session recording for an agent. This command starts capturing all subsequent operations within the session. The Python SDK provides a programmatic way to achieve the same.

```bash
# CLI
memvid session start knowledge.mv2 --name "Audit Session"
```

```python
# Python SDK
session_id = mem.session_start("Audit Session")
```

--------------------------------

### Enrichment Output Summary Example

Source: https://docs.memvid.com/concepts/memory-cards

This example shows the typical output after a successful enrichment process. It summarizes the engine used, the number of frames processed, memory cards extracted, unique entities identified, and the total time taken.

```text
Enriching memory.mv2 with groq engine...
  Processing: 150 frames
  Extracted: 423 memory cards
  Entities: 87 unique
  Time: 12.3s

Top entities:
  - John Smith (34 facts)
  - Acme Corp (28 facts)
  - Project Alpha (19 facts)
```

--------------------------------

### Install memvid-cli via Cargo

Source: https://docs.memvid.com/sdks/cli

Installs the memvid command-line interface using Cargo, the Rust package manager. This method requires Rust and Cargo to be installed.

```bash
cargo install memvid-cli
```

--------------------------------

### Search, Ask, and Timeline Queries with Memvid SDK

Source: https://docs.memvid.com/node-sdk/examples

Demonstrates how to use the Memvid SDK to open an existing memory file, perform deterministic searches with options, ask questions using LLM synthesis, and query timeline data. It requires the '@memvid/sdk' package and an OpenAI API key for LLM interactions. The function takes memory file path and search/ask parameters as input and outputs search results, answers, and timeline entries.

```typescript
import { use, create, Memvid } from '@memvid/sdk';

async function searchExample() {
  // Open existing memory
  const mv: Memvid = await use('basic', 'notes.mv2', { readOnly: true });

  // Search with options
  const hits = await mv.find('deterministic', { k: 5, snippetChars: 200 });
  console.log(`Found ${hits.total_hits} results`);
  for (const hit of hits.hits) {
    console.log(`  ${hit.score.toFixed(2)}: ${hit.title}`);
  }

  // Ask with LLM synthesis
  const answer = await mv.ask('How does the WAL work?', {
    model: 'openai:gpt-4o-mini',
    modelApiKey: process.env.OPENAI_API_KEY,
    k: 10
  });
  console.log(answer.answer);

  // Timeline queries
  const timeline = await mv.timeline({
    since: 1730000000,
    until: 1730003600,
    limit: 20
  });
  console.log(`Timeline entries: ${timeline.length}`);
}
```

--------------------------------

### Install Memvid SDK and LlamaIndex (Node.js)

Source: https://docs.memvid.com/frameworks/llamaindex

Installs the necessary packages for using Memvid with LlamaIndex in a Node.js environment. Requires `@memvid/sdk`, `llamaindex`, and `@llamaindex/openai`.

```bash
npm install @memvid/sdk llamaindex @llamaindex/openai
```

--------------------------------

### Memvid CLI: Exact Match Search for Code

Source: https://docs.memvid.com/comparisons/vector-databases

This example illustrates using the Memvid CLI for exact matching in a code base. It's designed to quickly find specific function names or code snippets, contrasting with the fuzzier results typical of vector databases.

```bash
# Finding a specific function (exact match)
memvid find codebase.mv2 --query "handleWebSocketConnection"
#  Returns exact matches instantly
```

--------------------------------

### Search Memvid Memory

Source: https://docs.memvid.com/quickstart/cli-to-dashboard

Perform searches within a Memvid memory file using different modes: hybrid (default), lexical (keyword-based), and semantic (concept-based). Results can be retrieved in a human-readable format or as JSON for scripting purposes.

```bash
memvid find my-knowledge.mv2 --query "portable memory"
```

```bash
memvid find my-knowledge.mv2 --query "how to store AI data" --mode auto
```

```bash
memvid find my-knowledge.mv2 --query "embeddings" --mode lex
```

```bash
memvid find my-knowledge.mv2 --query "machine learning storage" --mode sem
```

```bash
memvid find my-knowledge.mv2 --query "memory" --json
```

--------------------------------

### N-Triples (RDF) Export Example

Source: https://docs.memvid.com/cli/advanced-commands

Example of data exported in N-Triples format, commonly used for RDF data, by the `memvid export` command.

```turtle
<Alice> <employer> "Anthropic" .
<Alice> <role> "Senior Engineer" .
<Bob> <employer> "OpenAI" .
```

--------------------------------

### Install Memvid SDK and LlamaIndex (Python)

Source: https://docs.memvid.com/frameworks/llamaindex

Installs the necessary packages for using Memvid with LlamaIndex in a Python environment. Requires `memvid-sdk`, `llama-index`, and `llama-index-llms-openai`.

```bash
pip install memvid-sdk llama-index llama-index-llms-openai
```

--------------------------------

### Install Memvid SDK and Haystack

Source: https://docs.memvid.com/frameworks/haystack

Installs the necessary Python packages for using Memvid with Haystack. This includes the memvid-sdk and haystack-ai libraries.

```bash
pip install memvid-sdk haystack-ai
```

--------------------------------

### Install Memvid SDK and LangChain Packages (Python)

Source: https://docs.memvid.com/frameworks/langchain

Installs the Memvid SDK and required LangChain libraries for Python projects. This ensures compatibility for integrating Memvid's features into LangChain workflows.

```bash
pip install memvid-sdk langchain langchain-openai
```

--------------------------------

### Open or Create Memvid Instance (Node.js, Python)

Source: https://docs.memvid.com/quickstart/sdk-recipes

Initializes a Memvid instance using the 'basic' provider and a specified file. It supports optional modes like 'auto' and features like enabling lexical search. Dependencies include the '@memvid/sdk' for Node.js and 'memvid_sdk' for Python.

```typescript
import { use } from "@memvid/sdk";

const mem = await use("basic", "notes.mv2", { mode: "auto", enableLex: true });
```

```python
from memvid_sdk import use

mem = use("basic", "notes.mv2", mode="auto", enable_lex=True, enable_vec=False)
```

--------------------------------

### Quick Start: Initialize Memvid with AutoGen Adapter (Node.js)

Source: https://docs.memvid.com/frameworks/autogen

Initializes the Memvid SDK with the 'autogen' adapter to access a knowledge base. It also demonstrates how to retrieve function schemas compatible with OpenAI's function calling API.

```typescript
import { use } from '@memvid/sdk';

// Open with AutoGen adapter
const mem = await use('autogen', 'knowledge.mv2');

// Access function schemas (OpenAI-compatible)
const functions = mem.functions;
```

--------------------------------

### Initialize and Use Gemini CLIP Provider (Bash & Python)

Source: https://docs.memvid.com/concepts/visual-embeddings

This snippet illustrates the setup and usage of the Gemini CLIP provider for text embeddings. It first shows how to set the Gemini API key as an environment variable in bash. Then, it demonstrates initializing the Gemini CLIP provider using a factory function or direct instantiation in Python, followed by embedding text.

```bash
export GEMINI_API_KEY=your-key-here
```

```python
from memvid_sdk.clip import get_clip_provider, GeminiClip

# Using factory
clip = get_clip_provider('gemini')

# Or with specific model
clip = get_clip_provider('gemini:embedding-001')

# Direct instantiation
clip = GeminiClip(model='embedding-001')

# Embed text
embedding = clip.embed_text('data visualization dashboard')
```

--------------------------------

### Install memvid-cli from Source

Source: https://docs.memvid.com/sdks/cli

Installs the memvid command-line interface directly from its source code using Cargo. This method allows for specific feature enablement, such as parallel segment building and temporal tracking.

```bash
cargo install --path memvid/crates/memvid-cli --force --features parallel_segments,temporal_track
```

--------------------------------

### Ingest, Search, and Ask Knowledge Base (Python)

Source: https://docs.memvid.com/examples/knowledge-base

Shows how to set up a knowledge base, add documents, conduct natural language searches, and query for answers using the Memvid SDK in Python. This requires the 'memvid_sdk' library.

```python
from memvid_sdk import use
import os

# Create knowledge base
kb = use('basic', 'company-kb.mv2', mode='create')

# Ingest documentation
kb.put({
    "title": "Employee Handbook",
    "label": "hr",
    "file": "./docs/handbook.pdf",
    "tags": ["hr", "policies", "onboarding"]
})

kb.put({
    "title": "API Documentation",
    "label": "engineering",
    "file": "./docs/api-guide.md",
    "tags": ["api", "development", "integration"]
})

# Search
results = kb.find("vacation policy", k=5)

# Q&A
answer = kb.ask("How many vacation days do employees get?")
print(answer["answer"])
```

--------------------------------

### Python Example for Masking False Positives

Source: https://docs.memvid.com/concepts/pii-masking

Illustrates potential false positives in PII detection. This Python code shows examples where strings that resemble PII patterns (like order numbers or version strings) might be flagged by the masking system.

```python
# Might be flagged as phone
text = "Order #555-123-4567"  # Could be order number

# Might be flagged as SSN
text = "Version 123-45-6789"  # Could be version string
```

--------------------------------

### Markdown Audit Report Example

Source: https://docs.memvid.com/cli/advanced-commands

An example of the markdown output format for the `memvid audit` command, showcasing sourced information with URIs and snippets.

```markdown
# Audit Report: Revenue trends in Q4

## Source 1: Q4 Financial Report
**URI:** mv2://reports/q4-2024.pdf

Revenue increased 15% year-over-year, driven by enterprise sales...

## Source 2: Board Meeting Notes
**URI:** mv2://notes/board-dec.md

CFO presented Q4 projections showing strong growth in the APAC region...
```

--------------------------------

### Memvid SDK Integration with Express.js Server

Source: https://docs.memvid.com/node-sdk/examples

Provides an example of integrating the Memvid SDK into an Express.js server to handle search and ask requests. This requires 'express' and '@memvid/sdk' packages. The server opens a read-only Memvid file and exposes POST endpoints for search queries and LLM-based question answering.

```typescript
import express from 'express';
import { open } from '@memvid/sdk';

const app = express();
app.use(express.json());

// Reuse a shared-lock handle for the lifetime of the server.
const mem = await open('knowledge.mv2', 'basic', { readOnly: true });

app.post('/search', async (req, res) => {
  const { query, k = 10 } = req.body;

  const results = await mem.find(query, { k });
  res.json(results);
});

app.post('/ask', async (req, res) => {
  const { question } = req.body;

  const answer = await mem.ask(question, {
    model: 'openai:gpt-4o-mini',
    modelApiKey: process.env.OPENAI_API_KEY,
    maskPii: true,
  });
  res.json(answer);
});

app.listen(3000, () => console.log('Server running on port 3000'));
```

--------------------------------

### Next.js API Route with Streaming in TypeScript

Source: https://docs.memvid.com/examples

This TypeScript example demonstrates how to build a Next.js API route that uses Memvid for AI-powered responses with streaming. It integrates with `@ai-sdk/openai` and `ai` library to stream text, utilizing Memvid's tools for knowledge base interaction. The code requires Node.js, Next.js, and the specified SDKs.

```typescript
//app/api/chat/route.ts
import { use } from '@memvid/sdk';
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

const mem = await use('vercel-ai', 'knowledge.mv2');

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = await streamText({
    model: openai('gpt-4o'),
    system: 'You are a helpful assistant. Use the tools to search the knowledge base.',
    tools: mem.tools,
    messages,
    maxSteps: 5,
  });

  return result.toDataStreamResponse();
}
```

--------------------------------

### JSON Audit Report Example

Source: https://docs.memvid.com/cli/advanced-commands

An example of the JSON output format for the `memvid audit` command, structured for machine readability with question, sources, and synthesized answer.

```json
{
  "question": "Revenue trends in Q4",
  "sources": [
    {
      "title": "Q4 Financial Report",
      "uri": "mv2://reports/q4-2024.pdf",
      "snippet": "Revenue increased 15% year-over-year...",
      "score": 0.95
    }
  ],
  "answer": "Based on the sources, Q4 revenue increased 15%..."
}
```

--------------------------------

### Node.js: Ingest Documents with External Embeddings

Source: https://docs.memvid.com/quickstart/five-minute-guide

This snippet demonstrates how to ingest documents into Memvid using an external embedding provider like OpenAI. It initializes the embedder and then uses `putMany` to add documents and `find` for querying. Ensure the OPENAI_API_KEY environment variable is set.

```typescript
import { create, OpenAIEmbeddings } from '@memvid/sdk';

const embedder = new OpenAIEmbeddings({
  apiKey: process.env.OPENAI_API_KEY,
  model: 'text-embedding-3-small'
});

const mem = await create('project.mv2');
await mem.putMany(docs, { embedder });
await mem.find('query', { embedder });
```

--------------------------------

### Bash: Adaptive Strategies Examples

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Showcases the command-line usage for different adaptive retrieval strategies in Bash. Each command specifies a unique strategy like 'combined', 'relative', 'absolute', 'cliff', or 'elbow' for varied search result filtering.

```bash
memvid find memory.mv2 --query "term" --adaptive-strategy combined

memvid find memory.mv2 --query "term" --adaptive-strategy relative

memvid find memory.mv2 --query "term" --adaptive-strategy absolute --min-relevancy 0.5

memvid find memory.mv2 --query "term" --adaptive-strategy cliff

memvid find memory.mv2 --query "term" --adaptive-strategy elbow
```

--------------------------------

### JSON Output Example for Memvid State

Source: https://docs.memvid.com/cli/advanced-commands

An example of the JSON output format when querying the state of an entity using the `--json` flag with the `memvid state` command.

```json
{
  "entity": "Alice",
  "found": true,
  "slots": {
    "employer": {
      "value": "Anthropic",
      "kind": "fact",
      "polarity": "positive",
      "source_frame_id": 145,
      "document_date": "2024-01-15",
      "engine": "rules"
    },
    "role": {
      "value": "Senior Engineer",
      "kind": "fact",
      "source_frame_id": 145,
      "engine": "rules"
    }
  }
}
```

--------------------------------

### Customer Support with PII Protection (Python SDK)

Source: https://docs.memvid.com/concepts/pii-masking

This Python example illustrates how to use the Memvid SDK for customer support scenarios, ensuring PII is masked in AI responses. It demonstrates calling `mem.ask` with `mask_pii=True` and logging the response safely.

```python
# Support bot with PII protection
response = mem.ask(
    ticket_content,
    mask_pii=True  # Don't expose customer PII
)

# Log safely
logger.info(f"Response: {response.answer}")  # No PII in logs
```

--------------------------------

### Install Memvid SDK, CrewAI, and CrewAI Tools (Python)

Source: https://docs.memvid.com/frameworks/crewai

Installs the Python packages required for integrating Memvid with CrewAI and its associated tools.

```bash
pip install memvid-sdk crewai crewai-tools
```

--------------------------------

### Error Handling for Memvid Operations

Source: https://docs.memvid.com/node-sdk/examples

Demonstrates robust error handling for Memvid SDK operations, specifically catching `CapacityExceededError` and `LockedError`. This example requires the '@memvid/sdk' package and is designed to be used within a try-catch block. It provides specific console logs for different error types.

```typescript
import { use, CapacityExceededError, LockedError } from '@memvid/sdk';

async function errorHandlingExample() {
  try {
    const mv = await use('basic', 'knowledge.mv2');
    await mv.put({
      title: 'Large Document',
      label: 'docs',
      file: 'large-file.pdf',
      vectorCompression: true
    });
    await mv.seal();
  } catch (error) {
    if (error instanceof CapacityExceededError) {
      console.log('MV001: Capacity exceeded - upgrade plan');
    } else if (error instanceof LockedError) {
      console.log('MV007: File is locked by another process');
    } else if (error instanceof Error) {
      console.log('Error:', error.message);
    }
  }
}
```

--------------------------------

### Enable and Query Logic Mesh

Source: https://docs.memvid.com/introduction/the-memvid-approach

Demonstrates how to enable Memvid's Logic Mesh feature during data ingestion for advanced relationship extraction without traditional ML models. It also shows how to query these extracted relationships, starting from a specific entity and traversing defined links.

```bash
# Enable Logic Mesh during ingestion
memvid put memory.mv2 --input docs/ --logic-mesh

# Query relationships
memvid follow traverse memory.mv2 --start "John" --link "works_at"
```

--------------------------------

### Memvid CLI Global Options Examples

Source: https://docs.memvid.com/cli/index

Demonstrates how to use global options with Memvid CLI commands, including increasing logging verbosity, setting a default embedding model, and enabling JSON output. These options apply to most commands.

```bash
# Increase logging verbosity
memvid --verbose find memory.mv2 --query "test"
memvid -vvv find memory.mv2 --query "test"  # Maximum verbosity

# Set default embedding model
memvid --model bge-small find memory.mv2 --query "test"
memvid -m openai find memory.mv2 --query "test"

# JSON output (most commands support this)
memvid find memory.mv2 --query "test" --json
```

--------------------------------

### Quick Start Memvid SDK with Google ADK (Python)

Source: https://docs.memvid.com/frameworks/google-adk

This Python snippet shows how to initialize the Memvid SDK using the 'google-adk' adapter to access a knowledge base file ('knowledge.mv2') and retrieve ADK tool definitions.

```python
from memvid_sdk import use

# Open with Google ADK adapter
mem = use('google-adk', 'knowledge.mv2')

# Access ADK tools
tools = mem.tools  # Returns ADK tool definitions
```

--------------------------------

### Install adrflow CLI

Source: https://docs.memvid.com/examples/packages

Installs the adrflow command-line interface globally using npm. Adrflow is an MCP server that captures architectural decisions made during coding, facilitating easier retrieval and understanding of past choices.

```bash
npm install -g adrflow
```

--------------------------------

### Node.js SDK: Q&A with Ollama

Source: https://docs.memvid.com/concepts/local-models

Utilizes the Memvid Node.js SDK to conduct Q&A using a local Ollama model. This asynchronous example initializes the SDK and sends a query, then logs the answer.

```javascript
import { use } from '@memvid/sdk';

const mem = await use('basic', 'knowledge.mv2');

// Ask with local Ollama model
const response = await mem.ask(
  'What are the key takeaways?',
  { model: 'ollama:qwen2.5:1.5b', k: 10 }
);

console.log(response.answer);
```

--------------------------------

### Example of Memvid Portability using Bash

Source: https://docs.memvid.com/concepts/memory-architecture

Shows how Memvid's single-file nature simplifies file management and transfer. Using bash commands, it demonstrates listing the .mv2 file and copying it to different locations, highlighting its portability without sidecar files.

```bash
# Your entire knowledge base
ls ~/project/
#  knowledge.mv2

# Share it anywhere
cp knowledge.mv2 /team/shared/
scp knowledge.mv2 user@server:/data/
git add knowledge.mv2
```

--------------------------------

### Node.js SDK: Session Recording and Replay

Source: https://docs.memvid.com/concepts/time-travel-replay

Illustrates how to use the Node.js SDK for session recording and replay. It covers starting a session, performing an ask, and replaying a session with audit and diff capabilities.

```typescript
import { create } from '@anthropic/memvid-sdk';

const mem = await create('knowledge.mv2', { enableVec: true });

// Record session
const sessionId = await mem.sessionStart("Audit Session");
const result = await mem.ask("What was the revenue?", { model: "openai:gpt-4o-mini" });
console.log(`Cost: $${result.usage.costUsd.toFixed(6)}`);
await mem.sessionEnd();

// Replay
const replay = await mem.sessionReplay(sessionId, {
  audit: true,
  useModel: "gemini:gemini-2.5-flash",
  diff: true
});
```

--------------------------------

### Node.js: Batch Ingestion of Documents

Source: https://docs.memvid.com/quickstart/five-minute-guide

This code demonstrates how to perform batch ingestion of multiple documents into Memvid using the Node.js SDK. The `docs` array should contain objects with `title`, `label`, and `text` properties. The `putMany` method efficiently adds all documents in a single operation.

```typescript
const docs = [
  { title: 'Doc 1', label: 'kb', text: 'Content 1' },
  { title: 'Doc 2', label: 'kb', text: 'Content 2' },
  { title: 'Doc 3', label: 'kb', text: 'Content 3' }
];

await mem.putMany(docs);
```

--------------------------------

### JSON Export Example

Source: https://docs.memvid.com/cli/advanced-commands

Example of data exported in JSON format by the `memvid export` command, representing facts with subject, predicate, and object.

```json
[
  {
    "subject": "Alice",
    "predicate": "employer",
    "object": "Anthropic",
    "confidence": 0.95,
    "source_frame_id": 145
  }
]
```

--------------------------------

### JSON Output Example for memvid CLI

Source: https://docs.memvid.com/cli/search-and-ask

Illustrates the structure of the JSON output when using the '--json' flag with the 'memvid ask' command. It details the fields provided, such as the answer, retrieval statistics, and grounding information.

```json
{
  "question": "What is the architecture?",
  "answer": "The architecture follows a layered design with...",
  "mode": "hybrid",
  "context_only": false,
  "hits": [
    {
      "rank": 1,
      "frame_id": 124,
      "uri": "mv2://docs/arch.md",
      "title": "Architecture Overview",
      "score": 0.92,
      "text": "The system consists of..."
    }
  ],
  "grounding": {
    "score": 0.85,
    "label": "HIGH",
    "sentence_count": 3,
    "grounded_sentences": 3,
    "has_warning": false
  },
  "follow_up": {
    "needed": false
  },
  "stats": {
    "retrieval_ms": 5,
    "synthesis_ms": 1200,
    "latency_ms": 1205
  }
}
```

--------------------------------

### Python Example for Context Insensitive Masking

Source: https://docs.memvid.com/concepts/pii-masking

Highlights the context-insensitive nature of PII detection. This Python example shows how a string that is not actually a phone number but follows a similar pattern will still be masked, demonstrating the system's reliance on patterns over contextual understanding.

```python
# Both masked the same way
"Call me at 555-123-4567"  # Real phone
"The code is 555-123-4567"  # Not a phone, but still masked
```

--------------------------------

### Python SDK: Session Recording and Replay

Source: https://docs.memvid.com/concepts/time-travel-replay

Demonstrates how to use the Python SDK to start a session, record an interaction, and then replay it in audit mode with diff enabled. It shows how to access usage costs and grounding scores.

```python
from memvid_sdk import create

mem = create('knowledge.mv2', enable_vec=True)

# Record session
session_id = mem.session_start("Audit Session")
result = mem.ask("What was the revenue?", model="openai:gpt-4o-mini")
print(f"Cost: ${result.usage.cost_usd:.6f}")
print(f"Grounding: {result.grounding.score:.0%}")
summary = mem.session_end()

# Replay with audit mode
replay = mem.session_replay(
    session_id,
    audit=True,
    use_model="claude:claude-4-sonnet",
    diff=True
)
for action in replay.ask_results:
    print(f"Original: {action.original_answer}")
    print(f"New: {action.new_answer}")
    print(f"Diff: {action.diff_status}")
```

--------------------------------

### Basic Semantic Kernel Integration with Memvid

Source: https://docs.memvid.com/frameworks/semantic-kernel

Demonstrates basic integration by initializing a Semantic Kernel, adding an OpenAI chat completion service, and then loading a Memvid knowledge base as a plugin. It shows how to use the Memvid plugin within a prompt to retrieve context for answering a question.

```python
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from memvid_sdk import use

# Initialize kernel
kernel = sk.Kernel()

# Add OpenAI service
kernel.add_service(
    OpenAIChatCompletion(
        service_id="openai",
        ai_model_id="gpt-4o"
    )
)

# Initialize with semantic-kernel adapter
mem = use('semantic-kernel', 'knowledge.mv2', read_only=True)

# Get and register plugin
memvid_plugin = mem.as_plugin()
kernel.add_plugin(memvid_plugin, "memvid")

# Use in prompts
result = await kernel.invoke_prompt(
    """Based on this context from the knowledge base:\n    {{memvid.search "authentication"}}\n\n    Answer the question: How does authentication work?"""
)
print(result)
```

--------------------------------

### Install Memvid SDK and Google Generative AI (Node.js)

Source: https://docs.memvid.com/frameworks/google-adk

Installs the necessary packages for integrating Memvid with the Google Generative AI SDK. This includes the Memvid SDK and the official Google Generative AI library.

```bash
npm install @memvid/sdk @google/generative-ai
```

--------------------------------

### Python: Ingest Documents with External Embeddings

Source: https://docs.memvid.com/quickstart/five-minute-guide

This snippet shows how to ingest documents into Memvid using an external embedding provider like OpenAI in Python. It initializes the embedder and then uses `put_many` for ingestion and `find` for querying. The `OPENAI_API_KEY` environment variable must be set.

```python
from memvid_sdk import create
from memvid_sdk.embeddings import OpenAIEmbeddings
import os

embedder = OpenAIEmbeddings(
    api_key=os.environ['OPENAI_API_KEY'],
    model='text-embedding-3-small'
)

mem = create('project.mv2', enable_vec=True)
mem.put_many(docs, embedder=embedder)
mem.find('query', embedder=embedder)
```

--------------------------------

### CSV Export Example

Source: https://docs.memvid.com/cli/advanced-commands

Example of data exported in CSV format by the `memvid export` command, including headers for subject, predicate, object, confidence, and source frame ID.

```csv
subject,predicate,object,confidence,source_frame_id
Alice,employer,Anthropic,0.95,145
Alice,role,Senior Engineer,0.92,145
Bob,employer,OpenAI,0.88,156
```

--------------------------------

### Manage Whisper Models with Memvid CLI

Source: https://docs.memvid.com/concepts/audio-video

Install specific Whisper models for use with Memvid and then specify the installed model during audio ingestion for tailored transcription accuracy.

```bash
memvid models install whisper-medium
memvid put memory.mv2 --input audio.mp3 --whisper-model medium
```

--------------------------------

### Install Local LLM Models for Enrichment

Source: https://docs.memvid.com/installation/models

Details on installing local GGUF models, such as Phi-3.5-mini, for use in Memvid's enrichment workflows. It clarifies that these are distinct from models used for `memvid ask` synthesis.

```bash
memvid models install phi-3.5-mini
memvid models install phi-3.5-mini-q8
```

--------------------------------

### Install Memvid SDK and LangChain Packages (Node.js)

Source: https://docs.memvid.com/frameworks/langchain

Installs the necessary Memvid SDK and LangChain packages for Node.js applications. This includes core LangChain modules and specific integrations like OpenAI and LangGraph, along with zod for schema validation.

```bash
npm install @memvid/sdk @langchain/core @langchain/openai @langchain/langgraph zod
```

--------------------------------

### Node.js: PDF Ingestion and Table Extraction

Source: https://docs.memvid.com/quickstart/five-minute-guide

This Node.js snippet shows how to ingest a PDF file into Memvid, assigning a title and label. It further demonstrates enabling the extraction of tables from the PDF and listing the extracted tables.

```typescript
await mem.put({ file: 'report.pdf', title: 'Q4 Report', label: 'report' });
await mem.putPdfTables('report.pdf', true);
const tables = await mem.listTables();
```

--------------------------------

### Bash: Adaptive Retrieval Tuning Options

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Provides examples of tuning adaptive retrieval behavior in Bash. These commands demonstrate setting a minimum relevance threshold, a maximum results cap, and specifying a particular adaptive strategy.

```bash
# Minimum relevance threshold (0.0-1.0)
memvid find memory.mv2 --query "security" --min-relevancy 0.6

# Maximum results cap
memvid find memory.mv2 --query "config" --max-k 50

# Choose strategy
memvid find memory.mv2 --query "api" --adaptive-strategy cliff
```

--------------------------------

### Python: Memvid SDK Usage for Adaptive Retrieval

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Demonstrates how to use the Memvid Python SDK for adaptive retrieval. Examples cover default adaptive search, configuring adaptive parameters like relevance threshold and strategy, and disabling adaptive mode for fixed top-k.

```python
from memvid import use

mem = use('basic', 'memory.mv2')

# Adaptive (default)
results = mem.find("authentication patterns")
print(f"Returned {len(results)} relevant results")

# With custom settings
results = mem.find(
    "security best practices",
    adaptive=True,
    min_relevancy=0.5,
    max_k=25,
    adaptive_strategy="cliff"
)

# Disable adaptive
results = mem.find(
    "config files",
    adaptive=False,
    top_k=10
)
```

--------------------------------

### Install Memvid Models

Source: https://docs.memvid.com/cli/advanced-commands

Installs specified models for enrichment, visual search, or entity extraction. Supports forcing re-downloads and specifying model types like CLIP or NER. Dependencies include the memvid CLI and network access to model repositories.

```bash
# Install LLM model for enrichment
memvid models install phi-3.5-mini

# Install CLIP model for visual search
memvid models install --clip mobileclip-s2

# Install NER model for Logic-Mesh entity extraction
memvid models install --ner distilbert-ner

# Force re-download
memvid models install phi-3.5-mini --force
```

--------------------------------

### Create and Ingest Data with Memvid SDK

Source: https://docs.memvid.com/node-sdk/examples

Illustrates how to create a new Memvid memory file, enable lexical search, and ingest data from text content or files. This function requires the '@memvid/sdk' package. It takes the desired memory file name and data (title, label, text/file) as input and creates a sealed memory file with ingested data.

```typescript
import { create, Memvid } from '@memvid/sdk';

async function ingestExample() {
  // Create new memory
  const mv: Memvid = await create('knowledge.mv2');

  // Enable lexical search
  await mv.enableLex();

  // Add documents with vector compression (for text content)
  await mv.put({
    title: 'API Documentation',
    label: 'docs',
    text: 'The API provides endpoints for search and retrieval...', 
    vectorCompression: true
  });

  // Add from file
  await mv.put({
    title: 'User Guide',
    label: 'docs',
    file: 'guide.pdf',
    vectorCompression: true
  });

  await mv.seal();
}
```

--------------------------------

### Install commitreel CLI

Source: https://docs.memvid.com/examples/packages

Installs the commitreel command-line interface globally using npm. Commitreel provides time travel capabilities for AI agent sessions, allowing users to record, scrub, and replay moments from agent executions.

```bash
npm install -g commitreel
```

--------------------------------

### Single-File Portability with Memvid

Source: https://docs.memvid.com/comparisons/vector-databases

Illustrates the ease of transferring Memvid's entire knowledge base, stored in a single '.mv2' file, using a simple secure copy command. This highlights Memvid's portability and ease of distribution.

```bash
scp knowledge.mv2 user@server:/data/
```

--------------------------------

### Embedding Providers

Source: https://docs.memvid.com/sdks/node

Code examples for configuring and using different embedding providers with the Memvid SDK.

```APIDOC
## Embedding Providers

### OpenAI Embeddings
```typescript
import { OpenAIEmbeddings } from '@memvid/sdk';

const openai = new OpenAIEmbeddings({
  apiKey: process.env.OPENAI_API_KEY,
  model: 'text-embedding-3-small'
});
```

### Gemini Embeddings
```typescript
import { GeminiEmbeddings } from '@memvid/sdk';

const gemini = new GeminiEmbeddings({
  apiKey: process.env.GEMINI_API_KEY
});
```

### Mistral Embeddings
```typescript
import { MistralEmbeddings } from '@memvid/sdk';

const mistral = new MistralEmbeddings({
  apiKey: process.env.MISTRAL_API_KEY
});
```

### Local Embedding Models

#### Usage with `mem.put`
```typescript
import { LOCAL_EMBEDDING_MODELS } from '@memvid/sdk';

// Assuming 'mem' is an initialized Memvid instance
await mem.put({
  text: 'content',
  enableEmbedding: true,
  embeddingModel: LOCAL_EMBEDDING_MODELS.BGE_SMALL
});
```

#### Available Local Models

| Model       | Dimensions | Speed   | Quality |
| ----------- | ---------- | ------- | ------- |
| `BGE_SMALL` | 384        | Fastest | Good    |
| `BGE_BASE`  | 768        | Fast    | Better  |
| `NOMIC`     | 768        | Fast    | Better  |
| `GTE_LARGE` | 1024       | Slower  | Best    |

```

--------------------------------

### Memvid Session Management for Debugging (CLI)

Source: https://docs.memvid.com/index

Demonstrates how to start, perform operations within, and end a Memvid session for time-travel debugging purposes.

```bash
memvid session start knowledge.mv2 --name "qa-test"
memvid find knowledge.mv2 --query "test query"
memvid session end knowledge.mv2
```

--------------------------------

### Install and Manage Memvid Models

Source: https://docs.memvid.com/installation/models

Commands to install, list, verify, and remove various types of models used by Memvid. This includes CLIP and NER models, as well as local LLMs for enrichment. Models are cached locally under `MEMVID_MODELS_DIR`.

```bash
# See available/installed models (no downloads)
memvid models list

# Install optional CLIP + NER models
memvid models install --clip mobileclip-s2
memvid models install --ner distilbert-ner

# Install optional local LLM for enrichment (GGUF)
memvid models install phi-3.5-mini

# Filter model list
memvid models list --model-type embedding
memvid models list --model-type clip --json

# Verify installed enrichment LLM models
memvid models verify
memvid models verify phi-3.5-mini

# Remove an enrichment LLM model
memvid models remove phi-3.5-mini --yes
```

--------------------------------

### Explicitly Track Document Versions (Bash)

Source: https://docs.memvid.com/concepts/deduplication

This example demonstrates how to explicitly track document versions using metadata. By providing a JSON object with a 'version' key in the metadata, duplicate files with different versions can be distinguished.

```bash
# Version in metadata distinguishes duplicates
memvid put memory.mv2 --input contract.pdf --metadata '{"version": "1.0"}'
memvid put memory.mv2 --input contract.pdf --metadata '{"version": "1.1"}'
```

--------------------------------

### Handling Vercel Serverless /tmp Storage with Memvid SDK

Source: https://docs.memvid.com/node-sdk/overview

This TypeScript example demonstrates a strategy for managing Vercel's ephemeral `/tmp` storage when working with Memvid SDK's `.mv2` files. It shows how to check if a file exists locally, download it from cloud storage (like S3) if it doesn't, save it to `/tmp`, and then open it with the Memvid SDK. This approach is crucial for production applications where persistence is needed across serverless function invocations.

```typescript
// Example: Download from S3 if not in /tmp
import { existsSync } from 'fs';
import { writeFile } from 'fs/promises';

const localPath = `/tmp/${memoryId}.mv2`;

if (!existsSync(localPath)) {
  const buffer = await downloadFromS3(userId, memoryId);
  await writeFile(localPath, buffer);
}

const mem = await open(localPath);
```

--------------------------------

### Quick Start: Use Memvid with LlamaIndex (Python)

Source: https://docs.memvid.com/frameworks/llamaindex

Demonstrates the basic integration of Memvid with LlamaIndex in Python. It shows how to initialize Memvid with the LlamaIndex adapter and access tools and the query engine.

```python
from memvid_sdk import use

# Open with LlamaIndex adapter
mem = use('llamaindex', 'knowledge.mv2')

# Access LlamaIndex tools and query engine
tools = mem.tools
query_engine = mem.as_query_engine()
```

--------------------------------

### Bash: Set Cohere API Key

Source: https://docs.memvid.com/concepts/embedding-models

Environment variable setup for Cohere API key authentication.

```bash
export COHERE_API_KEY=your-key-here

```

--------------------------------

### Install memvid-mind via plugin

Source: https://docs.memvid.com/examples/packages

Installs the memvid-mind package through the Claude Code plugin marketplace. Memvid-mind enables persistent, single-file memory for Claude Code, simplifying data management and recall without external databases.

```bash
/plugin marketplace add memvid/memvid-mind
/plugin install memvid-mind
```

--------------------------------

### Python Direct Function Calling with OpenAI/Google ADK

Source: https://docs.memvid.com/frameworks/overview

Provides Python examples for using Memvid adapters with OpenAI and Google ADK for direct function calling. It showcases how to pass Memvid's tools directly to the respective client's completion or chat creation methods. Key dependencies are `memvid_sdk` and the respective AI client libraries.

```python
# OpenAI
mem = use('openai', 'knowledge.mv2')
response = client.chat.completions.create(
    model="gpt-4o",
    tools=mem.tools,
    messages=[...])

# Google Gemini
mem = use('google-adk', 'knowledge.mv2')
chat = client.chats.create(
    model="gemini-2.0-flash",
    config=types.GenerateContentConfig(tools=[mem.tools]))
```

--------------------------------

### Understanding Memvid Search Results with JSON Output

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Shows how to use the --json flag with the memvid CLI to get detailed results, including adaptive information, which helps in understanding the retrieval process.

```bash
memvid find memory.mv2 --query "authentication" --json
```

```json
{
  "query": "authentication",
  "strategy": "combined",
  "results": [
    {
      "frame_id": "frame_001",
      "score": 0.94,
      "title": "OAuth2 Implementation Guide"
    },
    {
      "frame_id": "frame_002",
      "score": 0.89,
      "title": "JWT Token Handling"
    },
    {
      "frame_id": "frame_003",
      "score": 0.85,
      "title": "Session Management"
    }
  ],
  "adaptive_info": {
    "total_candidates": 150,
    "cutoff_score": 0.72,
    "cutoff_reason": "cliff_detected",
    "results_returned": 3
  }
}
```

--------------------------------

### Manage Ollama as a Background Service (Linux systemd)

Source: https://docs.memvid.com/concepts/local-models

Offers commands to manage the Ollama service using systemd on Linux. This includes starting the service, enabling it to run on boot, and checking its status.

```bash
# Start service
sudo systemctl start ollama

# Enable on boot
sudo systemctl enable ollama

# Check status
sudo systemctl status ollama
```

--------------------------------

### Ingest and Query Documents with Python SDK

Source: https://docs.memvid.com/examples/document-qa

This Python code snippet shows how to utilize the 'memvid_sdk' to initialize a document store, upload PDF documents from a specified directory, and query the store for answers to questions. It also retrieves and displays the source of the answer.

```python
from memvid_sdk import use
import os

# Create document store
mem = use('basic', 'documents.mv2', mode='create')

# Ingest a folder of documents
for filename in os.listdir('./docs'):
    if filename.endswith('.pdf'):
        mem.put({
            "title": filename,
            "label": "document",
            "file": f"./docs/{filename}"
        })

# Ask questions
answer = mem.ask("What is the refund policy?")
print(f"Answer: {answer.get('answer')}")
print(f"Source: {(answer.get('sources') or [{}])[0].get('title', 'n/a')}")
```

--------------------------------

### Reinstall Memvid SDK (Python & Node.js)

Source: https://docs.memvid.com/resources/troubleshooting

Addresses import errors by providing commands to reinstall the Memvid SDK for both Python and Node.js environments. Ensure you have the latest version installed for optimal performance.

```bash
pip install --upgrade memvid-sdk  # Python
```

```bash
npm install @memvid/sdk           # Node.js
```

--------------------------------

### Rotating Memvid Passwords and Verifying Encryption

Source: https://docs.memvid.com/concepts/encryption

This example demonstrates the best practice of rotating Memvid passwords periodically. It covers decrypting a file with an old password and re-encrypting it with a new one. Additionally, it shows how to verify the integrity of an encrypted file by decrypting it to a temporary location and comparing stats.

```bash
# Decrypt with old password
echo "$OLD_PASSWORD" | memvid unlock memory.mv2e --password-stdin --out memory.mv2

# Re-encrypt with new password
echo "$NEW_PASSWORD" | memvid lock memory.mv2 --password-stdin --out memory.mv2e

# Encrypt
memvid lock memory.mv2 --out memory.mv2e

# Verify by decrypting to temp location
memvid unlock memory.mv2e --out /tmp/verify.mv2
memvid stats /tmp/verify.mv2  # Should match original
rm /tmp/verify.mv2
```

--------------------------------

### Get statistics and maintain memory file

Source: https://docs.memvid.com/sdks/cli

Provides statistics for a memvid memory file and performs maintenance operations. Commands include getting stats, verifying file integrity deeply, and vacuuming and rebuilding indexes.

```bash
memvid stats data.mv2
memvid verify data.mv2 --deep
memvid doctor data.mv2 --vacuum --rebuild-lex-index --rebuild-vec-index
```

--------------------------------

### Ingest, Search, and Ask Knowledge Base (Node.js)

Source: https://docs.memvid.com/examples/knowledge-base

Demonstrates how to create a knowledge base, ingest documents, perform natural language searches, and ask AI-powered questions using the Memvid SDK in Node.js. Requires the '@memvid/sdk' package.

```typescript
import { use } from '@memvid/sdk';

// Create knowledge base
const kb = await use('basic', 'company-kb.mv2', { mode: 'create' });

// Ingest documentation
await kb.put({
  title: 'Employee Handbook',
  label: 'hr',
  file: './docs/handbook.pdf',
  tags: ['hr', 'policies', 'onboarding'],
});

// Search
const results = await kb.find('vacation policy', { k: 5 });

// Q&A
const answer = await kb.ask('How many vacation days do employees get?');
console.log(answer.answer);
```

--------------------------------

### Managing Tickets and Capacity

Source: https://docs.memvid.com/node-sdk/overview

This snippet shows how to interact with memory capacity and tickets. It includes functions to get the current capacity, retrieve current ticket information, sync tickets from a dashboard using an API key, manually apply a ticket, get the memory binding, and unbind from the dashboard.

```typescript
const capacity = await mem.getCapacity();

const ticket = await mem.currentTicket();

const result = await mem.syncTickets('mem_abc123', apiKey);

await mem.applyTicket(ticketString);

const binding = await mem.getMemoryBinding();

await mem.unbindMemory();
```

--------------------------------

### View Memvid Timeline with Date Filtering

Source: https://docs.memvid.com/cli/timeline-and-view

Displays a timeline of entries in a memvid file, filtered by a start date. This is useful for reviewing recent additions or changes. It requires the memvid executable, the .mv2 file path, and a Unix timestamp for the start date.

```bash
# See what was added today
memvid timeline notes.mv2 --since $(date -d "today 00:00" +%s) --limit 50
```

--------------------------------

### Quick Start: Memvid Python SDK Usage

Source: https://docs.memvid.com/sdks/python

Demonstrates the basic usage of the Memvid Python SDK. This includes creating a new memory file, adding documents with titles, labels, metadata, and text, enabling embeddings, performing searches, asking AI questions using a specified model and API key, and finally closing the memory file.

```python
import memvid_sdk as memvid
import os

# Create a new memory file
mem = memvid.create('knowledge.mv2')

# Add documents
mem.put(
    title='Meeting Notes',
    label='notes',
    metadata={'source': 'slack'},
    text='Alice mentioned she works at Anthropic',
    enable_embedding=True
)

# Search
results = mem.find('who works at AI companies?')
print(results['hits'])

# Ask questions with AI
answer = mem.ask(
    'What does Alice do?',
    model='gpt-4o-mini',
    api_key=os.environ['OPENAI_API_KEY']
)
print(answer['text'])

# Close when done
mem.close()
```

--------------------------------

### Bash: Set Voyage API Key

Source: https://docs.memvid.com/concepts/embedding-models

Environment variable setup for Voyage AI API key authentication.

```bash
export VOYAGE_API_KEY=your-key-here

```

--------------------------------

### Entity Extraction

Source: https://docs.memvid.com/sdks/node

Code examples for performing entity extraction and managing memory cards.

```APIDOC
## Entity Extraction

### Extracting Facts
```typescript
// Extract facts using rules engine
const result = await mem.enrich('rules');
```

### Viewing Extracted Cards
```typescript
const { cards, count } = await mem.memories();
console.log(`Extracted ${count} memory cards`);
```

### Getting Entity State
```typescript
// Get entity state (O(1) lookup)
const alice = await mem.state('Alice');
console.log(alice.slots);
// { employer: 'Anthropic', role: 'Engineer' }
```

### Adding Memory Cards Manually
```typescript
await mem.addMemoryCards([
  { entity: 'Alice', slot: 'employer', value: 'Anthropic' },
  { entity: 'Bob', slot: 'team', value: 'Infrastructure' }
]);
```

### Exporting Facts
```typescript
const json = await mem.exportFacts('json');
const csv = await mem.exportFacts('csv', 'Alice');
```
```

--------------------------------

### Configure LLM: Local Ollama Model

Source: https://docs.memvid.com/cli/search-and-ask

Setting up and using a local Ollama model with memvid. This involves installing Ollama, pulling a model, and configuring memvid to use it. Recommended for privacy.

```bash
# Install Ollama
brew install ollama  # macOS
# or: curl -fsSL https://ollama.com/install.sh | sh  # Linux

# Start Ollama server
ollama serve &

# Pull a model
ollama pull qwen2.5:1.5b

# Use with memvid
memvid ask knowledge.mv2 --question "..." --use-model "ollama:qwen2.5:1.5b"
```

--------------------------------

### Basic Usage of Memvid and Gemini (Python)

Source: https://docs.memvid.com/frameworks/google-adk

This Python code initializes the Memvid SDK in read-only mode, retrieves tool definitions, creates a Gemini client, and starts a chat session configured with these tools and a system instruction to act as a helpful assistant.

```python
from google import genai
from google.genai import types
from memvid_sdk import use

# Initialize with google-adk adapter
mem = use('google-adk', 'knowledge.mv2', read_only=True)

# Get tool definitions
tools = mem.tools

# Create Gemini client
client = genai.Client()

# Create chat with tools
chat = client.chats.create(
    model="gemini-2.0-flash",
    config=types.GenerateContentConfig(
        tools=tools,
        system_instruction="You are a helpful assistant with access to a knowledge base."
    )
)

# Send message
response = chat.send_message("What are the best practices for deployment?")
print(response.text)
```

--------------------------------

### Troubleshoot Memvid Ticket Sync Failure

Source: https://docs.memvid.com/cli/tickets-and-capacity

Offers basic troubleshooting steps for failed ticket syncs, starting with verifying the API key.

```bash
# Check your API key is set
echo $MEMVID_API_KEY
```

--------------------------------

### Manage Ollama as a Background Service (macOS)

Source: https://docs.memvid.com/concepts/local-models

Provides commands to start, stop, and check the status of the Ollama service using Homebrew on macOS. This allows Ollama to run in the background.

```bash
# Start service
brew services start ollama

# Stop service
brew services stop ollama

# Check status
brew services list | grep ollama
```

--------------------------------

### Function Calling with Memvid and Gemini (Python)

Source: https://docs.memvid.com/frameworks/google-adk

This Python example defines `memvid_search` and `memvid_ask` functions that utilize the Memvid SDK's find and ask methods. These functions are then registered as tools for the Gemini client, demonstrating how to handle function calls returned by the model.

```python
from google import genai
from memvid_sdk import use

mem = use('google-adk', 'knowledge.mv2', read_only=True)

# Define tool functions
def memvid_search(query: str) -> str:
    """Search the knowledge base for relevant information."""
    results = mem.find(query, k=5)
    return "\n".join([f"- {r.title}: {r.snippet}" for r in results])

def memvid_ask(question: str) -> str:
    """Ask a question and get an AI-synthesized answer."""
    answer = mem.ask(question)
    return str(answer.get("answer", ""))

# Register functions
tools = [memvid_search, memvid_ask]

# Create client and generate
client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents="Search for information about authentication",
    config=types.GenerateContentConfig(tools=tools)
)

# Handle function calls
for part in response.candidates[0].content.parts:
    if hasattr(part, 'function_call'):
        fn = part.function_call
        if fn.name == "memvid_search":
            result = memvid_search(fn.args["query"])
            print(result)
```

--------------------------------

### Remove and Verify Memvid Models

Source: https://docs.memvid.com/cli/advanced-commands

Removes installed models from the local system and verifies the integrity of installed models. The `remove` command can skip confirmation prompts with the `--yes` flag. Verification ensures models are not corrupted.

```bash
# Remove a model
memvid models remove phi-3.5-mini

# Skip confirmation
memvid models remove phi-3.5-mini --yes

# Verify model integrity
memvid models verify phi-3.5-mini

# Verify all installed models
memvid models verify
```

--------------------------------

### Multi-Tool Agent with Memvid and Gemini (Python)

Source: https://docs.memvid.com/frameworks/google-adk

This Python snippet illustrates the setup for a multi-tool agent using Memvid and Gemini. It defines a `search_knowledge` function that uses the Memvid SDK to query a knowledge base, preparing for integration into a larger agentic loop.

```python
from google import genai
from google.genai import types
from memvid_sdk import use

mem = use('google-adk', 'knowledge.mv2', read_only=True)

# Define multiple tools
def search_knowledge(query: str) -> str:
    """Search the knowledge base for relevant information."""
    results = mem.find(query, k=5)

```

--------------------------------

### Implement Parallel Search with Python Asyncio

Source: https://docs.memvid.com/errors/troubleshooting

This Python snippet demonstrates how to search across multiple memvid files in parallel using asyncio for improved performance on large datasets.

```python
import asyncio

async def search_all(query):
    files = ['docs.mv2', 'wiki.mv2', 'papers.mv2']
    tasks = [search_file(f, query) for f in files]
    return await asyncio.gather(*tasks)
```

--------------------------------

### Function Calling Example with Memvid (Node.js)

Source: https://docs.memvid.com/frameworks/openai

Demonstrates how to use Memvid functions with OpenAI's chat completions API in Node.js. It includes setting up the OpenAI client, defining messages, and handling the response, including executing Memvid's `find`, `put`, and `ask` functions based on the AI's tool calls.

```typescript
import { use } from '@memvid/sdk';
import OpenAI from 'openai';

// Get Memvid functions
const mem = await use('openai', 'knowledge.mv2');
const functions = mem.functions;

// Create OpenAI client
const client = new OpenAI();

const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
  { role: 'system', content: 'You are a helpful assistant with access to a knowledge base.' },
  { role: 'user', content: 'Search for information about authentication' },
];

// Create completion with function calling
const response = await client.chat.completions.create({
  model: 'gpt-4o',
  messages,
  tools: functions.map((f: any) => ({ type: 'function' as const, function: f })),
  tool_choice: 'auto',
});

// Handle function calls
const message = response.choices[0].message;
if (message.tool_calls) {
  for (const toolCall of message.tool_calls) {
    const funcName = toolCall.function.name;
    const funcArgs = JSON.parse(toolCall.function.arguments);

    let result: any;
    if (funcName === 'memvid_find') {
      result = await mem.find(funcArgs.query, { k: funcArgs.top_k || 5 });
    } else if (funcName === 'memvid_put') {
      result = await mem.put({
        title: funcArgs.title,
        label: funcArgs.label,
        text: funcArgs.text,
      });
    } else if (funcName === 'memvid_ask') {
      result = await mem.ask(funcArgs.question, { mode: funcArgs.mode || 'auto' });
    }

    console.log(`Function ${funcName} result:`, result);
  }
}
```

--------------------------------

### Models Management API

Source: https://docs.memvid.com/cli/advanced-commands

Commands for installing, listing, removing, and verifying local models used for enrichment, embeddings, and visual search.

```APIDOC
## Models Management

### Description
Manages local models for enrichment, embeddings, and visual search.

### Install Models

```bash
# Install LLM model for enrichment
memvid models install phi-3.5-mini

# Install CLIP model for visual search
memvid models install --clip mobileclip-s2

# Install NER model for Logic-Mesh entity extraction
memvid models install --ner distilbert-ner

# Force re-download
memvid models install phi-3.5-mini --force
```

### List Models

```bash
# List all models
memvid models list

# JSON output
memvid models list --json

# Filter by model type
memvid models list --model-type embedding
memvid models list --model-type clip
memvid models list --model-type ner
```

### Remove Models

```bash
# Remove a model
memvid models remove phi-3.5-mini

# Skip confirmation
memvid models remove phi-3.5-mini --yes
```

### Verify Models

```bash
# Verify model integrity
memvid models verify phi-3.5-mini

# Verify all installed models
memvid models verify
```

### Model Types

- **embedding**: Text embedding models for semantic search
- **reranker**: Result reranking models
- **llm**: Local LLM models for inference
- **clip**: CLIP models for visual search
- **ner**: NER models for entity extraction
- **external**: External API-based models
```

--------------------------------

### Check Python Architecture on Apple Silicon with Python

Source: https://docs.memvid.com/errors/troubleshooting

This Python command checks the system architecture, specifically to verify if running on Apple Silicon (expected 'arm64').

```python
import platform; print(platform.machine())
```

--------------------------------

### MemVid Find CLI

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Command-line interface examples for using the MemVid find functionality, including options for JSON output, disabling adaptive retrieval, and pagination.

```APIDOC
## CLI Commands for MemVid Find

### Description
Examples of using the `memvid find` command-line tool with various options.

### Method
CLI Command

### Endpoint
N/A (Command-line tool)

### Parameters
* `--query` (string) - Required - The search query.
* `--json` (flag) - Optional - Output results in JSON format.
* `--no-adaptive` (flag) - Optional - Disable adaptive retrieval.
* `--top-k` (number) - Optional - Number of top results to return when adaptive is disabled.
* `--cursor` (string) - Optional - Cursor for pagination.
* `--min-relevancy` (number) - Optional - Minimum relevancy score threshold.
* `--max-k` (number) - Optional - Maximum number of results to consider.
* `--adaptive-strategy` (string) - Optional - Strategy for adaptive retrieval (e.g., 'absolute', 'relative', 'combined').
* `--start` (string) - Optional - Start date for time filtering (YYYY-MM-DD).
* `--end` (string) - Optional - End date for time filtering (YYYY-MM-DD).
* `--mode` (string) - Optional - Search mode (e.g., 'auto', 'lex', 'sem').

### Request Example (CLI)
```bash
# Basic search with JSON output
memvid find memory.mv2 --query "authentication" --json

# Pagination example
# Page 1
memvid find memory.mv2 --query "logs" --no-adaptive --top-k 20

# Page 2 (use cursor)
memvid find memory.mv2 --query "logs" --no-adaptive --top-k 20 --cursor <cursor>

# High-precision tuning
memvid find memory.mv2 --query "term" \
  --min-relevancy 0.7 \
  --adaptive-strategy absolute

# High-recall tuning
memvid find memory.mv2 --query "term" \
  --min-relevancy 0.3 \
  --max-k 100 \
  --adaptive-strategy relative

# Exploratory search
memvid find memory.mv2 --query "term" \
  --min-relevancy 0.4 \
  --max-k 50 \
  --adaptive-strategy combined

# Combining with time filtering
memvid find memory.mv2 --query "report" \
  --start 2024-01-01 \
  --end 2024-06-30 \
  --adaptive-strategy cliff

# Monitoring adaptive decisions
memvid find memory.mv2 --query "term" --json | jq '.adaptive_info'
```

### Response
#### Success Response (200)
* For JSON output, refer to the 'MemVid Find API (Node.js)' section.
* For non-JSON output, results are displayed in a human-readable format.

#### Response Example (JSON)
```json
{
  "query": "authentication",
  "strategy": "combined",
  "results": [
    {
      "frame_id": "frame_001",
      "score": 0.94,
      "title": "OAuth2 Implementation Guide"
    },
    {
      "frame_id": "frame_002",
      "score": 0.89,
      "title": "JWT Token Handling"
    },
    {
      "frame_id": "frame_003",
      "score": 0.85,
      "title": "Session Management"
    }
  ],
  "adaptive_info": {
    "total_candidates": 150,
    "cutoff_score": 0.72,
    "cutoff_reason": "cliff_detected",
    "results_returned": 3
  }
}
```
```

--------------------------------

### OpenAI Entities Setup and Usage with Python

Source: https://docs.memvid.com/concepts/entity-extraction

Provides instructions for setting up and using OpenAI's entity extraction capabilities within the Memvid Python SDK. It includes setting the OpenAI API key as an environment variable and demonstrates initializing the extractor with custom entity types.

```bash
export OPENAI_API_KEY=sk-your-key-here
```

```python
from memvid_sdk.entities import get_entity_extractor, OpenAIEntities

# Using factory with custom entity types
ner = get_entity_extractor('openai', entity_types=[
    'COMPANY',
    'PERSON',
    'LOCATION',
    'MONEY',
    'DATE',
    'PRODUCT',
    'DEAL_TYPE',
])

```

--------------------------------

### Python and Bash for Frame Deletion and Vacuuming

Source: https://docs.memvid.com/introduction/frames

Illustrates how to mark frames as deleted (tombstoning) using Python and then physically reclaim space by running a vacuum operation via the command line. This is crucial for managing storage effectively after deletions.

```python
# Mark frame as deleted
mem.delete(frame_id=42)

# Frame still exists but won't appear in searches
# Use vacuum to physically reclaim space
```

```bash
memvid doctor knowledge.mv2 --vacuum
```

--------------------------------

### Quick Start: Use Memvid with LlamaIndex (Node.js)

Source: https://docs.memvid.com/frameworks/llamaindex

Demonstrates the basic integration of Memvid with LlamaIndex in Node.js. It shows how to initialize Memvid with the LlamaIndex adapter, access tools, and use the query engine.

```typescript
import { use } from '@memvid/sdk';

// Open with LlamaIndex adapter
const mem = await use('llamaindex', 'knowledge.mv2');

// Access LlamaIndex tools
const tools = mem.tools;       // FunctionTool array
const functions = mem.functions; // Raw function schemas

// Use query engine
const queryEngine = mem.asQueryEngine();
const response = await queryEngine.query({ query: 'What is Memvid?' });
console.log(response.response);
```

--------------------------------

### Enable Vector Compression (CLI and Python)

Source: https://docs.memvid.com/concepts/embedding-models

Demonstrates how to enable vector compression for reduced storage using both the command-line interface and Python SDK. Compression uses Product Quantization (PQ) to maintain search quality.

```bash
# CLI
memvid put knowledge.mv2 --input docs/ --embedding --vector-compression
```

```python
# Python
from memvid_sdk import create

mem = create("knowledge.mv2", enable_vec=True, enable_lex=True)
mem.put("Doc", "kb", {}, text="...", enable_embedding=True, vector_compression=True)
```

--------------------------------

### MemVid Compliance Audit Trail Example

Source: https://docs.memvid.com/concepts/time-travel-replay

Demonstrates using MemVid for compliance by recording all decision-making processes within a session. Later, the session can be replayed using the '--audit' flag to provide an immutable record of exact frames and answers for verification.

```bash
# Record all decisions for audit
memvid session start knowledge.mv2 --name "Compliance Review 2024-12"
memvid ask knowledge.mv2 --question "Is this transaction fraudulent?" --use-model openai
memvid session end knowledge.mv2

# Later: Replay with frozen context to verify decision
memvid session replay knowledge.mv2 --session <id> --audit
# Shows exact frames and answer - reproducible for auditors
```

--------------------------------

### Filter Before Querying with Memvid CLI

Source: https://docs.memvid.com/concepts/query-usage

Illustrates using the `--scope` and `--start` options in the Memvid CLI to narrow down search scope and time range, making queries more efficient.

```bash
# Search entire memory
memvid find memory.mv2 --query "report"  # Broad

# Better: Scope to relevant documents
memvid find memory.mv2 --query "report" --scope "finance/" --start 2024-01-01
```

--------------------------------

### Listing Tables with Memvid CLI

Source: https://docs.memvid.com/concepts/table-extraction

Shows how to list all extracted tables stored within a Memvid memory file. Includes examples for standard output and for obtaining JSON output suitable for scripting and programmatic access.

```bash
# List all tables in memory
memvid tables list memory.mv2

# JSON output for scripting
memvid tables list memory.mv2 --json
```

--------------------------------

### Process Invoices with Table Extraction

Source: https://docs.memvid.com/cli/create-and-put

A workflow example for processing invoices, including creating a dedicated memory, ingesting invoices with table extraction, searching for specific data like 'total', listing extracted tables, and exporting line items to CSV.

```bash
# Create memory for invoices
memvid create invoices.mv2

# Ingest invoice with table extraction
memvid put invoices.mv2 --input amazon-invoice.pdf --tables --vector-compression

# Search for specific items
memvid find invoices.mv2 --query "total" --json

# List extracted tables
memvid tables list invoices.mv2

# Export line items to CSV
memvid tables export invoices.mv2 --table-id pdf_table_1_page1 --format csv
```

--------------------------------

### Initialize CLIP Provider and Store PDF (Python/Node.js)

Source: https://docs.memvid.com/concepts/visual-embeddings

This snippet demonstrates how to initialize a CLIP provider (e.g., OpenAI, local, or Gemini) and use the Memvid SDK to store a PDF for visual search. It covers setting up the memory, enabling lex, putting a file, generating a text embedding for a visual search query, and performing a search.

```python
from memvid_sdk import create
from memvid_sdk.clip import get_clip_provider

# Initialize CLIP provider
clip = get_clip_provider('openai')  # or 'local', 'gemini'
print(f"Provider: {clip.name} ({clip.dimension} dimensions)")

# Create memory and store a PDF
mem = create('visual_search.mv2')
mem.enable_lex()

frame_id = mem.put(
    title="Annual Report 2024",
    label="report",
    metadata={"year": 2024},
    file="report.pdf",
)

# Generate text embedding for visual search
query_embedding = clip.embed_text("revenue growth charts")
print(f"Query embedding: {len(query_embedding)} dimensions")

# Search (visual search requires vector index)
results = mem.find("revenue", k=10)
```

```typescript
import { create, getClipProvider } from '@memvid/sdk';

// Initialize CLIP provider
const clip = getClipProvider('openai');  // or 'local', 'gemini'
console.log(`Provider: ${clip.name} (${clip.dimension} dimensions)`);

// Create memory and store a PDF
const mem = await create('visual_search.mv2');
await mem.enableLex();

const frameId = await mem.put({
  title: 'Annual Report 2024',
  label: 'report',
  metadata: { year: 2024 },
  file: 'report.pdf',
});

// Generate text embedding for visual search
const queryEmbedding = await clip.embedText('revenue growth charts');
console.log(`Query embedding: ${queryEmbedding.length} dimensions`);

// Search
const results = await mem.find('revenue', { k: 10 })
```

--------------------------------

### OpenAI Entity Extraction Setup

Source: https://docs.memvid.com/concepts/entity-extraction

Configure and instantiate an OpenAI entity extractor using the Memvid SDK. Supports direct instantiation or using a factory function. Specify entity types and optionally a specific model for extraction. Input is text, output is a list of extracted entities.

```python
from memvid_sdk.entities import get_entity_extractor, OpenAIEntities

# Using factory
ner = get_entity_extractor('openai', entity_types=['COMPANY', 'PERSON'])

# Or with specific model
ner = get_entity_extractor('openai:gpt-4o-mini', entity_types=['COMPANY', 'PERSON'])

# Direct instantiation
ner = OpenAIEntities(
    model='gpt-4o-mini',
    entity_types=['COMPANY', 'EXECUTIVE', 'PRODUCT'],
)

# Extract entities
entities = ner.extract(text, min_confidence=0.5)
```

--------------------------------

### Session Management

Source: https://docs.memvid.com/cli/advanced-commands

Commands for managing video recording sessions, including starting, ending, listing, replaying, and deleting sessions.

```APIDOC
## Session Management

### End Session

**Description:** Ends the current recording session and generates a summary.

**Method:** CLI Command

**Endpoint:** N/A (CLI command)

**Parameters:**
#### Path Parameters
- **knowledge.mv2** (string) - Required - The database file for the session.

**Command Example:**
```bash
memvid session end knowledge.mv2
```

### List Sessions

**Description:** Lists all recorded sessions in the specified database. Can output in JSON format.

**Method:** CLI Command

**Endpoint:** N/A (CLI command)

**Parameters:**
#### Path Parameters
- **knowledge.mv2** (string) - Required - The database file to list sessions from.
#### Query Parameters
- **--json** (boolean) - Optional - Outputs the session list in JSON format.

**Command Example:**
```bash
# List all recorded sessions
memvid session list knowledge.mv2

# JSON output
memvid session list knowledge.mv2 --json
```

### Replay Session

**Description:** Replays a recorded session with specified parameters to observe results.

**Method:** CLI Command

**Endpoint:** N/A (CLI command)

**Parameters:**
#### Path Parameters
- **knowledge.mv2** (string) - Required - The database file containing the session.
- **<session-id>** (string) - Required - The ID of the session to replay.
#### Query Parameters
- **--adaptive** (boolean) - Optional - Enable adaptive retrieval.
- **--top-k** (integer) - Optional - Override the top-k value for searches.
- **--strategy** (string) - Optional - Specify the adaptive strategy (e.g., `elbow`, `cliff`, `relative`, `combined`).
- **--verbose** (boolean) - Optional - Show detailed replay output.
- **--json** (boolean) - Optional - Output results as JSON.

**Command Example:**
```bash
# Replay with default parameters
memvid session replay knowledge.mv2 <session-id>

# Replay with adaptive retrieval enabled
memvid session replay knowledge.mv2 <session-id> --adaptive

# Replay with different top-k
memvid session replay knowledge.mv2 <session-id> --top-k 20

# Replay with different strategy
memvid session replay knowledge.mv2 <session-id> --adaptive --strategy elbow
```

### Delete Session

**Description:** Deletes a specified session from the database.

**Method:** CLI Command

**Endpoint:** N/A (CLI command)

**Parameters:**
#### Path Parameters
- **knowledge.mv2** (string) - Required - The database file containing the session.
- **<session-id>** (string) - Required - The ID of the session to delete.
#### Query Parameters
- **--yes** (boolean) - Optional - Skips the confirmation prompt before deletion.

**Command Example:**
```bash
# Delete a session
memvid session delete knowledge.mv2 <session-id>

# Skip confirmation
memvid session delete knowledge.mv2 <session-id> --yes
```
```

--------------------------------

### Python: Testing PII Detection Patterns

Source: https://docs.memvid.com/concepts/pii-masking

Provides a Python example for testing PII detection capabilities. It defines a list of test strings and iterates through them, using `memvid.detect_pii` to find and print any PII items detected in each string.

```python
from memvid import detect_pii

# Test with your actual data patterns
test_data = [
    "Contact: support@yourcompany.com",
    "Phone: 1-800-YOUR-NUM",
    "Account: CUST-12345",
]

for text in test_data:
    pii = detect_pii(text)
    print(f"Found: {len(pii)} PII items in: {text}")
```

--------------------------------

### Python: Memvid Search for RAG Context Building

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Provides a Python example of using Memvid to retrieve a specific number of chunks for building a Retrieval-Augmented Generation (RAG) context, emphasizing the use of `adaptive=False` for fixed context sizes.

```python
# Need consistent counts for comparison
results_a = mem.find("topic A", adaptive=False, top_k=50)
results_b = mem.find("topic B", adaptive=False, top_k=50)

coverage_a = len([r for r in results_a if r.score > 0.5])
coverage_b = len([r for r in results_b if r.score > 0.5])

# Need exactly 5 chunks for context
results = mem.find(
    query,
    adaptive=False,
    top_k=5
)
context = "\n".join([r.text for r in results])
```

--------------------------------

### Migrating from ChromaDB to Memvid

Source: https://docs.memvid.com/comparisons/vector-databases

Presents a Python script for migrating data from ChromaDB to Memvid. It shows how to connect to ChromaDB, retrieve documents and metadata, create a Memvid memory, and then populate it with the migrated data.

```python
import chromadb
from memvid_sdk import create

# Export from ChromaDB
client = chromadb.Client()
collection = client.get_collection("my-collection")
results = collection.get(include=["documents", "metadatas"])

# Create Memvid memory
mem = create("knowledge.mv2")

# Migrate documents
for doc, meta in zip(results["documents"], results["metadatas"]):
    mem.put(
        title=meta.get("title", "Untitled"),
        label="migrated",
        metadata=meta,
        text=doc
    )

mem.seal()
```

--------------------------------

### Visual Search Query Example (Python)

Source: https://docs.memvid.com/concepts/visual-embeddings

This Python snippet illustrates how to perform a visual search query using an embedded text query. It shows initializing an OpenAI CLIP provider, generating an embedding for a specific query ('pie chart showing market share'), and then using this embedding with a memory search function to find relevant results.

```python
# Find pages with specific visual elements
clip = get_clip_provider('openai')
query = clip.embed_text('pie chart showing market share')

# Use with memory search
results = mem.find('market share', k=10)
```

--------------------------------

### Run Gemini Agent with Memvid (JavaScript)

Source: https://docs.memvid.com/frameworks/google-adk

This JavaScript snippet demonstrates how to initialize a Gemini client with ADK tools, start a chat session, and process user messages iteratively, handling function calls to the memvid_put and memvid_find executors. It requires setting the GEMINI_API_KEY environment variable.

```javascript
await executors.memvid_put({
        title: 'Agent Development Kit',
        label: 'frameworks',
        text: 'ADK is Google\'s framework for building AI agents.',
      });

      // Create Gemini client
      const geminiKey = process.env.GEMINI_API_KEY ?? process.env.GOOGLE_API_KEY;
      if (!geminiKey) throw new Error("Set GEMINI_API_KEY (or legacy GOOGLE_API_KEY)");
      const genAI = new GoogleGenerativeAI(geminiKey);
      const model = genAI.getGenerativeModel({
        model: 'gemini-2.0-flash',
        tools: [{ functionDeclarations: tools }],
        systemInstruction: 'You are a helpful assistant with access to a knowledge base. ' + 
          'Use memvid_find to search and memvid_ask to answer questions.',
      });

      // Run agentic loop
      const chat = model.startChat();

      async function processMessage(userMessage: string): Promise<string> {
        let result = await chat.sendMessage(userMessage);

        // Handle function calls iteratively
        while (true) {
          const parts = result.response.candidates?.[0]?.content?.parts || [];
          const functionCalls = parts.filter((p: any) => p.functionCall);

          if (functionCalls.length === 0) {
            // No more function calls, return text response
            return result.response.text() || 'No response';
          }

          // Execute all function calls
          const responses: any[] = [];
          for (const part of functionCalls) {
            const { name, args } = (part as any).functionCall;
            if (executors[name]) {
              const funcResult = await executors[name](args);
              responses.push({
                functionResponse: { name, response: { result: funcResult } },
              });
            }
          }

          // Send results back
          result = await chat.sendMessage(responses);
        }
      }

      // Example conversation
      const answer = await processMessage(
        'What is Google ADK and how does it relate to Gemini?'
      );
      console.log('Agent response:', answer);

      await mem.seal();
```

--------------------------------

### Memvid Lexical Search Examples

Source: https://docs.memvid.com/cli/search-and-ask

Demonstrates using the lexical search mode ('lex') for exact keyword matching, technical terms, function names, and date range filtering. This mode is best for precise queries where specific terms are important.

```bash
# Find exact keyword
memvid find knowledge.mv2 --query "WebAuthn" --mode lex

# Technical error codes
memvid find knowledge.mv2 --query "ERR_CONNECTION_REFUSED" --mode lex

# Function names
memvid find knowledge.mv2 --query "handleAuthentication" --mode lex

# Date range filtering
memvid find knowledge.mv2 --query "date:[2024-01-01 TO 2024-12-31]" --mode lex
```

--------------------------------

### Dockerfile for Memvid Application

Source: https://docs.memvid.com/examples/chatbot-memory

Provides a Dockerfile to containerize the Memvid application. It uses a Python 3.11 slim image, sets the working directory, installs dependencies from requirements.txt, copies the application code, and sets the command to run the Uvicorn server.

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

--------------------------------

### Memvid CLI: Lexical Search Examples

Source: https://docs.memvid.com/concepts/no-vec-mode

Illustrates practical use cases for Memvid's BM25 lexical search, demonstrating how to find exact code snippets, specific error codes, and precise terms within documentation. This highlights the effectiveness of lexical search for targeted queries where exact matches are crucial.

```bash
# Finding exact code
memvid find code.mv2 --query "handleAuthentication"

# Finding error codes
memvid find logs.mv2 --query "ERROR_404_NOT_FOUND"

# Finding specific terms
memvid find docs.mv2 --query "API rate limit"
```

--------------------------------

### Verify memvid File Integrity with Bash

Source: https://docs.memvid.com/errors/troubleshooting

This snippet demonstrates how to verify the integrity of a memvid file using the 'verify' command and check its header with 'xxd'.

```bash
memvid verify knowledge.mv2 --deep
xxd knowledge.mv2 | head -5
```

--------------------------------

### Ask a question with citations

Source: https://docs.memvid.com/sdks/cli

Asks a question against a memvid memory file, performing a hybrid search and synthesizing an answer. It includes citations by returning the top 'k' results in JSON format. This example asks about 'What changed last release?'

```bash
memvid ask data.mv2 --question "What changed last release?" --top-k 6 --mode hybrid --json
```

--------------------------------

### Python SDK Usage

Source: https://docs.memvid.com/concepts/memory-cards

Demonstrates how to use the Memvid Python SDK to interact with memory cards. Covers enriching documents, getting entity states, retrieving facts, and querying preferences and memory timelines.

```python
from memvid import use

mem = use('basic', 'memory.mv2')

# Enrich documents
mem.enrich(engine="groq")

# Get current entity state (O(1) lookup)
john = mem.get_entity_state("John Smith")
print(f"Job: {john['job_title']}")
print(f"Team: {john['team']}")

# Get all facts for an entity
facts = mem.get_facts(entity="John Smith")
for fact in facts:
    print(f"{fact.slot}: {fact.value} (from {fact.source_frame})")

# Get fact history for specific attribute
title_history = mem.get_facts(
    entity="John Smith",
    predicate="job_title"
)
for fact in title_history:
    print(f"{fact.extracted_at}: {fact.value}")

# Query preferences
prefs = mem.get_preferences(entity="user")
print(f"Preferred language: {prefs.get('preferred_language')}")

# Get memory timeline
timeline = mem.get_memory_timeline(entity="Project Alpha")
for event in timeline:
    print(f"{event.timestamp}: {event.slot} = {event.value}")
```

--------------------------------

### Bash: Encrypting Data and Masking Output

Source: https://docs.memvid.com/concepts/pii-masking

Shows a workflow combining data encryption at rest with PII masking on output for maximum protection. This Bash example encrypts a file, decrypts it to a temporary file for processing, masks the output, and then re-encrypts the sensitive data.

```bash
# Encrypt at rest + mask on output
memvid lock sensitive.mv2 --out sensitive.mv2e

# When using:
memvid unlock sensitive.mv2e --out temp.mv2
memvid ask temp.mv2 -q "..." --mask-pii
memvid lock temp.mv2 --out sensitive.mv2e
rm temp.mv2
```

--------------------------------

### Diagnose Empty/Missing Tools in Memvid Adapter

Source: https://docs.memvid.com/errors/troubleshooting

This Python snippet demonstrates how to diagnose issues where `mem.tools` returns empty or None when using a framework adapter. It helps identify if the adapter is loaded correctly and what type of object is returned. Ensure the correct adapter name is used.

```python
from memvid_sdk import use

mem = use('langchain', 'knowledge.mv2')
print(f"Tools: {mem.tools}")
print(f"Type: {type(mem.tools)}")
```

--------------------------------

### Record, Replay, and Manage Agent Sessions with Memvid SDK

Source: https://docs.memvid.com/sdks/node

This snippet demonstrates how to use the Memvid SDK to start, perform operations within, checkpoint, end, replay, and delete agent sessions. It highlights session recording for debugging purposes and replaying sessions with specific parameters. The output includes the number of actions recorded and the match rate during replay.

```typescript
import { MemvidClient } from '@memvid/sdk';

const mem = new MemvidClient();

async function manageSession() {
  try {
    // Start recording
    const sessionId = await mem.sessionStart('Debug Session');
    console.log(`Session started with ID: ${sessionId}`);

    // Perform operations (all recorded)
    await mem.put({ title: 'Notes', text: 'Content...' });
    await mem.find('test query');

    // Add checkpoint
    await mem.sessionCheckpoint();
    console.log('Checkpoint added.');

    // End session
    const summary = await mem.sessionEnd();
    console.log(`Recorded ${summary.actionCount} actions`);

    // Replay with different parameters
    const replay = await mem.sessionReplay(sessionId, {
      adaptive: true,
      topK: 20
    });
    console.log(`Replay match rate: ${(replay.matchRate * 100).toFixed(1)}%`);

    // Delete session
    await mem.sessionDelete(sessionId);
    console.log(`Session ${sessionId} deleted.`);
  } catch (error) {
    console.error('An error occurred:', error);
  }
}

manageSession();
```

--------------------------------

### Quick Diagnosis Commands for Memvid

Source: https://docs.memvid.com/errors/troubleshooting

Run these bash commands to quickly diagnose common Memvid issues by checking file health, viewing statistics, and identifying file locks.

```bash
memvid verify knowledge.mv2 --deep
memvid stats knowledge.mv2 --json
lsof knowledge.mv2
```

--------------------------------

### Memvid CLI: Best Practice - Monitoring with JSON Output

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Advises using the `--json` output with `jq` to monitor Memvid's adaptive decisions, helping to understand search behavior and troubleshoot issues.

```bash
memvid find memory.mv2 --query "term" --json | jq '.adaptive_info'
```

--------------------------------

### Verify and Repair Memvid Files

Source: https://docs.memvid.com/node-sdk/examples

Shows how to verify the integrity of a Memvid file and perform repairs if necessary using the Memvid SDK. This function requires the '@memvid/sdk' package and the path to the memory file. It returns the verification status and a repair report.

```typescript
import { use } from '@memvid/sdk';

async function maintenanceExample() {
  // Verify integrity
  const result = await use.verify('notes.mv2', { deep: true }) as {
    overall_status: string;
    checks: unknown[];
  };

  if (result.overall_status === 'passed') {
    console.log('File is valid');
  } else {
    console.log('Issues found:', result.checks);
  }

  // Repair if needed
  const report = await use.doctor('notes.mv2', {
    rebuildTimeIndex: true,
    rebuildLexIndex: true,
    vacuum: true
  });
  console.log('Repair complete:', report);
}
```

--------------------------------

### Verify Memvid Storage Integrity

Source: https://docs.memvid.com/concepts/performance-tuning

Check Memvid storage files for corruption. Perform a quick check or a deep check for more thorough verification.

```bash
# Quick check
memvid verify memory.mv2

# Deep check
memvid verify memory.mv2 --deep
```

--------------------------------

### Python: Batch Ingestion with External Embedder

Source: https://docs.memvid.com/concepts/embedding-models

Example demonstrating the core workflow of using an external embedding model (like OpenAI) for batch ingestion into Memvid. It highlights passing the embedder instance during `put_many`.

```python
from memvid_sdk import create
from memvid_sdk.embeddings import OpenAIEmbeddings

```

--------------------------------

### Inspect Lock State

Source: https://docs.memvid.com/troubleshooting/cli

Inspects the lock state of a memvid file. This includes checking who currently holds the lock and requesting its release.

```bash
# Who holds the lock?
memvid who myfile.mv2

# Request release
memvid nudge myfile.mv2
```

--------------------------------

### LangChain Integration with Memvid SDK (Python)

Source: https://docs.memvid.com/resources/examples

Integrates Memvid with LangChain for building applications. This example shows how to use Memvid as a retriever within a LangChain RetrievalQA chain. It requires memvid_sdk and langchain_openai.

```python
from memvid_sdk import use
from langchain_openai import ChatOpenAI

mem = use('langchain', 'knowledge.mv2')
retriever = mem.as_retriever(k=5)

# Use in a chain
from langchain.chains import RetrievalQA
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    retriever=retriever
)
```

--------------------------------

### Memvid Semantic Search Examples

Source: https://docs.memvid.com/cli/search-and-ask

Illustrates semantic search mode ('sem') for natural language queries, conceptual searches, and finding similar content. This mode is ideal for understanding the meaning and context of queries rather than just keywords.

```bash
# Natural language question
memvid find knowledge.mv2 --query "how do users log in" --mode sem

# Conceptual search
memvid find knowledge.mv2 --query "best practices for security" --mode sem

# Find similar content
memvid find knowledge.mv2 --query "machine learning model training" --mode sem
```

--------------------------------

### Access Memvid Files with Python SDK

Source: https://docs.memvid.com/faq/security-and-compliance

Python code examples for opening Memvid files in read-only or exclusive write modes. Demonstrates concurrent reader access and exclusive writer access.

```python
from memvid_sdk import use

# Reader (concurrent access OK)
mem = use('basic', 'knowledge.mv2', read_only=True)

# Writer (exclusive access)
mem = use('basic', 'knowledge.mv2')
```

--------------------------------

### Memvid Node.js SDK Usage

Source: https://docs.memvid.com/index

Illustrates how to use the Memvid Node.js SDK to create a knowledge base, add entries, perform searches, and get entity states programmatically.

```typescript
import { create } from '@memvid/sdk';

const mem = await create('knowledge.mv2');
await mem.put({ title: 'Team Info', label: 'notes', text: 'Alice works at Anthropic...' });
const results = await mem.find('who works at AI companies', { k: 5 });
const alice = await mem.state('Alice');
// { slots: { employer: 'Anthropic', role: 'Senior Engineer' } }
```

--------------------------------

### MemVid Answer Caching Example

Source: https://docs.memvid.com/concepts/time-travel-replay

Illustrates the efficiency of MemVid's answer caching. The first 'ask' command triggers an LLM call with token and cost details. A subsequent identical 'ask' command returns a cached result instantly, showing 'cached: true' and zero cost, highlighting the saved resources.

```bash
# First call - hits LLM
memvid ask knowledge.mv2 --question "What is the revenue?" --use-model openai
# tokens: 2500 input + 50 output   cost: $0.00042

# Second call - cached
memvid ask knowledge.mv2 --question "What is the revenue?" --use-model openai
# cached: true   cost: $0.00 (saved $0.00042)
```

--------------------------------

### Apply Scope Filtering in Memvid Search

Source: https://docs.memvid.com/concepts/performance-tuning

Narrows down the search scope to specific directories or documents, leading to faster retrieval of relevant results.

```bash
# Search only in specific directory
memvid find memory.mv2 --query "config" --scope "src/config/"

# Search specific document
memvid find memory.mv2 --query "api key" --uri "docs/security.md"
```

--------------------------------

### Rebuild Memvid Indexes

Source: https://docs.memvid.com/concepts/performance-tuning

Periodically rebuild Memvid indexes to ensure optimal search performance. Rebuilding is recommended after significant data deletions or if search results degrade.

```bash
# Rebuild all indexes
memvid doctor memory.mv2 --rebuild-lex-index --rebuild-vec-index --rebuild-time-index

# Rebuild specific index
memvid doctor memory.mv2 --rebuild-vec-index
```

--------------------------------

### Monitor Memvid Usage Statistics

Source: https://docs.memvid.com/concepts/performance-tuning

Track Memvid usage patterns and monitor system statistics for performance analysis. Commands are available for viewing plan details and raw statistics in JSON format.

```bash
# View usage statistics
memvid plan show

# JSON format for monitoring
memvid stats memory.mv2 --json
```

--------------------------------

### Graph Traversal in Memvid (Bash)

Source: https://docs.memvid.com/cli/cheat-sheet

This section is intended for commands related to graph traversal within Memvid memories, though specific examples are not provided in the input text.

```bash
# Placeholder for graph traversal commands
# Example: memvid graph traverse memory.mv2 --start-node <node_id> --depth 3
```

--------------------------------

### Sharing Encrypted Memvid Files Securely

Source: https://docs.memvid.com/concepts/encryption

This example illustrates the process of sharing encrypted Memvid files. The sender encrypts the file and sends it, while the password must be communicated through a separate, secure channel. The recipient can then unlock and access the data.

```bash
# Sender
memvid lock shared-docs.mv2 --out shared-docs.mv2e
# Send shared-docs.mv2e via email/cloud
# Send password via separate secure channel

# Recipient
memvid unlock shared-docs.mv2e --out shared-docs.mv2
memvid find shared-docs.mv2 --query "search"
```

--------------------------------

### Memvid LLM-Powered Q&A with Sources (CLI)

Source: https://docs.memvid.com/index

Illustrates how to ask natural language questions using Memvid CLI, specifying an LLM model and receiving answers with cited sources.

```bash
export OPENAI_API_KEY=sk-...
memvid ask knowledge.mv2 --question "What is Alice's role?" --use-model openai
# Answer: Alice is a Senior Engineer at Anthropic in San Francisco.
# Sources: [Meeting Notes, Team Directory]
```

--------------------------------

### View Frame Details

Source: https://docs.memvid.com/troubleshooting/cli

Views details of frames within a memvid file. Supports viewing by frame ID or URI, with options for JSON output and media preview.

```bash
# View by frame ID
memvid view myfile.mv2 --frame-id 1

# View by URI
memvid view myfile.mv2 --uri "mv2://docs/readme.md"

# JSON output
memvid view myfile.mv2 --frame-id 1 --json

# Preview media
memvid view myfile.mv2 --frame-id 1 --preview
```

--------------------------------

### Potential Collisions with Short Content (Bash)

Source: https://docs.memvid.com/concepts/deduplication

This example highlights a scenario where very short or structurally similar content might be flagged as a near-duplicate due to SimHash limitations. Putting 'yes' and 'no' might result in a collision, demonstrating the need for distinguishing context or unique URIs for such cases.

```bash
# Very short content may collide
echo "yes" | memvid put memory.mv2 --input -
echo "no" | memvid put memory.mv2 --input -  # Might be seen as near-duplicate
```

--------------------------------

### Analyze Memvid Memory Statistics

Source: https://docs.memvid.com/concepts/performance-tuning

Obtain detailed statistics about Memvid memory usage, including frame counts, size breakdowns for different indexes, and average query times.

```bash
# Detailed stats
memvid stats memory.mv2

# Output example:
# Frames: 10,234
# Size: 45.2 MB
# Vector index: 23.1 MB
# Lexical index: 8.4 MB
# Avg query time: 12ms
```

--------------------------------

### Troubleshoot Memvid Search No Results

Source: https://docs.memvid.com/errors/troubleshooting

Learn how to troubleshoot when Memvid search returns no results. This includes bash commands to check file content and indices, and Python snippets to specify search modes.

```bash
# Check if file has content
memvid stats knowledge.mv2

# Check which indices are enabled
memvid info knowledge.mv2

# Try different search modes
memvid find knowledge.mv2 --query "test" --mode lex
memvid find knowledge.mv2 --query "test" --mode sem
```

```bash
# Check if file has content
memvid timeline knowledge.mv2 --limit 10
```

```python
# Exact keyword match
results = mem.find('exact phrase', mode='lex')

# Semantic/conceptual match
results = mem.find('related concept', mode='sem')
```

```bash
# Rebuild indices if they're corrupted
memvid doctor knowledge.mv2 --rebuild-lex-index --rebuild-vec-index

# Check embeddings were enabled during ingestion
memvid put knowledge.mv2 --input doc.pdf --embeddings
```

--------------------------------

### Skip Embeddings During Memvid Ingestion

Source: https://docs.memvid.com/concepts/performance-tuning

Accelerates ingestion and reduces file size by omitting vector embeddings, enabling lexical-only search. Embeddings can be added later.

```bash
# No vector embeddings (lexical only)
memvid create memory.mv2 --no-vec
memvid put memory.mv2 --input docs/

# Or skip per-ingestion
memvid put memory.mv2 --input logs.txt --embedding-skip
```

--------------------------------

### Gemini Entity Extraction Setup

Source: https://docs.memvid.com/concepts/entity-extraction

Configure and utilize Google's Gemini for entity extraction with the Memvid SDK. This involves setting the API key and then instantiating the extractor, either via a factory function or direct instantiation, specifying desired entity types and models. Input is text, output is a list of extracted entities.

```bash
export GEMINI_API_KEY=your-key-here
```

```python
from memvid_sdk.entities import get_entity_extractor, GeminiEntities

# Using factory
ner = get_entity_extractor('gemini', entity_types=['COMPANY', 'PERSON'])

# With specific model
ner = get_entity_extractor('gemini:gemini-2.0-flash', entity_types=['COMPANY'])

entities = ner.extract(text, min_confidence=0.5)
```

--------------------------------

### Optimize Memvid Ingestion with Python SDK

Source: https://docs.memvid.com/concepts/performance-tuning

Improve Memvid ingestion throughput using the Python SDK by reusing memory instances and leveraging asynchronous operations or batching.

```python
from memvid import use

# Reuse memory instance
mem = use('basic', 'memory.mv2')

# Batch operations
texts = [...] 
for text in texts:
    mem.put(text)  # Batched internally

# Async for better throughput
import asyncio
from memvid import use_async

async def main():
    mem = await use_async('basic', 'memory.mv2')
    results = await asyncio.gather(*[
        mem.find(q) for q in queries
    ])
```

--------------------------------

### Perform Regular Maintenance

Source: https://docs.memvid.com/cli/maintenance-and-tickets

Outlines best practices for maintaining Memvid knowledge bases, including periodic verification, vacuuming after deletions, and monitoring storage capacity to prevent issues.

```bash
# Verify periodically
memvid verify --deep

# Vacuum after deletions
memvid doctor --vacuum

# Monitor capacity
memvid stats
```

--------------------------------

### Optimize Memvid Operations with Node.js SDK

Source: https://docs.memvid.com/concepts/performance-tuning

Enhance Memvid performance in Node.js by reusing memory instances, executing parallel searches, and streaming large results.

```typescript
import { use } from '@anthropics/memvid'

// Reuse memory instance
const mem = await use('basic', 'memory.mv2')

// Parallel searches
const results = await Promise.all(
  queries.map(q => mem.find(q))
)

// Stream large results
for await (const chunk of mem.findStream(query)) {
  process.stdout.write(chunk)
}
```

--------------------------------

### Memvid SDK Type Hinting in Python

Source: https://docs.memvid.com/sdks/python

Illustrates the use of type hints in Python for the Memvid SDK to improve code readability and enable better IDE support. The example shows a function `process_memory` that takes a file path and returns a dictionary of results obtained from the `mem.find` method.

```python
from typing import Dict, Any

def process_memory(path: str) -> Dict[str, Any]:
    mem = memvid.use('basic', path)
    results: Dict[str, Any] = mem.find('query')
    return results
```

--------------------------------

### Node.js SDK Usage

Source: https://docs.memvid.com/concepts/memory-cards

Illustrates the usage of the Memvid Node.js SDK for interacting with memory cards programmatically. Includes examples for enriching documents, retrieving entity states, and fetching facts filtered by entity and predicate.

```typescript
import { use } from '@anthropics/memvid'

const mem = await use('basic', 'memory.mv2')

// Enrich documents
await mem.enrich({ engine: "groq" })

// Get current entity state
const john = await mem.getEntityState("John Smith")
console.log(`Job: ${john.job_title}`)
console.log(`Team: ${john.team}`)

// Get all facts for an entity
const facts = await mem.getFacts({ entity: "John Smith" })
for (const fact of facts) {
  console.log(`${fact.slot}: ${fact.value}`)
}

// Query by predicate
const titles = await mem.getFacts({
  entity: "John Smith",
  predicate: "job_title"
})
```

--------------------------------

### Get Graph Statistics with Logic Mesh (Bash)

Source: https://docs.memvid.com/concepts/graph-search

Provides summary statistics about the extracted knowledge graph, including counts of entities and relationships, and graph density.

```bash
memvid follow stats memory.mv2
```

--------------------------------

### Search Memvid Knowledge Base with TypeScript

Source: https://docs.memvid.com/frameworks/vercel-ai

This snippet shows how to initialize the Memvid knowledge base and use it to generate text based on a prompt. It demonstrates setting up the model, providing tools from the knowledge base, and handling potential errors by ensuring the memory is closed. The `query` and `top_k` parameters are implicitly used by the `mem.tools` when `generateText` is called with a relevant prompt.

```typescript
const mem = await use('vercel-ai', 'knowledge.mv2');
try {
  // Use tools...
  const result = await generateText({
    model: openai('gpt-4o-mini'),
    tools: mem.tools,
    prompt: 'Search for...', 
  });
} finally {
  await mem.seal();
}
```

--------------------------------

### Knowledge Base with Entity Extraction using Memvid SDK (Python)

Source: https://docs.memvid.com/resources/examples

Creates a searchable knowledge repository using the Memvid SDK with entity extraction. This example demonstrates ingesting documents, extracting entities (PERSON, ORG, TOPIC), and then searching the knowledge base by entity. Requires memvid_sdk and its entities module.

```python
from memvid_sdk import create
from memvid_sdk.entities import get_entity_extractor

mem = create('knowledge.mv2')
ner = get_entity_extractor('openai', entity_types=['PERSON', 'ORG', 'TOPIC'])

# Ingest with entity extraction
for doc in documents:
    entities = ner.extract(doc.text)
    mem.put({
        'title': doc.title,
        'text': doc.text,
        'metadata': {'entities': entities}
    })

# Search by entity
results = mem.find('Microsoft', k=10)
```

--------------------------------

### Configure Search Mode in Memvid

Source: https://docs.memvid.com/concepts/performance-tuning

Select the search mode to optimize for speed or relevance. 'lex' is fastest for exact matches, 'sem' is for conceptual queries, and 'auto' provides a balanced hybrid approach.

```bash
# Lexical only (fastest)
memvid find memory.mv2 --query "handleAuth" --mode lex

# Semantic only
memvid find memory.mv2 --query "authentication logic" --mode sem

# Hybrid (default)
memvid find memory.mv2 --query "auth" --mode auto
```

--------------------------------

### Verify Single File Integrity

Source: https://docs.memvid.com/troubleshooting/cli

Verifies the integrity of a single memvid file. This command checks for consistency and potential corruption within the specified .mv2 file.

```bash
memvid verify-single-file myfile.mv2
```

--------------------------------

### Use Memvid as a Query Engine (Python)

Source: https://docs.memvid.com/frameworks/llamaindex

Sets up and utilizes Memvid as a query engine in Python with LlamaIndex. This example shows initialization in read-only mode, obtaining the query engine, executing a query, and accessing source nodes.

```python
from memvid_sdk import use

# Initialize
mem = use('llamaindex', 'knowledge.mv2', read_only=True)

# Get query engine
query_engine = mem.as_query_engine()

# Query
response = query_engine.query("What are the best practices?")
print(response.response)

# Access sources
for source in response.source_nodes:
    print(f"Source: {source.node.metadata.get('title')}")
```

--------------------------------

### Python: Mask PII Before Logging

Source: https://docs.memvid.com/concepts/pii-masking

Provides a Python example for the best practice of masking PII before logging messages. This function `safe_log` takes a message, masks any PII within it using `mask_pii`, and then logs the sanitized message.

```python
import logging

def safe_log(message):
    logging.info(mask_pii(message))
```

--------------------------------

### Get entity state

Source: https://docs.memvid.com/sdks/cli

Retrieves the current state of an entity within the memvid memory. This operation is optimized for O(1) time complexity and outputs the state in JSON format.

```bash
# Example: memvid state data.mv2 --entity "ObjectName" --json
```

--------------------------------

### Local NER Extraction with Python SDK

Source: https://docs.memvid.com/concepts/entity-extraction

Demonstrates using the Memvid Python SDK for local entity extraction with the DistilBERT-NER model. It shows how to initialize the extractor either through a factory function or direct instantiation and provides an example of extracting entities from a sentence.

```python
from memvid_sdk.entities import get_entity_extractor, LocalNER

# Using factory
ner = get_entity_extractor('local')

# Or direct instantiation
ner = LocalNER(model='distilbert-ner')

# Extract entities
entities = ner.extract("Apple CEO Tim Cook visited Paris headquarters.")
# [
#   {'name': 'Apple', 'type': 'ORG', 'confidence': 0.98},
#   {'name': 'Tim Cook', 'type': 'PERSON', 'confidence': 0.97},
#   {'name': 'Paris', 'type': 'LOCATION', 'confidence': 0.95},
# ]
```

--------------------------------

### Compact Storage with Memvid

Source: https://docs.memvid.com/concepts/performance-tuning

Reclaim space in Memvid storage files after deletions or updates. The `--vacuum` command compacts the storage, while additional flags enable full optimization including index rebuilding.

```bash
# Compact storage
memvid doctor memory.mv2 --vacuum

# Full optimization
memvid doctor memory.mv2 --vacuum --rebuild-lex-index --rebuild-vec-index
```

--------------------------------

### Enable Parallel Ingestion in Memvid

Source: https://docs.memvid.com/concepts/performance-tuning

Speeds up the ingestion process for large folders by processing multiple files concurrently. Can be combined with `--embedding-skip` for maximum ingestion speed.

```bash
# Process multiple files concurrently
memvid put memory.mv2 --input ./large-folder/ --parallel-segments

# Combine with embedding skip for fastest ingestion
memvid put memory.mv2 --input ./logs/ --embedding-skip --parallel-segments
```

--------------------------------

### Memvid Memory File Creation JSON Output (JSON)

Source: https://docs.memvid.com/cli/create-and-put

Example JSON output when creating a Memvid memory file with the `--json` flag. Includes file path, size limit, and index status.

```json
{
  "path": "my-memory.mv2",
  "size_limit_bytes": 536870912,
  "lex_enabled": true,
  "vec_enabled": true,
  "created_at": "2024-01-15T10:30:00Z"
}
```

--------------------------------

### Build Sketch Index in Memvid

Source: https://docs.memvid.com/concepts/performance-tuning

Creates a sketch index for very large memories (100k+ frames) to significantly speed up approximate search queries. Different variants offer tradeoffs between build time, query speed, and accuracy.

```bash
# Build sketch index
memvid sketch build memory.mv2 --variant medium

# Check sketch status
memvid sketch info memory.mv2
```

--------------------------------

### Vercel AI SDK Integration with Memvid SDK (TypeScript)

Source: https://docs.memvid.com/resources/examples

Demonstrates integrating Memvid with the Vercel AI SDK for generating text, leveraging Memvid's tools for information retrieval. This example requires @memvid/sdk and ai libraries.

```typescript
import { use } from '@memvid/sdk';
import { generateText } from 'ai';

const mem = await use('vercel-ai', 'knowledge.mv2');

const result = await generateText({
  model: openai('gpt-4o'),
  tools: mem.tools,
  prompt: 'Search for information about authentication'
});
```

--------------------------------

### Monitor Memvid Memory Capacity

Source: https://docs.memvid.com/cli/tickets-and-capacity

Provides commands to check current memory usage, plan limits, and includes a script example for alerting when memory capacity exceeds a certain threshold.

```bash
# Check current usage
memvid tickets list project.mv2

# Check plan limits
memvid plan show

# Script to alert on low capacity
USAGE=$(memvid tickets list project.mv2 --json | jq '.usage.usage_percent')
if (( $(echo "$USAGE > 80" | bc -l) )); then
  echo "Warning: Memory at ${USAGE}% capacity"
fi
```

--------------------------------

### Resolve Memvid CapacityExceeded Errors

Source: https://docs.memvid.com/errors/troubleshooting

Guidance on resolving 'CapacityExceeded' errors in Memvid. This includes bash commands to check current usage, manage tickets, delete old content, and rebuild the file.

```bash
# Check current usage
memvid stats knowledge.mv2 --json | jq '.size_bytes, .capacity_bytes'

# Check ticket info
memvid tickets list knowledge.mv2
```

```bash
# Upgrade your plan for more capacity
memvid tickets sync knowledge.mv2 --memory-id YOUR_ID

# Delete old content
memvid delete knowledge.mv2 --before "2024-01-01" --yes

# Vacuum to reclaim space
memvid doctor knowledge.mv2 --vacuum

# Archive and create new file
mv knowledge.mv2 archive/knowledge-$(date +%Y%m%d).mv2
memvid create knowledge.mv2
```

--------------------------------

### Clean Up Stale Files

Source: https://docs.memvid.com/troubleshooting/cli

Cleans up stale files that may be leftover from older versions of memvid. This involves checking for sidecar files, removing them, and then verifying the clean state.

```bash
# Check for sidecars
ls -la myfile.mv2*

# Remove any leftover files
rm -f myfile.mv2-wal myfile.mv2-shm myfile.mv2.lock

# Verify clean state
memvid verify-single-file myfile.mv2
```

--------------------------------

### Basic Usage: Search Tool with AutoGen (Python)

Source: https://docs.memvid.com/frameworks/autogen

Demonstrates how to initialize Memvid with the AutoGen adapter, retrieve a search tool, and configure an AssistantAgent to use this tool for knowledge retrieval within an AutoGen conversation.

```python
from autogen import AssistantAgent, UserProxyAgent
from memvid_sdk import use

# Initialize with autogen adapter
mem = use('autogen', 'knowledge.mv2', read_only=True)

# Get the search tool
search_tool = mem.get_search_tool()

# Create assistant with Memvid knowledge
assistant = AssistantAgent(
    name="assistant",
    llm_config={
        "model": "gpt-4o",
        "functions": [search_tool.schema]
    },
    system_message="You have access to a knowledge base. Use search_knowledge to find information."
)

# Register the function
assistant.register_function(
    function_map={search_tool.name: search_tool.func}
)

# Start conversation
user_proxy = UserProxyAgent(name="user", human_input_mode="NEVER")
user_proxy.initiate_chat(
    assistant,
    message="Find information about authentication and summarize it"
)
```

--------------------------------

### Configure Embedding Options

Source: https://docs.memvid.com/cli/create-and-put

Demonstrates how to enable semantic embeddings, select embedding models (local or OpenAI), and apply vector compression for reduced storage.

```bash
# Use built-in BGE (default, no API key needed)
memvid put knowledge.mv2 --input docs/ --embedding

# Use OpenAI embeddings
export OPENAI_API_KEY=sk-...
memvid put knowledge.mv2 --input docs/ --embedding -m openai-small

# Use OpenAI large model for higher quality
memvid put knowledge.mv2 --input docs/ --embedding -m openai-large
```

--------------------------------

### TypeScript Vercel AI SDK Integration for Streaming

Source: https://docs.memvid.com/frameworks/overview

Shows how to use the Memvid adapter with the Vercel AI SDK in TypeScript for Next.js applications. This example demonstrates setting up a POST endpoint that streams text responses, leveraging Memvid's tools for AI models. It requires `@memvid/sdk` and `ai` packages.

```typescript
import { use } from '@memvid/sdk';
import { streamText } from 'ai';

const mem = await use('vercel-ai', 'knowledge.mv2');

export async function POST(req: Request) {
  const result = await streamText({
    model: openai('gpt-4o'),
    tools: mem.tools,
    messages: await req.json(),
  });
  return result.toDataStreamResponse();
}
```

--------------------------------

### Example of Low Grounding in JSON Output

Source: https://docs.memvid.com/cli/search-and-ask

Presents a sample JSON output where the 'grounding' object indicates potential hallucination or poor context support. It includes fields like 'has_warning' and 'warning_reason', along with a 'follow_up' object suggesting further actions.

```json
{
  "grounding": {
    "score": 0.15,
    "label": "LOW",
    "sentence_count": 2,
    "grounded_sentences": 0,
    "has_warning": true,
    "warning_reason": "Answer appears to be poorly grounded in context"
  },
  "follow_up": {
    "needed": true,
    "reason": "Answer may not be well-supported by the available context",
    "hint": "This memory contains information about different topics. Try asking about those instead.",
    "available_topics": ["API Reference", "Authentication", "Database Schema"],
    "suggestions": [
      "Tell me about API Reference",
      "Tell me about Authentication",
      "What topics are in this memory?"
    ]
  }
}
```

--------------------------------

### Debug RAG Failures with Memvid

Source: https://docs.memvid.com/cli/advanced-commands

A use case demonstrating how to debug Retrieval Augmented Generation (RAG) failures by starting a session, ingesting data, querying, ending the session, and replaying with adaptive retrieval to identify missed documents.

```bash
# Start recording
memvid session start knowledge.mv2 "Terminology Mismatch Debug"

# Ingest documents
memvid put knowledge.mv2 --text "Databricks acquired Tabular"

# Query with mismatched terminology (fails with top-k)
memvid find knowledge.mv2 "Databricks purchases"

# End recording
memvid session end knowledge.mv2

# Replay with adaptive retrieval
memvid session replay knowledge.mv2 <session-id> --adaptive --verbose
```

--------------------------------

### Python: Initialize and Use Memvid SDK

Source: https://docs.memvid.com/frameworks/google-adk

This Python snippet illustrates initializing the Memvid SDK in read-only mode and setting up the Gemini client. It includes a try-finally block for resource management, emphasizing that the `mem.close()` method should be called to release resources. This is crucial for proper cleanup after SDK usage.

```python
mem = use('google-adk', 'knowledge.mv2', read_only=True)
try:
    # Create agent and run queries
    client = genai.Client()
    # ... use client
finally:
    mem.close()
```

--------------------------------

### PII Masking with Memvid Node.js SDK

Source: https://docs.memvid.com/faq/security-and-compliance

Node.js SDK examples for PII masking in LLM queries and standalone text processing. Includes enabling PII masking for 'ask' operations and using the `maskPii` utility function.

```typescript
import { use, maskPii } from '@memvid/sdk';

const mem = await use('basic', 'knowledge.mv2');

// Enable PII masking for ask queries
const answer = await mem.ask('What are the customer contact details?', {
  model: 'openai:gpt-4o',
  modelApiKey: process.env.OPENAI_API_KEY,
  maskPii: true
});
console.log(answer.answer);

// Standalone PII masking function
const text = 'Contact john@example.com or call 555-123-4567';
const masked = maskPii(text);
// Output: "Contact [EMAIL] or call [PHONE]"
```

--------------------------------

### MemVid Ask Command for Token and Cost Tracking

Source: https://docs.memvid.com/concepts/time-travel-replay

Demonstrates the 'memvid ask' command to query knowledge bases, specify an LLM model, and output results in JSON format. The output includes answer details, token usage, estimated cost, and grounding information.

```bash
memvid ask knowledge.mv2 --question "Summarize the report" \
  --use-model openai:gpt-4o-mini --json
```

--------------------------------

### Optimize Ingestion Speed

Source: https://docs.memvid.com/troubleshooting/cli

Provides various methods to optimize ingestion speed in memvid. This includes using batch operations, enabling parallel segments, skipping embeddings, and using vector compression.

```bash
# Ingest entire directory at once
memvid put myfile.mv2 --input ./docs/

# Enable parallel segments
memvid put myfile.mv2 --input ./docs/ --parallel-segments

# Skip embeddings for faster ingestion
memvid put myfile.mv2 --input ./docs/ --no-embedding

# Use vector compression for smaller files
memvid put myfile.mv2 --input ./docs/ --vector-compression
```

--------------------------------

### Generate chronological timeline

Source: https://docs.memvid.com/sdks/cli

Generates a chronological list of events from a memvid memory file. Allows limiting the number of results, specifying a start (`--since`) or end (`--until`) time, and reversing the order.

```bash
# Example: memvid timeline data.mv2 --limit 50 --since "2023-01-01" --reverse
```

--------------------------------

### Use NVIDIA Embeddings with Memvid Node.js SDK

Source: https://docs.memvid.com/concepts/embedding-models

Configures and utilizes NVIDIA embeddings within the Memvid Node.js SDK. This example demonstrates setting up the `NvidiaEmbeddings` embedder using an `NVIDIA_API_KEY` for semantic search operations.

```typescript
import { create, NvidiaEmbeddings } from '@memvid/sdk';

const mem = await create('knowledge.mv2');
const embedder = new NvidiaEmbeddings({ model: 'nvidia/nv-embed-v1' }); // uses NVIDIA_API_KEY
await mem.putMany([{ title: 'Doc', text: 'Vector search with NVIDIA embeddings.' }], { embedder });
const res = await mem.find('nvidia embeddings', { mode: 'sem', embedder });
```

--------------------------------

### Mask Real Data for Development/Testing (Python SDK)

Source: https://docs.memvid.com/concepts/pii-masking

This Python example demonstrates how to export masked data for development and testing environments using the Memvid SDK. It iterates through the timeline, masks content using `mask_pii`, and puts the sanitized content into a development memory instance.

```python
# Export masked data for dev/test
for frame in mem.timeline():
    masked_content = mask_pii(frame.text)
    dev_mem.put(masked_content)
```

--------------------------------

### Create Memvid Chatbot Class with Langchain and OpenAI

Source: https://docs.memvid.com/examples/chatbot-memory

Defines the core MemvidChatbot class, initializing conversation memory and an OpenAI language model. It includes methods for processing user messages, retrieving context, and adding new knowledge to the memory. The system prompt guides the AI's behavior.

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage

class MemvidChatbot:
    def __init__(self, memory_path: str = "chatbot-memory.mv2"):
        self.memory = ConversationMemory(memory_path)
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.7)

        self.system_prompt = """You are a helpful AI assistant with access to a knowledge base and conversation history.

When answering questions:
1. Use the provided context to give accurate, relevant answers
2. Reference specific information from the knowledge base when applicable
3. Remember details from previous conversations
4. Be conversational and helpful

If you don't have enough information, say so honestly."""

    def chat(self, user_message: str) -> str:
        """Process a user message and return a response."""

        # Store the user message
        self.memory.add_message("user", user_message)

        # Get relevant context from memory
        context = self.memory.get_relevant_context(user_message)

        # Build the prompt with context
        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=f"""Context from knowledge base and previous conversations:
{context}

---

User message: {user_message}

Please respond helpfully based on the context above.""")
        ]

        # Get response from LLM
        response = self.llm.invoke(messages)
        assistant_message = response.content

        # Store the assistant's response
        self.memory.add_message("assistant", assistant_message)

        return assistant_message

    def add_knowledge(self, title: str, content: str):
        """Add new knowledge to the chatbot's memory."""
        self.memory.mem.put(title, "knowledge", {}, text=content)
        print(f"Added knowledge: {title}")
```

--------------------------------

### List Memvid Models

Source: https://docs.memvid.com/cli/advanced-commands

Lists available and installed models, with options to filter by model type (embedding, clip, ner) or to output in JSON format. This helps users understand which models are ready for use.

```bash
# List all models
memvid models list

# JSON output
memvid models list --json

# Filter by model type
memvid models list --model-type embedding
memvid models list --model-type clip
memvid models list --model-type ner
```

--------------------------------

### Use Memvid as a Retriever in a QA Chain (Python)

Source: https://docs.memvid.com/frameworks/langchain

Explains how to utilize Memvid as a retrieval source within a LangChain RetrievalQA chain in Python. It initializes Memvid in read-only mode, gets the retriever, and sets up the QA chain with an LLM.

```python
from memvid_sdk import use
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# Initialize with langchain adapter
mem = use('langchain', 'knowledge.mv2', read_only=True)

# Get the retriever
retriever = mem.as_retriever(k=5)

# Create QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4o"),
    retriever=retriever
)

result = qa_chain.run("What is the main concept?")
print(result)
```

--------------------------------

### Create Memvid Memory Files with Specific Tiers and Sizes

Source: https://docs.memvid.com/concepts/tickets-and-capacity

Demonstrates how to create new Memvid memory files using the CLI, specifying storage tiers (default, dev, enterprise) or explicit sizes. Also shows how to use the Python SDK to open or create memory files.

```bash
# Create with default (free) tier
memvid create knowledge.mv2

# Create with dev tier
memvid create knowledge.mv2 --tier dev

# Create with enterprise tier
memvid create knowledge.mv2 --tier enterprise

# Create with explicit size
memvid create knowledge.mv2 --size 500MB
memvid create knowledge.mv2 --capacity 2GB
```

```python
from memvid_sdk import use

# Open or create with default capacity
mem = use('basic', 'knowledge.mv2')

# Capacity is set at creation time
mem = use('basic', 'new-memory.mv2', mode='create')
```

--------------------------------

### Diagnose Slow Query Performance with Bash

Source: https://docs.memvid.com/errors/troubleshooting

This snippet demonstrates how to diagnose slow query performance in memvid. It includes commands to check file size, retrieve frame count, and profile query execution times.

```bash
ls -lh knowledge.mv2
memvid stats knowledge.mv2 --json | jq '.frame_count'
time memvid find knowledge.mv2 --query "test" --json
```

--------------------------------

### Check Memvid Memory File Statistics and Capacity

Source: https://docs.memvid.com/concepts/tickets-and-capacity

Shows how to retrieve statistics for a Memvid memory file, including document count, active frames, current size, capacity, and utilization percentage. Examples are provided for CLI (standard and JSON output) and SDKs (Python and Node.js).

```bash
# CLI command to get stats
memvid stats knowledge.mv2
```

```bash
# CLI command to get stats in JSON format
memvid stats knowledge.mv2 --json
```

```python
stats = mem.stats()
print(f"Size: {stats['size_bytes']} bytes")
print(f"Capacity: {stats['capacity_bytes']} bytes")
print(f"Utilization: {stats['storage_utilisation_percent']:.1f}%")
```

```typescript
const stats = await mv.stats();
console.log(`Size: ${stats.sizeBytes} bytes`);
console.log(`Capacity: ${stats.capacityBytes} bytes`);
console.log(`Utilization: ${stats.storageUtilisationPercent.toFixed(1)}%`);
```

--------------------------------

### Enable Verbose Output for Debugging

Source: https://docs.memvid.com/troubleshooting/cli

Enables verbose output for troubleshooting memvid commands. Different levels of verbosity (-v, -vv, -vvv, -vvvv) correspond to different logging levels (WARN, INFO, DEBUG, TRACE).

```bash
# Increase verbosity
memvid -v find myfile.mv2 --query "test"    # WARN
memvid -vv find myfile.mv2 --query "test"   # INFO
memvid -vvv find myfile.mv2 --query "test"  # DEBUG
memvid -vvvv find myfile.mv2 --query "test" # TRACE
```

--------------------------------

### Ingest and Query Documents with Node.js SDK

Source: https://docs.memvid.com/examples/document-qa

This Node.js code snippet demonstrates how to use the '@memvid/sdk' to create a document store, ingest PDF files from a local directory, and then ask questions against the ingested documents. It returns the answer and the source of the information.

```typescript
import { use } from '@memvid/sdk';
import * as fs from 'fs';
import * as path from 'path';

// Create document store
const mem = await use('basic', 'documents.mv2', { mode: 'create' });

// Ingest documents
const docsDir = './docs';
for (const filename of fs.readdirSync(docsDir)) {
  if (filename.endsWith('.pdf')) {
    await mem.put({
      title: filename,
      label: 'document',
      file: path.join(docsDir, filename),
    });
  }
}

// Ask questions
const answer = await mem.ask('What is the refund policy?', { returnSources: true });
console.log(`Answer: ${answer.answer}`);
console.log(`Source: ${answer.sources?.[0]?.title ?? 'n/a'}`);
```

--------------------------------

### Set Memory Size Limit in Memvid

Source: https://docs.memvid.com/concepts/performance-tuning

Defines the size limit for a Memvid memory file during creation. This helps manage storage efficiency, with recommendations provided for different types of content.

```bash
# Small memory for quick projects
memvid create notes.mv2 --size 10MB

# Large memory for document archives
memvid create archive.mv2 --size 50MB
```

--------------------------------

### Optimize memvid Queries with Python

Source: https://docs.memvid.com/errors/troubleshooting

This snippet shows how to optimize memvid query performance in Python by reducing the 'k' value, using specific search modes like 'lex', and applying scope filters.

```python
results = mem.find('query', k=5)  # Instead of k=50
results = mem.find('exact term', mode='lex')
results = mem.find('query', scope='category:docs')
```

--------------------------------

### Traverse Relationships with Logic Mesh (Bash)

Source: https://docs.memvid.com/concepts/graph-search

Explores connections between entities in the knowledge graph. Allows specifying start entity, relationship type, traversal depth, and direction (outgoing/incoming).

```bash
# Find what John Smith is connected to
memvid follow traverse memory.mv2 --start "John Smith"

# Follow specific relationship type
memvid follow traverse memory.mv2 --start "John Smith" --link works_at

# Control traversal depth (default: 2)
memvid follow traverse memory.mv2 --start "Acme Corp" --hops 3

# Direction: outgoing, incoming, or both
memvid follow traverse memory.mv2 --start "Jane Doe" --direction incoming
```

--------------------------------

### Handling Memvid Decryption Errors

Source: https://docs.memvid.com/concepts/encryption

This section provides examples of common error scenarios when using Memvid, specifically dealing with incorrect passwords or corrupted files. It shows the expected error messages and suggests the `--force` option for overwriting existing files.

```bash
memvid unlock memory.mv2e --out memory.mv2
# Enter password: 
# Error: Decryption failed - incorrect password or corrupted file

memvid unlock corrupted.mv2e --out memory.mv2
# Error: Authentication failed - file may be corrupted or tampered with

memvid unlock memory.mv2e --out memory.mv2
# Error: memory.mv2 already exists. Use --force to overwrite.

# Solution
memvid unlock memory.mv2e --out memory.mv2 --force
```

--------------------------------

### Enable Context-Only Mode in Memvid

Source: https://docs.memvid.com/concepts/performance-tuning

Skip LLM synthesis and retrieve only relevant context for maximum speed. This mode is useful for debugging retrieval quality, batch processing, or feeding context to external LLMs.

```bash
# Get relevant context without LLM synthesis
memvid ask memory.mv2 --question "What are the config options?" --context-only
```

--------------------------------

### Best Practices

Source: https://docs.memvid.com/frameworks/google-adk

Recommended practices for using the Memvid SDK effectively.

```APIDOC
## Best Practices

1. **Use read-only mode** for retrieval agents
2. **Handle function calls** appropriately in responses
3. **Use streaming** for long responses
4. **Close the memory** when done

<Tabs>
  <Tab title="Node.js">
    ```typescript
    const mem = await use('google-adk', 'knowledge.mv2', { readOnly: true });
    try {
      // Create agent and run queries
      const geminiKey = process.env.GEMINI_API_KEY ?? process.env.GOOGLE_API_KEY;
      if (!geminiKey) throw new Error("Set GEMINI_API_KEY (or legacy GOOGLE_API_KEY)");
      const genAI = new GoogleGenerativeAI(geminiKey);
      // ... use client
    } finally {
      // No explicit close needed; dropping the handle releases the shared lock.
    }
    ```
  </Tab>

  <Tab title="Python">
    ```python
    mem = use('google-adk', 'knowledge.mv2', read_only=True)
    try:
        # Create agent and run queries
        client = genai.Client()
        # ... use client
    finally:
        mem.close()
    ```
  </Tab>
</Tabs>
```

--------------------------------

### Recover Corrupted memvid Files with Bash

Source: https://docs.memvid.com/errors/troubleshooting

This snippet shows how to recover corrupted memvid files using the 'doctor' command with vacuuming, rebuilding indices (lexical, vector, time), or restoring from backup.

```bash
memvid doctor knowledge.mv2 --vacuum
memvid doctor knowledge.mv2 \
  --rebuild-lex-index \
  --rebuild-vec-index \
  --rebuild-time-index
cp /backups/knowledge.mv2 ./knowledge.mv2
```

--------------------------------

### Memvid Advanced Search Limiting

Source: https://docs.memvid.com/cli/search-and-ask

Provides examples of limiting the number of search results using `--top-k` and adjusting the length of context snippets with `--snippet-chars`. This helps in controlling the verbosity and focus of the search output.

```bash
# Get top 5 results
memvid find knowledge.mv2 --query "performance optimization" --top-k 5

# Get single best match
memvid find knowledge.mv2 --query "main entry point" --top-k 1

# Longer snippets
memvid find knowledge.mv2 --query "architecture" --snippet-chars 800
```

--------------------------------

### Deduplication Across Different Memory Files (Bash)

Source: https://docs.memvid.com/concepts/deduplication

This example illustrates that deduplication is scoped within a single `.mv2` file. The same content in different memory files (`work.mv2` and `personal.mv2`) will be stored independently, as deduplication does not cross file boundaries.

```bash
# These are independent - both will store the content
memvid put work.mv2 --input document.pdf
memvid put personal.mv2 --input document.pdf
```

--------------------------------

### Initialize Memvid and Embed Documents (Python)

Source: https://docs.memvid.com/concepts/embedding-models

Sets up OpenAI embeddings and a Memvid knowledge base, then ingests multiple documents with embeddings. Requires the 'memvid_sdk' and 'openai' libraries. Input is a list of dictionaries, output is frame IDs.

```python
from memvid_sdk import create
from openai import OpenAIEmbeddings

embedder = OpenAIEmbeddings()
mem = create('knowledge.mv2', enable_vec=True, enable_lex=True)

documents = [
    {"title": "Doc 1", "label": "research", "text": "Content 1..."},
    {"title": "Doc 2", "label": "research", "text": "Content 2{"title": "Doc 2", "label": "research", "text": "Content 2..."},
]
frame_ids = mem.put_many(documents, embedder=embedder)

query = "What is the main finding?"
results = mem.find(query, k=10, mode="sem", embedder=embedder)
```

--------------------------------

### GitHub Actions CI/CD for Memvid Backup

Source: https://docs.memvid.com/concepts/encryption

This GitHub Actions workflow automates the process of backing up Memvid data. It checks out the code, installs the Memvid CLI, decrypts an existing encrypted memory, adds new data, re-encrypts it, and uploads the encrypted artifact. It relies on a `MEMVID_PASSWORD` secret for decryption and re-encryption.

```yaml
name: Backup Memory
on:
  schedule:
    - cron: '0 0 * * *'  # Daily

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Memvid
        run: curl -fsSL https://get.memvid.com | sh

      - name: Decrypt, update, re-encrypt
        env:
          MEMVID_PASSWORD: ${{ secrets.MEMVID_PASSWORD }}
        run: |
          echo "$MEMVID_PASSWORD" | memvid unlock memory.mv2e --password-stdin --out memory.mv2
          memvid put memory.mv2 --input ./new-data/
          echo "$MEMVID_PASSWORD" | memvid lock memory.mv2 --password-stdin --out memory.mv2e

      - name: Upload encrypted backup
        uses: actions/upload-artifact@v4
        with:
          name: encrypted-memory
          path: memory.mv2e
```

--------------------------------

### Resolve Python Import Errors with Bash

Source: https://docs.memvid.com/errors/troubleshooting

This snippet shows bash commands to resolve Python import errors by installing/reinstalling the SDK, checking Python version compatibility (3.8+), and activating virtual environments.

```bash
pip install --upgrade memvid-sdk
python3 --version
source venv/bin/activate
pip install memvid-sdk
```

--------------------------------

### Manage Capacity Issues

Source: https://docs.memvid.com/cli/maintenance-and-tickets

Provides commands to check current storage usage, delete specific frames, and reclaim disk space by vacuuming the knowledge base file. These actions help resolve 'CapacityExceeded' errors.

```bash
# Check current usage
memvid stats knowledge.mv2

# Delete unused frames
memvid delete knowledge.mv2 --frame-id 42 --yes

# Vacuum to reclaim space
memvid doctor knowledge.mv2 --vacuum
```

--------------------------------

### Get Memvid memory statistics (Node.js SDK)

Source: https://docs.memvid.com/concepts/capacity-and-plans

Retrieves memory statistics using the Memvid Node.js SDK. It outputs the memory's size, capacity, and storage utilization percentage. Requires the '@memvid/sdk' package.

```typescript
import { use } from '@memvid/sdk';

const mem = await use('basic', 'knowledge.mv2', { readOnly: true });
const stats = await mem.stats();

console.log(`Size: ${(stats.size_bytes / 1e9).toFixed(2)} GB`);
console.log(`Capacity: ${(stats.capacity_bytes / 1e9).toFixed(2)} GB`);
console.log(`Utilization: ${stats.storage_utilisation_percent.toFixed(1)}%`);
```

--------------------------------

### Perform hybrid search in memvid

Source: https://docs.memvid.com/sdks/cli

Executes a hybrid search query against a memvid memory file. It returns the top 'k' results and outputs them in JSON format. This example searches for 'hybrid search' and requests the top 5 results.

```bash
memvid find data.mv2 --query "hybrid search" --top-k 5 --json
```

--------------------------------

### Memvid Python SDK: Get PII Locations

Source: https://docs.memvid.com/concepts/pii-masking

Explains the `detect_pii` utility function in the Python SDK, which returns a list of detected PII items, including their type, value, and position within the text.

```APIDOC
## Python SDK: Get PII Locations

### Description
Uses the `detect_pii` utility function from the Memvid Python SDK to identify and locate all instances of PII within a text string.

### Function
```python
detect_pii(text: str) -> list[PIIItem]
```

### Parameters
*   `text` (str) - The input text to analyze.

### Returns
A list of `PIIItem` objects, where each object has the following attributes:
*   `type` (str): The type of PII detected (e.g., 'email', 'phone').
*   `value` (str): The actual PII string found.
*   `start` (int): The starting index of the PII in the text.
*   `end` (int): The ending index of the PII in the text.

### Request Example
```python
from memvid import detect_pii

text = "Email john@test.com or call 555-1234"
pii_items = detect_pii(text)

for item in pii_items:
    print(f"Type: {item.type}, Value: {item.value}, Position: {item.start}-{item.end}")
```

### Response Example
```
Type: email, Value: john@test.com, Position: 6-19
Type: phone, Value: 555-1234, Position: 28-36
```
```

--------------------------------

### Manage Memvid Capacity by Reclaiming Space and Migrating Data

Source: https://docs.memvid.com/concepts/tickets-and-capacity

Provides instructions and code examples for managing Memvid file capacity. This includes reclaiming space by deleting frames and vacuuming the file, as well as migrating data to a new, larger file using the Python SDK.

```bash
# Check current usage before operations
memvid stats knowledge.mv2

# Delete old content
memvid delete knowledge.mv2 --frame-id 42 --yes
memvid delete knowledge.mv2 --uri "mv2://old/doc.md" --yes

# Vacuum to reclaim space
memvid doctor knowledge.mv2 --vacuum

# Verify space was reclaimed
memvid stats knowledge.mv2

# Create new file with larger capacity
memvid create new-knowledge.mv2 --tier dev
```

```python
from memvid_sdk import use

# Open old file read-only
old = use('basic', 'knowledge.mv2', read_only=True)

# Create new file with larger capacity
new = use('basic', 'new-knowledge.mv2', mode='create')

# Migrate content
timeline = old.timeline(limit=10000)
for entry in timeline['entries']:
    frame = old.frame(f"mv2://{entry['uri']}")
    if frame:
        new.put(text=frame['content'], title=frame['title'])

new.seal()
```

--------------------------------

### Handle Memvid Capacity Exceeded Errors

Source: https://docs.memvid.com/concepts/tickets-and-capacity

Illustrates how to handle the `CapacityExceededError` when attempting to add content that surpasses a Memvid file's limit. Provides examples for CLI error messages and Python/Node.js SDK error handling, along with suggested solutions like deleting frames, vacuuming, or creating a larger file.

```bash
# Example CLI error output and solutions
Error: CapacityExceeded
File capacity: 25000000000 bytes (25 GB)
Current usage: 24000000000 bytes (24 GB)
Requested: 2000000000 bytes (2 GB)

Solutions:
1. Delete unused frames: memvid delete knowledge.mv2 --frame-id <id>
2. Vacuum to reclaim space: memvid doctor knowledge.mv2 --vacuum
3. Create a larger memory file: memvid create new.mv2 --tier dev
```

```python
from memvid_sdk import use, CapacityExceededError

try:
    mem.put(file="large-file.pdf")
except CapacityExceededError as e:
    print(f"MV001: {e}")
    # Handle by cleaning up or using a larger file
```

```typescript
import { open, CapacityExceededError } from '@memvid/sdk';

const mv = await open('knowledge.mv2');

try {
  await mv.put({ title: 'Large Document', label: 'docs', file: 'large-file.pdf' });
  await mv.seal();
} catch (error) {
  if (error instanceof CapacityExceededError) {
    console.log('MV001:', error.message);
  }
}
```

--------------------------------

### Memvid CLI: Combining Adaptive Search with Sketch Pre-filtering

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Illustrates how to combine Memvid's adaptive search with sketch pre-filtering for improved speed and precision. It shows building sketches and then performing a search with adaptive features enabled by default.

```bash
# Sketches + adaptive = fast + precise
memvid sketch build memory.mv2 --variant medium
memvid find memory.mv2 --query "term"  # Both enabled by default
```

--------------------------------

### Diagnose Node.js Native Binding Errors with Bash

Source: https://docs.memvid.com/errors/troubleshooting

This snippet provides bash commands to diagnose native binding errors in Node.js memvid projects. It checks the Node version, native module existence, and platform information.

```bash
node --version
ls node_modules/@memvid/sdk/*.node
node -e "console.log(process.platform, process.arch)"
```

--------------------------------

### Integrate Memvid with LangChain (Python/Node.js)

Source: https://docs.memvid.com/faq/general

Demonstrates how to integrate Memvid with the LangChain framework using both Python and Node.js SDKs. This allows for seamless use of Memvid's memory capabilities within LangChain applications.

```python
from memvid_sdk import use
mem = use('langchain', 'knowledge.mv2')
tools = mem.tools  # LangChain StructuredTool objects
```

```typescript
import { use } from '@memvid/sdk';
const mv = await use('langchain', 'knowledge.mv2');
```

--------------------------------

### Get Memvid memory statistics (Python SDK)

Source: https://docs.memvid.com/concepts/capacity-and-plans

Retrieves memory statistics using the Memvid Python SDK. It outputs the memory's size, capacity, storage utilization percentage, and current plan details. Requires the 'memvid_sdk' library.

```python
from memvid_sdk import use

mem = use('basic', 'knowledge.mv2', read_only=True)
stats = mem.stats()

print(f"Size: {stats['size_bytes'] / 1e9:.2f} GB")
print(f"Capacity: {stats['capacity_bytes'] / 1e9:.2f} GB")
print(f"Utilization: {stats['storage_utilisation_percent']:.1f}%")

# Check current ticket info
ticket = mem.current_ticket()
print(f"Plan: {ticket['issuer']}")
print(f"Capacity: {ticket['capacity_bytes'] / 1e9:.2f} GB")
```

--------------------------------

### Semantic Kernel Planner Integration with Memvid

Source: https://docs.memvid.com/frameworks/semantic-kernel

Demonstrates integrating Memvid with Semantic Kernel's `SequentialPlanner`. This allows the planner to create and execute multi-step plans that utilize Memvid's capabilities, such as searching for information and summarizing it.

```python
import semantic_kernel as sk
from semantic_kernel.planners import SequentialPlanner
from memvid_sdk import use

# Setup kernel with plugins
kernel = sk.Kernel()
kernel.add_service(OpenAIChatCompletion(service_id="openai", ai_model_id="gpt-4o"))

mem = use('semantic-kernel', 'knowledge.mv2', read_only=True)
kernel.add_plugin(mem.as_plugin(), "memvid")

# Create planner
planner = SequentialPlanner(kernel)

# Create and execute plan
plan = await planner.create_plan(
    "Find information about API endpoints and summarize the key points"
)
result = await plan.invoke()
print(result)
```

--------------------------------

### Traverse Logic-Mesh Relationships

Source: https://docs.memvid.com/cli/advanced-commands

Traverses the entity-relationship graph (Logic-Mesh) starting from a specified entity. Supports following specific relationship types, controlling traversal depth, and specifying direction. Outputs can be filtered by JSON format.

```bash
# Find entities related to Microsoft
memvid follow traverse knowledge.mv2 --start "Microsoft"

# Follow specific relationship type
memvid follow traverse knowledge.mv2 --start "Satya Nadella" --link "manager"

# Deeper traversal
memvid follow traverse knowledge.mv2 --start "Seattle" --hops 3

# JSON output
memvid follow traverse knowledge.mv2 --start "Microsoft" --json
```

--------------------------------

### Select Embedding Model in Memvid

Source: https://docs.memvid.com/concepts/performance-tuning

Choose an embedding model based on the desired tradeoff between speed, quality, and resource usage. Smaller models are faster but offer lower quality, while larger models provide better quality at the cost of speed.

```bash
# Use smaller model for speed
memvid -m bge-small put memory.mv2 --input docs/

# Use larger model for quality
memvid -m gte-large put memory.mv2 --input docs/
```

--------------------------------

### Memvid Session Recording and Replay

Source: https://docs.memvid.com/sdks/python

Details how to use Memvid's session recording capabilities for debugging RAG failures. This involves starting a session, performing operations (which are recorded), adding checkpoints, ending the session, and replaying it with different parameters to analyze performance.

```python
# Start recording session
session_id = mem.session_start("Debug Session")

# Perform operations (all recorded)
mem.put(title="Meeting Notes", label="notes", metadata={}, text="Discussed Q4...")
results = mem.find("roadmap", k=5)

# Add checkpoints at key moments
mem.session_checkpoint()

# End session
summary = mem.session_end()
print(f"Recorded {summary['action_count']} actions")

# Replay with different parameters
replay_result = mem.session_replay(
    session_id,
    adaptive=True,
    top_k=20
)
print(f"Match rate: {replay_result['match_rate']:.1%}")
```

--------------------------------

### Claude Entity Extraction Setup

Source: https://docs.memvid.com/concepts/entity-extraction

Set up and use Anthropic's Claude for entity extraction via the Memvid SDK. This includes setting the API key and instantiating the extractor using either a factory function or direct instantiation. It supports specifying entity types and specific Claude models. Input is text, output is a list of extracted entities.

```bash
export ANTHROPIC_API_KEY=your-key-here
```

```python
from memvid_sdk.entities import get_entity_extractor, ClaudeEntities

# Using factory
ner = get_entity_extractor('claude', entity_types=['COMPANY', 'PERSON', 'REGULATION'])

# With specific model
ner = get_entity_extractor('claude:claude-3-5-sonnet-20241022', entity_types=['COMPANY'])

# Direct instantiation
ner = ClaudeEntities(
    model='claude-3-5-sonnet-20241022',
    entity_types=['COMPANY', 'EXECUTIVE', 'DEAL'],
)

entities = ner.extract(text, min_confidence=0.6)
```

--------------------------------

### Fetching External Data

Source: https://docs.memvid.com/frameworks/google-adk

Instructions on how to fetch external data, such as the llms.txt file.

```APIDOC
---

> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.memvid.com/llms.txt
```

--------------------------------

### Python: Keep Original Data for Internal Use

Source: https://docs.memvid.com/concepts/pii-masking

Illustrates the best practice of retaining original, unmasked data for internal processing while masking only for external or display purposes. This Python example shows fetching raw results with `mem.find` and then creating a masked list for display.

```python
# Raw data for internal processing
results = mem.find(query)  # No masking

# Masked for display
MASKED_RESULTS = [mask_pii(r.text) for r in results]
```

--------------------------------

### Configure Memory for Large Files

Source: https://docs.memvid.com/troubleshooting/cli

Configures environment variables to increase available memory for large ingestion operations. This includes setting the model cache directory, enabling offline mode, and controlling parallel ingestion.

```bash
# Set model cache directory
export MEMVID_MODELS_DIR=~/.memvid/models

# Enable offline mode (skip model downloads)
export MEMVID_OFFLINE=1

# Control parallel ingestion
export MEMVID_PARALLEL_SEGMENTS=1
```

--------------------------------

### Get CLIP Provider Factory Function (TypeScript)

Source: https://docs.memvid.com/concepts/visual-embeddings

This snippet demonstrates how to use the `getClipProvider` factory function in TypeScript (Node.js environment) to retrieve a CLIP provider instance. It takes a provider string as an argument.

```typescript
// Node.js
	import { getClipProvider } from '@memvid/sdk';

	const clip = getClipProvider(provider); // 'local' | 'openai' | 'gemini'
```

--------------------------------

### Ask Configuration (Local Model)

Source: https://docs.memvid.com/cli/search-and-ask

Querying documentation or codebases using a local Ollama model to keep data private. This is useful for understanding environment variables or code architecture.

```bash
memvid ask docs.mv2 \
  --question "What environment variables are required?" \
  --use-model "ollama:qwen2.5:1.5b"
```

```bash
memvid ask code.mv2 \
  --question "How does the authentication flow work?" \
  --use-model "ollama:qwen2.5:1.5b"
```

--------------------------------

### Exporting Tables with Memvid CLI

Source: https://docs.memvid.com/concepts/table-extraction

Provides examples for exporting extracted tables using the Memvid CLI into various formats, including CSV and JSON. Covers exporting single tables, multiple tables to a directory, and different JSON structures (array of arrays vs. array of records).

```bash
# Export to CSV
memvid tables export memory.mv2 --table-id pdf_table_1_page1 --format csv --out data.csv

# Export to JSON (array of arrays)
memvid tables export memory.mv2 --table-id pdf_table_1_page1 --format json --out data.json

# Export to JSON (array of objects/records)
memvid tables export memory.mv2 --table-id pdf_table_1_page1 --format json --as-records --out data.json

# Export all tables
memvid tables export memory.mv2 --all --format csv --out-dir ./tables/
```

--------------------------------

### Memvid SDK: Ingesting and Searching Transcriptions

Source: https://docs.memvid.com/concepts/audio-video

This Python code snippet demonstrates how to use the Memvid SDK to interact with Memvid functionalities. It shows how to ingest media via the CLI using `subprocess.run` and then perform searches on transcriptions using the SDK's `use` function. This requires the Memvid Python library to be installed.

```python
import subprocess

# Ingest via CLI
subprocess.run([
    'memvid', 'put', 'memory.mv2',
    '--input', 'audio.mp3',
    '--language', 'en'
])

# Search transcriptions via SDK
from memvid import use
mem = use('basic', 'memory.mv2')
results = mem.find("meeting action items")
```

--------------------------------

### Create a memvid memory file

Source: https://docs.memvid.com/sdks/cli

Creates a new memvid memory file (`.mv2`). This command allows specifying the storage tier and size for the new file. For example, it creates a file named 'data.mv2' with a 'free' tier and 1GB size.

```bash
memvid create data.mv2 --tier free --size 1GB
```

--------------------------------

### Python: Multi-Agent Crew with CrewAI and Memvid SDK

Source: https://docs.memvid.com/frameworks/crewai

This snippet demonstrates a multi-agent workflow in Python using the CrewAI library and the Memvid SDK. It sets up three agents: researcher, writer, and reviewer, each with specific roles, goals, and backstories. The workflow involves sequential tasks: research, writing, and reviewing, where each task depends on the output of the previous one. The Memvid SDK is used to provide a search tool to the researcher agent. This setup facilitates a collaborative agent process.

```python
from crewai import Agent, Task, Crew, Process
from memvid_sdk import use

# Initialize
mem = use('crewai', 'knowledge.mv2', read_only=True)
search_tool = mem.get_search_tool()

# Create multiple agents
researcher = Agent(
    role="Researcher",
    goal="Find comprehensive information from the knowledge base",
    backstory="Skilled at finding relevant information quickly",
    tools=[search_tool],
    verbose=True
)

writer = Agent(
    role="Technical Writer",
    goal="Write clear documentation based on research",
    backstory="Expert at translating technical content into readable docs",
    verbose=True
)

reviewer = Agent(
    role="Editor",
    goal="Review and improve the documentation for clarity",
    backstory="Meticulous editor with attention to detail",
    verbose=True
)

# Define tasks
research_task = Task(
    description="Research authentication mechanisms in the knowledge base",
    agent=researcher,
    expected_output="Detailed findings about authentication"
)

writing_task = Task(
    description="Write a guide based on the research findings",
    agent=writer,
    expected_output="A clear authentication guide",
    context=[research_task]  # Depends on research
)

review_task = Task(
    description="Review and polish the guide for publication",
    agent=reviewer,
    expected_output="Final polished guide",
    context=[writing_task]  # Depends on writing
)

# Create crew with sequential process
crew = Crew(
    agents=[researcher, writer, reviewer],
    tasks=[research_task, writing_task, review_task],
    process=Process.sequential,
    verbose=True
)

result = crew.kickoff()
print(result)

```

--------------------------------

### Memvid Python SDK Context Manager

Source: https://docs.memvid.com/sdks/python

Utilizes the context manager for Memvid memory files, ensuring automatic closure upon completion. This example shows creating or opening a memory file, adding content, and performing a search within the `with` block.

```python
import memvid_sdk as memvid

# Automatically closes when done
with memvid.use('basic', 'memory.mv2') as mem:
    mem.put(title='Doc', label='test', metadata={}, text='Content')
    results = mem.find('query')
```

--------------------------------

### Memvid CLI: Adding Optional Vector Embeddings

Source: https://docs.memvid.com/concepts/no-vec-mode

This example shows how to enhance an existing Memvid memory by adding a vector index, enabling semantic search capabilities. It demonstrates the `doctor` command for rebuilding the vector index and how to perform semantic searches using the `--mode sem` flag.

```bash
# Add vector index to existing memory
memvid doctor my-memory.mv2 --rebuild-vec-index

# Now semantic search works too
memvid find my-memory.mv2 --query "concepts related to authentication" --mode sem
```

--------------------------------

### Rebuild Vector Index with External Embeddings and Search

Source: https://docs.memvid.com/introduction/the-memvid-approach

Illustrates how to rebuild the vector index of an existing Memvid memory file to use embeddings from an external provider like OpenAI. It then shows how to perform searches using semantic or hybrid modes with these new embeddings.

```bash
# Rebuild vector index with OpenAI embeddings
memvid doctor my-memory.mv2 --rebuild-vec-index -m openai

# Search with semantic mode
memvid find my-memory.mv2 --query "financial performance" --mode sem

# Or use hybrid (lexical + semantic)
memvid find my-memory.mv2 --query "financial performance" --mode hybrid
```

--------------------------------

### Disable Adaptive Retrieval in Memvid

Source: https://docs.memvid.com/concepts/performance-tuning

Disables adaptive retrieval for more predictable search performance by ensuring a fixed number of results. The default adaptive mode may return fewer results for higher relevance.

```bash
# Fixed result count (faster, predictable)
memvid find memory.mv2 --query "term" --no-adaptive --top-k 10

# Adaptive (may return fewer, but higher quality)
memvid find memory.mv2 --query "term"  # Default
```

--------------------------------

### Memvid Best Practice: Bulk Imports and Enrichment

Source: https://docs.memvid.com/concepts/memory-cards

Recommends performing bulk document imports before initiating the enrichment process to optimize efficiency.

```bash
# Load all documents first
memvid put memory.mv2 --input docs/

# Then enrich in one pass
memvid enrich memory.mv2 --engine groq
```

--------------------------------

### Memvid SDK Error Handling with Python

Source: https://docs.memvid.com/python-sdk/overview

Demonstrates how to catch specific Memvid SDK errors like CapacityExceededError and LockedError. It imports various error types from memvid_sdk and uses try-except blocks for graceful error management. Ensure the memvid_sdk is installed and configured.

```python
from memvid_sdk import (
    MemvidError,
    CapacityExceededError,      # MV001
    TicketInvalidError,         # MV002
    TicketReplayError,          # MV003
    LexIndexDisabledError,      # MV004
    TimeIndexMissingError,      # MV005
    VerifyFailedError,          # MV006
    LockedError,                # MV007
    ApiKeyRequiredError,        # MV008
    MemoryAlreadyBoundError,    # MV009
    FrameNotFoundError,         # MV010
    VecIndexDisabledError,      # MV011
    CorruptFileError,           # MV012
    VecDimensionMismatchError,  # MV014
    EmbeddingFailedError,       # MV015
    EncryptionError,            # MV016
    NerModelNotAvailableError,  # MV017
    ClipIndexDisabledError      # MV018
)

try:
    mem.put('Large file', 'data', {}, file='huge.bin')
except CapacityExceededError:
    print('Storage capacity exceeded (MV001)')
except LockedError:
    print('File locked by another process (MV007)')
except VecIndexDisabledError:
    print('Enable vector index first (MV011)')
```

--------------------------------

### Manage Tickets and Memory Capacity

Source: https://docs.memvid.com/python-sdk/overview

Covers functionalities related to managing tickets and capacity within the system. This includes getting current capacity, retrieving ticket information, syncing tickets from a dashboard, applying a ticket manually, and managing memory bindings.

```python
# Get current capacity
capacity = mem.get_capacity()

# Get current ticket info
ticket = mem.current_ticket()

# Sync tickets from dashboard
result = mem.sync_tickets('mem_abc123', api_key)

# Apply ticket manually
mem.apply_ticket(ticket_string)

# Get memory binding
binding = mem.get_memory_binding()

# Unbind from dashboard
mem.unbind_memory()
```

--------------------------------

### Memvid Memory File Inspection JSON Output (JSON)

Source: https://docs.memvid.com/cli/create-and-put

Example JSON output when inspecting a Memvid memory file with the `--json` flag. Details include version, creation date, frame count, size, index status, tracks, and binding information.

```json
{
  "path": "my-memory.mv2",
  "version": "2.1.0",
  "created_at": "2024-01-15T10:30:00Z",
  "frame_count": 1234,
  "size_bytes": 47395430,
  "size_limit_bytes": 536870912,
  "indexes": {
    "lex": { "enabled": true, "term_count": 12456 },
    "vec": { "enabled": true, "vector_count": 1234, "dimension": 384 },
    "time": { "enabled": true, "entry_count": 1234 }
  },
  "tracks": {
    "default": 890,
    "meetings": 234,
    "emails": 110
  },
  "binding": {
    "memory_id": "mem_abc123",
    "bound_at": "2024-01-15T10:30:00Z"
  }
}
```

--------------------------------

### Memvid Framework Adapters (LangChain, LlamaIndex, CrewAI, AutoGen, Haystack)

Source: https://docs.memvid.com/sdks/python

Shows how to integrate Memvid memory files with popular AI frameworks. This includes creating framework-specific objects like retrievers or query engines for LangChain and LlamaIndex, accessing tools for CrewAI, and basic setup for AutoGen and Haystack.

```python
# LangChain
mem = memvid.use('langchain', 'knowledge.mv2')
retriever = mem.as_retriever()

# LlamaIndex
mem = memvid.use('llamaindex', 'knowledge.mv2')
query_engine = mem.as_query_engine()

# CrewAI
mem = memvid.use('crewai', 'knowledge.mv2')
tools = mem.tools

# AutoGen
mem = memvid.use('autogen', 'knowledge.mv2')

# Haystack
mem = memvid.use('haystack', 'knowledge.mv2')
```

--------------------------------

### Profile Memvid Operations Performance

Source: https://docs.memvid.com/errors/troubleshooting

This Python script profiles the performance of common Memvid operations like search, ask, and timeline. It measures the execution time for each operation, helping to identify performance bottlenecks within the Memvid SDK.

```python
import time
from memvid_sdk import use

def profile_operations(filepath: str):
    """Profile common Memvid operations."""

    print(f"Profiling: {filepath}\n")

    mem = use('basic', filepath, read_only=True)

    # Profile search
    start = time.perf_counter()
    for _ in range(10):
        mem.find('test query', k=10)
    search_time = (time.perf_counter() - start) / 10
    print(f"Average search time: {search_time*1000:.2f}ms")

    # Profile ask
    start = time.perf_counter()
    mem.ask('What is this about?')
    ask_time = time.perf_counter() - start
    print(f"Ask time: {ask_time*1000:.2f}ms")

    # Profile timeline
    start = time.perf_counter()
    mem.timeline(limit=100)
    timeline_time = time.perf_counter() - start
    print(f"Timeline time: {timeline_time*1000:.2f}ms")

    print("\n Profiling complete")

profile_operations('knowledge.mv2')
```

--------------------------------

### Enrichment with Groq Cloud Engine (CLI)

Source: https://docs.memvid.com/concepts/memory-cards

This command uses the Groq API for fast and high-quality LLM-based enrichment. It requires a `GROQ_API_KEY` environment variable to be set and is considered a recommended balance of speed and quality.

```bash
export GROQ_API_KEY=gsk_xxx
memvid enrich memory.mv2 --engine groq
```

--------------------------------

### Initialize Memory Store with Knowledge

Source: https://docs.memvid.com/examples/chatbot-memory

Initializes the Memvid memory store and populates it with initial knowledge about the product overview and pricing. This Python code uses the memvid-sdk to create or open a memory file and store structured text data under specific labels.

```python
from memvid_sdk import use
import os

# Create or open memory file
mem = use('langchain', 'chatbot-memory.mv2', mode='auto')

# Add some initial knowledge
mem.put(
    "Product Overview",
    "knowledge",
    {},
    text="""Our product is an AI-powered analytics platform that helps businesses
understand their data. Key features include:
- Real-time dashboards
- Automated insights
- Custom reports
- API access for developers"""
)

mem.put(
    "Pricing Information",
    "knowledge",
    {},
    text="""Pricing tiers:
- Starter: $29/month - Up to 10,000 events
- Pro: $99/month - Up to 100,000 events
- Enterprise: Custom pricing - Unlimited events
All plans include 14-day free trial."""
)

print("Memory initialized with knowledge base")
```

--------------------------------

### Batch Import from arXiv (Python)

Source: https://docs.memvid.com/examples/research-assistant

Imports multiple research papers from arXiv in a batch process. It utilizes the 'arxiv' library to search for papers based on a query, downloads their PDFs, and adds them to the research assistant's corpus with associated metadata. Includes an example of importing papers related to 'transformer attention mechanism'.

```python
import arxiv
import tempfile

def batch_import_arxiv(query: str, max_results: int = 50):
    """Import papers from arXiv search."""
    assistant = ResearchAssistant()

    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.Relevance
    )

    for paper in search.results():
        print(f"Downloading: {paper.title}")

        # Download PDF
        pdf_path = paper.download_pdf()

        # Add to corpus
        assistant.add_paper(
            title=paper.title,
            pdf_path=pdf_path,
            metadata={
                "arxiv_id": paper.entry_id,
                "authors": [a.name for a in paper.authors],
                "abstract": paper.summary,
                "published": paper.published.isoformat(),
                "categories": paper.categories
            }
        )

    print(f" Imported {max_results} papers")


# Import transformer papers
batch_import_arxiv("transformer attention mechanism", max_results=100)
```

--------------------------------

### Deal Memo Analysis with Entity Extraction (Python)

Source: https://docs.memvid.com/concepts/entity-extraction

Example of analyzing deal memos by extracting structured information such as company names, investors, monetary values, deal types, dates, and locations. It uses a pre-configured entity extractor for deal-related information.

```python
ner = get_entity_extractor('openai', entity_types=[
    'COMPANY', 'INVESTOR', 'MONEY', 'DEAL_TYPE', 'DATE', 'LOCATION'
])

deal_text = """
Series B Funding: Atlas Logistics
Atlas Logistics, headquartered in Seattle, announced a $50 million Series B round.
Lead investor Pinnacle Capital. Deal closes Q1 2025.
"""

entities = ner.extract(deal_text)
# Structured output:
# - COMPANY: Atlas Logistics
# - LOCATION: Seattle
# - MONEY: $50 million
# - DEAL_TYPE: Series B
# - INVESTOR: Pinnacle Capital
# - DATE: Q1 2025
```

--------------------------------

### Process Legal Contracts with Entity Extraction (Python)

Source: https://docs.memvid.com/concepts/entity-extraction

Example of processing legal contracts by first initializing a specific entity extractor for the legal domain. It then extracts entities like PARTY, DATE, and MONEY from a contract text and demonstrates building a structured summary of parties and dates.

```python
# Process legal contracts
ner = get_entity_extractor('claude', entity_types=[
    'PARTY', 'DATE', 'MONEY', 'TERM', 'JURISDICTION'
])

contract_text = "Agreement between Acme Corp and Beta Inc dated January 15, 2024..."
entities = ner.extract(contract_text)

# Build structured contract summary
parties = [e['name'] for e in entities if e['type'] == 'PARTY']
dates = [e['name'] for e in entities if e['type'] == 'DATE']
```

--------------------------------

### CrewAI Memory Management with Memvid SDK (Python)

Source: https://docs.memvid.com/frameworks/crewai

Demonstrates the creation and execution of a CrewAI crew, ensuring proper memory management by sealing the Memvid memory instance. This example assumes 'agents' and 'tasks' are defined elsewhere. It utilizes the 'crewai' and 'memvid_sdk' libraries.

```python
mem = use('crewai', 'knowledge.mv2', read_only=True)
try:
    # Create and run crew
    crew = Crew(agents=[...], tasks=[...])
    result = crew.kickoff()
finally:
    mem.seal()
```

--------------------------------

### Memvid CLI: File Verification

Source: https://docs.memvid.com/architecture/overview

Commands for checking the integrity of Memvid database files. The 'verify' command checks file health without making changes, with an option for a more thorough 'deep' verification. Usage requires the Memvid CLI to be installed.

```bash
# Quick verification
memvid verify notes.mv2

# Deep verification (slower, more thorough)
memvid verify notes.mv2 --deep
```

--------------------------------

### Ingest Data into Memvid

Source: https://docs.memvid.com/cli/create-and-put

Commands for creating and populating memvid projects. The `create` command initializes a new project file. The `put` command ingests data from local directories or files, with options for vector compression, tracking, tagging, and parallel processing.

```bash
# Documentation Knowledge Base
memvid create docs.mv2
memvid put docs.mv2 --input ./docs/ --vector-compression --track "documentation"
memvid put docs.mv2 --input ./api-reference/ --vector-compression \
  --track "api" \
  --tag "type=reference"

# Research Paper Archive
memvid create papers.mv2
for paper in ./papers/*.pdf; do
  memvid put papers.mv2 --input "$paper" --vector-compression \
    --track "research" \
    --tag "source=arxiv"
done

# Code Repository
memvid create codebase.mv2
memvid put codebase.mv2 --input ./src/ --vector-compression \
  --parallel-segments \
  --track "source"
memvid put codebase.mv2 --input ./tests/ --vector-compression --track "tests"
memvid put codebase.mv2 --input ./docs/ --vector-compression --track "docs"
```

--------------------------------

### Deduplication Statistics Output (JSON)

Source: https://docs.memvid.com/concepts/deduplication

Example JSON output from the 'memvid stats' command, detailing deduplication metrics. It shows the total number of frames, unique content hashes, duplicate frames that were prevented, and the configuration of the sketch track if it exists (variant and whether it's present).

```json
{
  "frame_count": 1250,
  "unique_content_hashes": 1248,
  "duplicate_frames_prevented": 127,
  "has_sketch_track": true,
  "sketch_variant": "medium"
}
```

--------------------------------

### Enable Vector Compression for Embeddings (Bash & Python)

Source: https://docs.memvid.com/concepts/capacity-and-plans

Demonstrates how to enable 16x compression for embeddings, reducing storage size. This can be done via the command line or through the SDK.

```bash
memvid put knowledge.mv2 --input docs/ --vector-compression
```

```python
mem.put_many(docs, enable_embedding=True, vector_compression=True)
```

--------------------------------

### Consistent Embedder Usage in Python

Source: https://docs.memvid.com/errors/reference

Illustrates how to maintain consistent embedder usage in Python applications to avoid vector dimension mismatch errors. It provides examples for initializing a memory object and applying the same embedding model across put and find operations.

```python
# Ensure consistent embedder usage
mem = use('basic', 'knowledge.mv2')

# Use same model for put and find
mem.put(text='content', embedding_model='bge-small')
mem.find('query', embedding_model='bge-small')
```

--------------------------------

### Configure and Use External Embedding Providers

Source: https://docs.memvid.com/python-sdk/overview

Details the configuration and usage of various external embedding providers like OpenAI, Gemini, Mistral, Cohere, Voyage, NVIDIA, and HuggingFace. It shows how to instantiate them with API keys and models, and use them with `put_many` or `find` operations. A factory function `get_embedder` is also demonstrated.

```python
from memvid_sdk.embeddings import (
    OpenAIEmbeddings,
    GeminiEmbeddings,
    MistralEmbeddings,
    CohereEmbeddings,
    VoyageEmbeddings,
    NvidiaEmbeddings,
    HuggingFaceEmbeddings,
    get_embedder
)

# OpenAI
openai = OpenAIEmbeddings(
    api_key=os.environ['OPENAI_API_KEY'],
    model='text-embedding-3-small'  # or 'text-embedding-3-large'
)

# Gemini
gemini = GeminiEmbeddings(
    api_key=os.environ['GEMINI_API_KEY'],
    model='text-embedding-004'
)

# Mistral
mistral = MistralEmbeddings(
    api_key=os.environ['MISTRAL_API_KEY']
)

# Cohere
cohere = CohereEmbeddings(
    api_key=os.environ['COHERE_API_KEY'],
    model='embed-english-v3.0'
)

# Voyage
voyage = VoyageEmbeddings(
    api_key=os.environ['VOYAGE_API_KEY'],
    model='voyage-3'
)

# NVIDIA
nvidia = NvidiaEmbeddings(
    api_key=os.environ['NVIDIA_API_KEY']
)

# HuggingFace (local)
hf = HuggingFaceEmbeddings(model='all-MiniLM-L6-v2')

# Factory function
embedder = get_embedder('openai', api_key='...')

# Use with put_many
mem.put_many(docs, embedder=openai)

# Use with find
mem.find('query', embedder=gemini)
```

--------------------------------

### Memvid CLI: Troubleshooting Transcription Output

Source: https://docs.memvid.com/concepts/audio-video

This section provides bash commands for troubleshooting common transcription issues with Memvid. It covers checking for and installing necessary Whisper models, and re-attempting transcription with an explicit language setting. These commands rely on the Memvid CLI and its model management capabilities.

```bash
# Check if Whisper model is installed
memvid models list

# Install required model
memvid models install whisper-small

# Try with explicit language
memvid put memory.mv2 --input audio.mp3 --language en
```

--------------------------------

### Memvid Encrypted File JSON Output

Source: https://docs.memvid.com/concepts/encryption

Example of JSON output from Memvid's lock command when the `--json` flag is used. This output provides detailed information about the encryption process, including source and destination files, sizes, and the ciphers used.

```json
{
  "status": "success",
  "source": "memory.mv2",
  "destination": "memory.mv2e",
  "original_size": 54938189,
  "encrypted_size": 54938301,
  "cipher": "AES-256-GCM",
  "kdf": "Argon2id"
}
```

--------------------------------

### Diagnose and Resolve File Lock Issues in Memvid

Source: https://docs.memvid.com/errors/troubleshooting

This section details how to diagnose and resolve 'File is locked' errors in Memvid. It includes bash commands to find the locking process and Python code to open files in read-only mode.

```bash
# Find process holding the lock
lsof knowledge.mv2

# Check for zombie processes
ps aux | grep memvid
```

```bash
# Kill the blocking process (if stuck)
kill -9 <PID>
```

```python
mem = use('basic', 'knowledge.mv2', read_only=True)
```

--------------------------------

### Consistent Embedder Usage in Node.js

Source: https://docs.memvid.com/errors/reference

Demonstrates how to ensure consistent embedder usage in Node.js applications to prevent vector dimension mismatch errors. It shows examples of initializing a memory object and using the same embedding model for both putting and finding data.

```typescript
// Ensure consistent embedder usage
const mem = await use('basic', 'knowledge.mv2');

// Use same model for put and find
await mem.put({ text: 'content', embeddingModel: 'bge-small' });
await mem.find('query', { embeddingModel: 'bge-small' });
```

--------------------------------

### Memvid CLI Advanced Options with Ollama

Source: https://docs.memvid.com/concepts/local-models

Demonstrates advanced command-line options for querying with Ollama models, including adjusting context, filtering by scope, and querying specific frames.

```bash
# More context for complex questions
memvid ask knowledge.mv2 \
  --question "Explain the architecture in detail" \
  --use-model "ollama:qwen2.5:3b" \
  --top-k 15 \
  --snippet-chars 800

# Filter by scope
memvid ask knowledge.mv2 \
  --question "What API endpoints exist?" \
  --use-model "ollama:qwen2.5:1.5b" \
  --scope "mv2://api/"

# Time-travel query
memvid ask knowledge.mv2 \
  --question "What was the status?" \
  --use-model "ollama:qwen2.5:1.5b" \
  --as-of-frame 100
```

--------------------------------

### Resolve CapacityExceeded Errors in Memvid CLI

Source: https://docs.memvid.com/troubleshooting/cli

This snippet covers solutions for the `CapacityExceeded` error in the Memvid CLI. It includes commands to list and delete frames, compact files, create new files with larger capacities, and check current usage. Ensure you have sufficient disk space or manage your Memvid files effectively.

```bash
# List frames to find candidates for deletion
memvid timeline myfile.mv2 --limit 100

# Delete a specific frame
memvid delete myfile.mv2 --frame-id 42 --yes

# Compact the file to reclaim space
memvid doctor myfile.mv2 --vacuum

# Create with specific size
memvid create newfile.mv2 --size 2GB

# Check current usage
memvid stats myfile.mv2
```

--------------------------------

### Check Memvid CLI Version (Bash)

Source: https://docs.memvid.com/errors/reference

Provides the command to check the currently installed version of the Memvid command-line interface. This is part of troubleshooting general system errors like MV999 - InternalError, ensuring the user is on a recent version.

```bash
# Check version
memvid --version
```

--------------------------------

### Troubleshoot: No Tables Detected (Specific Methods)

Source: https://docs.memvid.com/concepts/table-extraction

If aggressive mode doesn't work, try specific table detection methods like 'stream' or 'lattice'. 'stream' is good for tables without clear borders, while 'lattice' is for bordered tables.

```bash
# Try specific method
memvid tables import memory.mv2 --input report.pdf --method stream
memvid tables import memory.mv2 --input report.pdf --method lattice
```

--------------------------------

### Initialize Memvid with Semantic Kernel Adapter

Source: https://docs.memvid.com/frameworks/semantic-kernel

Quickly initializes Memvid using the 'semantic-kernel' adapter, connecting to a specified knowledge base file ('knowledge.mv2'). This allows access to Semantic Kernel plugins.

```python
from memvid_sdk import use

# Open with Semantic Kernel adapter
mem = use('semantic-kernel', 'knowledge.mv2')

# Access SK plugins
plugin = mem.as_plugin()
```

--------------------------------

### Rebuild Time Index for Memvid Files

Source: https://docs.memvid.com/troubleshooting/cli

This solution addresses `TimeIndexMissing` errors or failures in `memvid verify --deep`. It provides the specific command to rebuild the time index for a Memvid file and then re-run the deep verification to confirm the fix.

```bash
memvid doctor myfile.mv2 --rebuild-time-index
memvid verify myfile.mv2 --deep
```

--------------------------------

### Python SDK: Q&A with Different Ollama Models

Source: https://docs.memvid.com/concepts/local-models

Shows how to use the Memvid Python SDK to query with different Ollama models, demonstrating flexibility for quick answers with smaller models or detailed analysis with larger ones.

```python
# Quick answer with small model
quick_response = mem.ask(
    "What is this document about?",
    model="ollama:qwen2.5:0.5b"
)

# Detailed analysis with larger model
detailed_response = mem.ask(
    "Provide a comprehensive analysis of the findings",
    model="ollama:qwen2.5:3b",
    k=15
)
```

--------------------------------

### Add Timestamp for Unique Audit Trails (Bash)

Source: https://docs.memvid.com/concepts/deduplication

This command adds a timestamp to the input file when putting it into memory, ensuring each entry is unique for audit trail purposes. It utilizes the `date -u +%s` command to get the current UTC timestamp.

```bash
# Add timestamp to make each entry unique
memvid put memory.mv2 --input report.pdf --timestamp "$(date -u +%s)"
```

--------------------------------

### Available Tools

Source: https://docs.memvid.com/frameworks/langchain

Lists the tools provided by the Memvid LangChain adapter, including their purpose.

```APIDOC
## Available Tools

The LangChain adapter provides three tools:

| Tool          | Description                                           |
| ------------- | ----------------------------------------------------- |
| `memvid_put`  | Store documents in memory with title, label, and text |
| `memvid_find` | Search for relevant documents by query                |
| `memvid_ask`  | Ask questions with RAG-style answer synthesis         |
```

--------------------------------

### Automatic Enrichment of Memories

Source: https://docs.memvid.com/node-sdk/overview

This snippet demonstrates how to automatically enrich memories using a rules engine, retrieve and filter memories by entity, get the state of an entity, access statistics about memories, and list all entities. It relies on the 'mem' object for all operations.

```typescript
const result = await mem.enrich('rules');

const { cards, count } = await mem.memories();

const aliceCards = await mem.memories('Alice');

const alice = await mem.state('Alice');
console.log(alice.slots);
// { employer: 'Anthropic', role: 'Engineer', location: 'SF' }

const stats = await mem.memoriesStats();
console.log(stats.entityCount, stats.cardCount);

const entities = await mem.memoryEntities();
```

--------------------------------

### Python: Interact with Memvid Knowledge Base

Source: https://docs.memvid.com/frameworks/google-adk

This snippet demonstrates how to initialize the Memvid SDK, create a client with specific tools, and send messages to the AI for processing. It covers retrieving recent entries, searching for information, and handling AI responses. The system instruction configures the AI's behavior.

```python
from google import genai
from memvid_sdk import use

# Assume search_knowledge, get_timeline, get_stats, ask_question are defined elsewhere

mem = use('google-adk', 'knowledge.mv2', read_only=True)

def get_timeline(limit: int = 10) -> str:
    """Get recent entries from the knowledge base."""
    entries = mem.timeline(limit=limit)
    return "\n".join([f"- [{e.timestamp}] {e.title}" for e in entries])

def get_stats() -> str:
    """Get statistics about the knowledge base."""
    stats = mem.stats()
    return f"Documents: {stats['frame_count']}, Size: {stats['size_bytes']} bytes"

def ask_question(question: str) -> str:
    """Ask a question and get an AI-synthesized answer."""
    answer = mem.ask(question)
    return str(answer.get("answer", ""))

# Create client with all tools
client = genai.Client()
chat = client.chats.create(
    model="gemini-2.0-flash",
    config=types.GenerateContentConfig(
        tools=[search_knowledge, get_timeline, get_stats, ask_question],
        system_instruction="You are a helpful assistant with full access to a knowledge base."
    )
)

# Interactive session
response = chat.send_message("Show me recent entries and then search for authentication info")
print(response.text)
```

--------------------------------

### Troubleshoot Empty Search Results in Memvid

Source: https://docs.memvid.com/troubleshooting/cli

This snippet helps diagnose and resolve cases where Memvid search queries return empty results. It includes checking index status, rebuilding the lexical index, and trying different search modes (lexical, semantic, hybrid). Verify that content exists using `timeline` or `view` commands.

```bash
# Check index status
memvid stats myfile.mv2 --json | grep has_lex_index

# Rebuild the lexical index
memvid doctor myfile.mv2 --rebuild-lex-index

# Try different search modes
# Lexical only
memvid find myfile.mv2 --query "test" --mode lex

# Semantic only
memvid find myfile.mv2 --query "test" --mode sem

# Hybrid (default)
memvid find myfile.mv2 --query "test" --mode auto

# Check if content exists
memvid timeline myfile.mv2 --limit 5
memvid view myfile.mv2 --frame-id 1
```

--------------------------------

### Get Legal Domain Entity Extractor (Python)

Source: https://docs.memvid.com/concepts/entity-extraction

Initializes an entity extractor specifically for the legal domain using the 'claude' provider. It supports entity types like PARTY, COURT, CASE_NUMBER, etc. This is useful for processing legal documents.

```python
ner = get_entity_extractor('claude', entity_types=[
    'PARTY',
    'COURT',
    'JUDGE',
    'CASE_NUMBER',
    'STATUTE',
    'DATE',
    'JURISDICTION',
])
```

--------------------------------

### Vacuum After Deletions (Bash)

Source: https://docs.memvid.com/concepts/capacity-and-plans

Illustrates the process of reclaiming space after deletions by first checking statistics, then performing the vacuum operation, and finally checking statistics again to confirm space reclamation.

```bash
# Check before
memvid stats knowledge.mv2

# Vacuum
memvid doctor knowledge.mv2 --vacuum

# Check after
memvid stats knowledge.mv2
```

--------------------------------

### Memvid CLI: Combining Adaptive Search with Hybrid Search Modes

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Demonstrates that Memvid's adaptive search works seamlessly with different hybrid search modes, including automatic, lexical, and semantic search.

```bash
# Adaptive works with all search modes
memvid find memory.mv2 --query "term" --mode auto  # Hybrid + adaptive
memvid find memory.mv2 --query "term" --mode lex   # Lexical + adaptive
memvid find memory.mv2 --query "term" --mode sem   # Semantic + adaptive
```

--------------------------------

### Run Doctor Commands for Index Management

Source: https://docs.memvid.com/troubleshooting/cli

Runs doctor commands to diagnose and fix issues with memvid files. This includes previewing fixes, rebuilding various indexes (time, lexical, vector), vacuuming deleted frames, and applying multiple fixes.

```bash
# Preview what would be fixed
memvid doctor myfile.mv2 --plan-only

# Rebuild time index
memvid doctor myfile.mv2 --rebuild-time-index

# Rebuild lexical index
memvid doctor myfile.mv2 --rebuild-lex-index

# Rebuild vector index
memvid doctor myfile.mv2 --rebuild-vec-index

# Compact deleted frames
memvid doctor myfile.mv2 --vacuum

# Fix multiple issues
memvid doctor myfile.mv2 --rebuild-time-index --rebuild-lex-index
```

--------------------------------

### Memvid CLI: File Management Operations

Source: https://docs.memvid.com/comparisons/vector-databases

This bash script demonstrates how to manage Memvid knowledge base files, including copying for sharing, version control with Git, and creating backups. It highlights the single-file nature of Memvid's knowledge bases.

```bash
# Share your entire knowledge base
cp knowledge.mv2 /team/shared/

# Version control it
git add knowledge.mv2 && git commit -m "Updated docs"

# Backup
cp knowledge.mv2 knowledge.mv2.backup
```

--------------------------------

### Memvid Memory Instance Usage (Node.js)

Source: https://docs.memvid.com/introduction/glossary

Demonstrates how to open a Memvid memory file using the SDK and perform basic operations like inserting documents and searching. Requires the `@memvid/sdk` package.

```javascript
// Open a memory
const mem = await memvid.use("knowledge.mv2");

// The 'mem' object is your Memory instance
await mem.put({ text: "Hello world" });
await mem.find({ query: "hello" });
```

--------------------------------

### Memory Card JSON Structure Example

Source: https://docs.memvid.com/concepts/memory-cards

A memory card represents a single piece of structured knowledge as a JSON object. It includes fields like entity, slot, value, kind, confidence, source_frame, and extracted_at. The 'kind' field categorizes the type of information.

```json
{
  "entity": "John Smith",
  "slot": "job_title",
  "value": "Senior Engineer",
  "kind": "fact",
  "confidence": 0.92,
  "source_frame": "frame_abc123",
  "extracted_at": "2024-12-31T10:30:00Z"
}
```

--------------------------------

### Custom Finance Domain Entity Types for OpenAI

Source: https://docs.memvid.com/concepts/entity-extraction

Example of configuring the OpenAI entity extractor with custom entity types specific to the finance domain. This allows for more tailored extraction of financial concepts like 'INVESTOR', 'FUND', and 'DEAL_TYPE'. Input is a list of strings representing entity types.

```python
ner = get_entity_extractor('openai', entity_types=[
    'COMPANY',
    'INVESTOR',
    'FUND',
    'MONEY',
    'DEAL_TYPE',      # IPO, M&A, Series A
    'VALUATION',
    'EXECUTIVE',
    'DATE',
])
```

--------------------------------

### Get PII Locations (Python Utility)

Source: https://docs.memvid.com/concepts/pii-masking

This Python utility function `detect_pii` from the Memvid library identifies and returns the locations of all PII entities within a given text. It returns a list of objects, each containing the type, value, and start/end positions of the detected PII.

```python
from memvid import detect_pii

text = "Email john@test.com or call 555-1234"
pii_items = detect_pii(text)

for item in pii_items:
    print(f"Type: {item.type}, Value: {item.value}, Position: {item.start}-{item.end}")

# Type: email, Value: john@test.com, Position: 6-19
# Type: phone, Value: 555-1234, Position: 28-36
```

--------------------------------

### Building and Info for Sketch Tracks (Bash)

Source: https://docs.memvid.com/concepts/deduplication

Shows how to build and retrieve information about sketch tracks using the memvid CLI. Sketch tracks are used for fast pre-filtering of duplicate candidates in large memories. The 'build' command creates the sketch index, and 'info' displays its properties like variant, frame count, size, and average lookup time.

```bash
# Build sketch index (recommended for large memories)
memvid sketch build memory.mv2 --variant medium

# Check sketch stats
memvid sketch info memory.mv2
```

--------------------------------

### Gemini Entity Extraction Usage (TypeScript)

Source: https://docs.memvid.com/concepts/entity-extraction

Implement entity extraction using Gemini models in TypeScript with the Memvid SDK. This example shows the usage of the factory function and asynchronous extraction. Input is text, output is a promise resolving to a list of extracted entities.

```typescript
import { getEntityExtractor, GeminiEntities } from '@memvid/sdk';

const ner = getEntityExtractor('gemini', {
  entityTypes: ['COMPANY', 'PERSON'],
});

const entities = await ner.extract(text, 0.5);
```

--------------------------------

### Get CLIP Provider Factory Function (Python)

Source: https://docs.memvid.com/concepts/visual-embeddings

This snippet shows how to use the `get_clip_provider` factory function in Python to obtain a CLIP provider instance. It accepts a provider string, which can be a general type like 'local', 'openai', or 'gemini', or a more specific one including the model name.

```python
# Python
from memvid_sdk.clip import get_clip_provider

clip = get_clip_provider(provider)  # 'local', 'openai', 'gemini', 'openai:model-name'
```

--------------------------------

### Memvid CLI: Tuning for High-Precision Search

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Demonstrates how to configure Memvid for high-precision search by setting a strict `min-relevancy` threshold and using the `absolute` adaptive strategy.

```bash
# Strict threshold
memvid find memory.mv2 --query "term" \
  --min-relevancy 0.7 \
  --adaptive-strategy absolute
```

--------------------------------

### Memvid CLI: Accessing and Filtering Frame Data

Source: https://docs.memvid.com/concepts/audio-video

These bash commands demonstrate how to interact with Memvid using its Command Line Interface (CLI). The `view` command retrieves a specific frame's metadata, while the `timeline` command allows filtering frames based on content type. These commands require the Memvid CLI to be installed and configured.

```bash
# View frame with metadata
memvid view memory.mv2 --frame-id frame_abc --json

# Filter by media type
memvid timeline memory.mv2 --filter "content_type:audio/transcript"
```

--------------------------------

### Python AI Agent with LangChain/AutoGen Adapters

Source: https://docs.memvid.com/frameworks/overview

Illustrates integrating Memvid with AI agent frameworks like LangChain and AutoGen using Python. The code shows how to initialize the respective Memvid adapters and make their tools available for agent execution. Dependencies include `memvid_sdk`, `langgraph`, and `autogen`.

```python
# LangChain Agent
from langgraph.prebuilt import create_react_agent
mem = use('langchain', 'knowledge.mv2')
agent = create_react_agent(model, mem.tools)

# AutoGen
mem = use('autogen', 'knowledge.mv2')
assistant = AssistantAgent("helper", llm_config={"tools": mem.tools})
```

--------------------------------

### Error Handling in Memvid SDK

Source: https://docs.memvid.com/node-sdk/overview

This code illustrates robust error handling for operations performed with the Memvid SDK. It shows how to import specific error types from '@memvid/sdk' and use `try...catch` blocks to gracefully manage potential issues like capacity limits, invalid tickets, or disabled indexes. The example checks for `CapacityExceededError`, `LockedError`, and `VecIndexDisabledError`.

```typescript
import {
  MemvidError,
  CapacityExceededError,    // MV001
  TicketInvalidError,       // MV002
  TicketReplayError,        // MV003
  LexIndexDisabledError,    // MV004
  TimeIndexMissingError,    // MV005
  VerifyFailedError,        // MV006
  LockedError,              // MV007
  ApiKeyRequiredError,      // MV008
  MemoryAlreadyBoundError,  // MV009
  FrameNotFoundError,       // MV010
  VecIndexDisabledError,    // MV011
  CorruptFileError,         // MV012
  VecDimensionMismatchError // MV014
} from '@memvid/sdk';

try {
  await mem.put({ title: 'Large file', file: 'huge.bin' });
} catch (err) {
  if (err instanceof CapacityExceededError) {
    console.log('Storage capacity exceeded (MV001)');
  } else if (err instanceof LockedError) {
    console.log('File locked by another process (MV007)');
  } else if (err instanceof VecIndexDisabledError) {
    console.log('Enable vector index first (MV011)');
  }
}
```

--------------------------------

### Search and Ask Questions (Node.js, Python)

Source: https://docs.memvid.com/quickstart/sdk-recipes

Performs hybrid searches and asks questions against the Memvid data. Both SDKs allow specifying search parameters like 'k' (number of results) and 'scope'. The 'ask' function also supports modes and PII masking.

```typescript
const results = await mem.find("hybrid search", { k: 5, scope: "mv2://docs/" });
const answer = await mem.ask("What is hybrid search?", { k: 6, mode: "auto", maskPii: true });
```

```python
hits = mem.find("hybrid search", k=5, scope="mv2://docs/")["hits"]
answer = mem.ask("What is hybrid search?", k=6, mode="auto", mask_pii=True)
```

--------------------------------

### Get Healthcare Domain Entity Extractor (Python)

Source: https://docs.memvid.com/concepts/entity-extraction

Initializes an entity extractor for the healthcare domain using the 'openai:gpt-4o' provider. It supports entity types such as PATIENT, PROVIDER, MEDICATION, and DIAGNOSIS. Ideal for analyzing clinical notes or health records.

```python
ner = get_entity_extractor('openai:gpt-4o', entity_types=[
    'PATIENT',
    'PROVIDER',
    'MEDICATION',
    'DIAGNOSIS',
    'PROCEDURE',
    'DATE',
    'FACILITY',
])
```

--------------------------------

### OpenAI Entity Extraction Usage (TypeScript)

Source: https://docs.memvid.com/concepts/entity-extraction

Implement entity extraction using OpenAI models in TypeScript with the Memvid SDK. This demonstrates using the factory function to get an extractor and performing asynchronous extraction. Input is text, output is a promise resolving to a list of extracted entities.

```typescript
import { getEntityExtractor, OpenAIEntities } from '@memvid/sdk';

const ner = getEntityExtractor('openai', {
  entityTypes: ['COMPANY', 'PERSON', 'LOCATION', 'MONEY', 'DATE'],
});

// Or with specific model
const ner = getEntityExtractor('openai:gpt-4o-mini', {
  entityTypes: ['COMPANY', 'PERSON'],
});

const entities = await ner.extract(text, 0.5);
```

--------------------------------

### Memvid SDK: Python Basic Usage

Source: https://docs.memvid.com/concepts/no-vec-mode

This Python snippet outlines the essential steps for using the Memvid SDK. It covers creating a memory instance, enabling lexical search, adding content with metadata, performing immediate searches, and finally sealing the memory. It emphasizes the instant indexing and search capabilities provided by the SDK.

```python
from memvid_sdk import create

# Create memory (no embeddings by default)
mem = create('memory.mv2')

# Enable lexical search
mem.enable_lex()

# Add content - indexed instantly
mem.put(
    title='My Document',
    label='note',
    metadata={},
    text='Your document content...' 
)

# Search works immediately
results = mem.find('search query', k=5)

# Seal when done
mem.seal()
```

--------------------------------

### Memvid CLI: Best Practice - Tuning Based on Feedback

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Provides guidance on tuning Memvid's adaptive search parameters (`min-relevancy`, `adaptive-strategy`, `max-k`) based on whether too many or too few results are being returned.

```bash
if you're getting too many results:
memvid find memory.mv2 --query "term" \
  --min-relevancy 0.6  # Raise threshold
memvid find memory.mv2 --query "term" \
  --adaptive-strategy cliff  # Stricter cutoff

if you're missing results:
memvid find memory.mv2 --query "term" \
  --min-relevancy 0.3  # Lower threshold
memvid find memory.mv2 --query "term" \
  --max-k 100  # Raise cap
```

--------------------------------

### Memvid: Index Statistics JSON Output

Source: https://docs.memvid.com/concepts/indexes-and-tracks

Example JSON output from the `memvid stats --json` command. This structure details the status and size of the various indices within the memvid database, including frame count, and the byte sizes for lexical, vector, and time indices. It helps in understanding the database's composition.

```json
{
  "frame_count": 150,
  "has_lex_index": true,
  "has_vec_index": true,
  "has_time_index": true,
  "lex_index_bytes": 2202009,
  "vec_index_bytes": 1887436,
  "time_index_bytes": 310478
}
```

--------------------------------

### Memvid Node.js SDK: Basic Operations

Source: https://docs.memvid.com/architecture/overview

Demonstrates fundamental operations using the Memvid Node.js SDK, including opening/creating a database, adding content, searching, asking questions with an LLM, and properly closing the database connection. Requires the '@memvid/sdk' package.

```typescript
import { use } from '@memvid/sdk';

// Open or create
const mem = await use('basic', 'notes.mv2');

// Add content
await mem.put({ text: 'Introduction to neural networks...', title: 'NN Intro', label: 'intro' });

// Search
const results = await mem.find('neural networks', { k: 5 });

// Ask with LLM
const answer = await mem.ask('What is a neural network?', {
  model: 'openai:gpt-4o',
  modelApiKey: process.env.OPENAI_API_KEY
});

// Close properly
await mem.seal();
```

--------------------------------

### Enrichment with Gemini Cloud Engine (CLI)

Source: https://docs.memvid.com/concepts/memory-cards

This command leverages the Google Gemini API for fast and high-quality LLM-based enrichment. It requires a `GOOGLE_API_KEY` environment variable to be set and offers a low cost.

```bash
export GOOGLE_API_KEY=xxx
memvid enrich memory.mv2 --engine gemini
```

--------------------------------

### Playback Controls for Audio and Video with Memvid

Source: https://docs.memvid.com/concepts/audio-video

Control playback of audio and video segments using the 'memvid view' command. Options include playing the entire file, specific segments defined by start and end times in seconds, or just a preview. The '--frame-id' is required to specify the media.

```bash
# Play entire audio
memvid view memory.mv2 --frame-id frame_abc --play

# Play specific segment
memvid view memory.mv2 --frame-id frame_abc --play --start-seconds 30 --end-seconds 60

# Play from timestamp
memvid view memory.mv2 --frame-id frame_abc --play --start-seconds 120

# Play video
memvid view memory.mv2 --frame-id frame_xyz --play

# Play segment
memvid view memory.mv2 --frame-id frame_xyz --play --start-seconds 0 --end-seconds 30

# Preview mode (thumbnail)
memvid view memory.mv2 --frame-id frame_xyz --preview
```

--------------------------------

### Memvid CLI Operations

Source: https://docs.memvid.com/index

Demonstrates basic Memvid CLI commands for creating, adding data, finding information, and retrieving entity states from a knowledge file.

```bash
npm install -g memvid-cli

memvid create knowledge.mv2
echo "Alice works at Anthropic as a Senior Engineer." | memvid put knowledge.mv2
memvid find knowledge.mv2 --query "who works at AI companies"
memvid state knowledge.mv2 "Alice"
# { employer: 'Anthropic', role: 'Senior Engineer' }
```

--------------------------------

### Check Memvid Capacity Before Ingesting Data (Python)

Source: https://docs.memvid.com/concepts/tickets-and-capacity

This Python function checks if there is sufficient available capacity in a Memvid file before attempting to ingest new data. It uses the memvid_sdk to get file statistics and compares the available space with the required bytes. Returns True if enough capacity, False otherwise.

```python
from memvid_sdk import use

def check_capacity_before_ingest(path: str, needed_bytes: int) -> bool:
    """Check if there's enough capacity before ingesting."""
    mem = use('basic', path, read_only=True)
    stats = mem.stats()

    available = stats['capacity_bytes'] - stats['size_bytes']
    if available < needed_bytes:
        print(f"Warning: Only {available} bytes available, need {needed_bytes}")
        return False
    return True

# Usage
if check_capacity_before_ingest('knowledge.mv2', 50_000_000):
    mem = use('basic', 'knowledge.mv2')
    mem.put(file='large-file.pdf')
    mem.seal()
```

--------------------------------

### Migrating from Pinecone to Memvid

Source: https://docs.memvid.com/comparisons/vector-databases

Provides a Python code snippet for migrating data from Pinecone to Memvid. It includes initializing Pinecone, creating a Memvid memory object, and a note about needing the original documents as Pinecone only stores vectors.

```python
import pinecone
from memvid_sdk import create

# Export from Pinecone
pinecone.init(api_key="...")
index = pinecone.Index("my-index")

# Create Memvid memory
mem = create("knowledge.mv2")

# Fetch and migrate (you'll need your original documents)
# Pinecone doesn't store original text, only vectors
# This is why Memvid stores everything in one place
```

--------------------------------

### Handle Invalid Flag and File Exists Errors in Memvid CLI

Source: https://docs.memvid.com/troubleshooting/cli

This section covers two common Memvid CLI errors: `Invalid Flag` and `File Already Exists`. For invalid flags, it suggests checking the command's help documentation. For existing files, it shows how to remove the file or create a new one with a different name.

```bash
# For invalid flags:
memvid <command> --help

# For file already exists:
# Remove existing file first
rm myfile.mv2
memvid create myfile.mv2

# Or use a different name
memvid create myfile-v2.mv2
```

--------------------------------

### Run Memvid Full Health Check

Source: https://docs.memvid.com/errors/troubleshooting

This bash script performs a comprehensive health check on a Memvid file. It gathers basic information, verifies the file's integrity, checks for file locks using `lsof`, and lists associated tickets. This is useful for diagnosing general issues.

```bash
#!/bin/bash
# health-check.sh

FILE=$1

echo "=== Memvid Health Check ==="
echo "File: $FILE"
echo ""

echo "--- Basic Info ---"
memvid stats "$FILE" --json | jq '.'

echo ""
echo "--- Verification ---"
memvid verify "$FILE" --deep

echo ""
echo "--- Lock Status ---"
lsof "$FILE" 2>/dev/null || echo "No locks detected"

echo ""
echo "--- Ticket Info ---"
memvid tickets list "$FILE"
```

--------------------------------

### Get Memvid JSON Ticket Output

Source: https://docs.memvid.com/cli/tickets-and-capacity

Retrieves JSON formatted output for various ticket operations like sync, list, and revoke. This is useful for programmatic access and integration with other systems. The JSON output provides structured data on ticket status, memory details, and usage.

```json
{
  "success": true,
  "memory_id": "mem_abc123",
  "already_bound": true,
  "ticket": {
    "issuer": "memvid.com",
    "seq_no": 43,
    "capacity_bytes": 1073741824,
    "expires_in_secs": 2592000
  }
}
```

```json
{
  "path": "project.mv2",
  "active": true,
  "ticket": {
    "issuer": "memvid.com",
    "seq_no": 43,
    "capacity_bytes": 1073741824,
    "expires_at": "2024-02-20T10:30:00Z",
    "expires_in_secs": 2592000
  },
  "usage": {
    "current_bytes": 131691315,
    "capacity_bytes": 1073741824,
    "available_bytes": 942050509,
    "usage_percent": 12.2
  }
}
```

--------------------------------

### Personal Knowledge Management with Memvid

Source: https://docs.memvid.com/concepts/memory-cards

Demonstrates using Memvid for personal knowledge management, including ingesting notes, enriching the data, and querying information about entities.

```bash
# Ingest your notes
memvid put brain.mv2 --input ~/notes/

# Extract knowledge
memvid enrich brain.mv2 --engine groq

# Query what you know about someone
memvid state brain.mv2 --entity "Sarah from Marketing"
```

--------------------------------

### Repair Corrupted Files with Memvid Doctor

Source: https://docs.memvid.com/troubleshooting/cli

This snippet addresses file corruption issues in Memvid. It demonstrates commands for verifying file integrity (basic and deep), planning repairs, and rebuilding specific indexes like time, lexical, and vector indexes using `memvid doctor` and `memvid verify`.

```bash
# Quick verification
memvid verify myfile.mv2

# Deep verification
memvid verify myfile.mv2 --deep

# Preview repairs
memvid doctor myfile.mv2 --plan-only

# Rebuild specific index
memvid doctor myfile.mv2 --rebuild-time-index
memvid doctor myfile.mv2 --rebuild-lex-index
memvid doctor myfile.mv2 --rebuild-vec-index

# Verify single-file integrity
memvid verify-single-file myfile.mv2
```

--------------------------------

### Manage Embedding Dimensions for Vector Search

Source: https://docs.memvid.com/errors/reference

Handles VecDimensionMismatch errors by ensuring consistent embedding model usage between ingestion and search, or by rebuilding the vector index with the correct embeddings. It includes commands to check the current embedding dimension and examples for using matching models in search and ingestion.

```bash
# Check current embedding dimension
memvid stats knowledge.mv2 --json | jq '.vec_dimension'

# Use matching model for search
memvid find knowledge.mv2 --query "test" --embedding-model bge-small

# Rebuild vector index with new embeddings
memvid doctor knowledge.mv2 --rebuild-vec-index --embedding-model openai
```

--------------------------------

### Node.js: Initialize and Use Memvid SDK

Source: https://docs.memvid.com/frameworks/google-adk

This Node.js snippet demonstrates how to initialize the Memvid SDK in read-only mode and set up the Gemini API client. It includes error handling for missing API keys and shows the basic structure for interacting with the client within a try-finally block. Note that explicit closing is not required for the memory handle in Node.js.

```typescript
const mem = await use('google-adk', 'knowledge.mv2', { readOnly: true });
try {
  // Create agent and run queries
  const geminiKey = process.env.GEMINI_API_KEY ?? process.env.GOOGLE_API_KEY;
  if (!geminiKey) throw new Error("Set GEMINI_API_KEY (or legacy GOOGLE_API_KEY)");
  const genAI = new GoogleGenerativeAI(geminiKey);
  // ... use client
} finally {
  // No explicit close needed; dropping the handle releases the shared lock.
}
```

--------------------------------

### Memvid CLI: Tuning for Exploratory Search

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Explains how to tune Memvid for exploratory search by using a combined strategy with moderate `min-relevancy` and `max-k` settings.

```bash
# Combined strategy with moderate settings
memvid find memory.mv2 --query "term" \
  --min-relevancy 0.4 \
  --max-k 50 \
  --adaptive-strategy combined
```

--------------------------------

### Memvid CLI Commands

Source: https://docs.memvid.com/architecture/overview

Demonstrates common commands for interacting with Memvid using its command-line interface. Includes operations for creating, adding data, searching, asking questions, viewing timelines, and checking the health of `.mv2` files.

```bash
memvid create notes.mv2

memvid put notes.mv2 --input ./docs/ --vector-compression

memvid find notes.mv2 --query "machine learning" --mode auto

memvid ask notes.mv2 --question "What are the key points?"

memvid timeline notes.mv2 --limit 10

memvid doctor notes.mv2 --plan-only
```

--------------------------------

### Handle File Lock Errors in Memvid CLI

Source: https://docs.memvid.com/troubleshooting/cli

This section provides commands to resolve 'File is locked by another process' errors. It includes checking lock holders, nudging writers, identifying processes using `lsof`, and force-taking locks. Use the `--force` option with caution to avoid data corruption.

```bash
# Check who holds the lock
memvid who myfile.mv2

# Request the writer to release
memvid nudge myfile.mv2

# macOS/Linux: Find the process holding the lock
lsof myfile.mv2

# Wait and retry with timeout
memvid put myfile.mv2 --input doc.pdf --lock-timeout 5000

# Force takeover of stale lock
memvid put myfile.mv2 --input doc.pdf --force
```

--------------------------------

### LLM Q&A with ask()

Source: https://docs.memvid.com/python-sdk/overview

The ask() function enables question answering using large language models. It allows specifying retrieval parameters like k and mode, LLM settings including model, API key, and context size, privacy options like PII masking, and time filters. It also supports adaptive retrieval and can return only context without synthesis.

```python
answer = mem.ask(
    'What was decided about the budget?',
    k=8,
    mode='auto',

    # LLM settings
    model='gpt-4o-mini',
    api_key=os.environ['OPENAI_API_KEY'],
    llm_context_chars=120000,

    # Privacy
    mask_pii=True,

    # Time filters
    since=1704067200,
    until=1706745600,

    # Options
    context_only=False,    # Set True to skip synthesis

    # Adaptive retrieval
    adaptive=True,
    min_relevancy=0.5
)

print(answer['text'])
print(answer['hits'])  # Source documents
```

--------------------------------

### Python SDK: Q&A with Ollama

Source: https://docs.memvid.com/concepts/local-models

Uses the Memvid Python SDK to perform a Q&A task with a locally hosted Ollama model. Initializes the SDK with a basic configuration and specifies the model for inference.

```python
from memvid_sdk import use

mem = use('basic', 'knowledge.mv2')

# Ask with local Ollama model
response = mem.ask(
    "What are the main conclusions?",
    model="ollama:qwen2.5:1.5b",
    k=10
)

print(response['answer'])
```

--------------------------------

### Python for Batch Frame Ingestion

Source: https://docs.memvid.com/introduction/frames

Demonstrates the use of the `put_many()` method in Python for efficient batch ingestion of multiple frames. This method is significantly faster than individual insertions for bulk data loading.

```python
docs = [
    {'text': 'Content 1', 'title': 'Doc 1', 'label': 'docs'},
    {'text': 'Content 2', 'title': 'Doc 2', 'label': 'docs'},
    # ... thousands more
]

mem.put_many(docs)
```

--------------------------------

### AI-Powered Q&A with Cloud Providers

Source: https://docs.memvid.com/cli/search-and-ask

Utilize various cloud-based Large Language Models (LLMs) for AI-powered Q&A. This involves specifying the database file, the question, and the cloud provider's model identifier. API keys for the respective services are required.

```bash
# Use cloud providers
memvid ask knowledge.mv2 --question "Why is determinism important?" --use-model openai
memvid ask knowledge.mv2 --question "Why is determinism important?" --use-model "gemini-2.0-flash"
memvid ask knowledge.mv2 --question "Why is determinism important?" --use-model claude
memvid ask knowledge.mv2 --question "Why is determinism important?" --use-model "nvidia:meta/llama3-8b-instruct"
```

--------------------------------

### Memvid CLI: Semantic and Hybrid Search Modes

Source: https://docs.memvid.com/comparisons/vector-databases

This snippet shows how to enable and use semantic and hybrid search modes in the Memvid CLI. It requires rebuilding the vector index and demonstrates querying for conceptual information.

```bash
# Enable semantic search
memvid doctor knowledge.mv2 --rebuild-vec-index

# Semantic mode
memvid find knowledge.mv2 --query "cost reduction strategies" --mode vec

# Hybrid mode (auto-selects best approach)
memvid find knowledge.mv2 --query "budget optimization" --mode auto
```

--------------------------------

### Ingest Media Files

Source: https://docs.memvid.com/cli/create-and-put

Illustrates how to ingest media files such as images, audio, and video using the memvid CLI. Includes options for EXIF extraction from images and audio/video processing.

```bash
# Images with EXIF extraction
memvid put knowledge.mv2 --input photo.jpg

# Audio files
memvid put knowledge.mv2 --input recording.mp3 --audio

# Video files (stored without transcoding)
memvid put knowledge.mv2 --input video.mp4 --video
```

--------------------------------

### Inspect Memory File

Source: https://docs.memvid.com/cli/create-and-put

Inspect metadata and manifests of an existing memory file.

```APIDOC
## INSPECT /websites/memvid

### Description
Retrieves metadata and manifest information for an existing memory file.

### Method
`memvid open`

### Endpoint
`memvid open <FILE> [OPTIONS]`

### Parameters
#### Path Parameters
* **<FILE>** (string) - Required - The path to the memory file to inspect.

#### Query Parameters
* **--json** (boolean) - Optional - Emit JSON output. If not specified, a human-readable format is returned.

### Request Example
```bash
memvid open my-memory.mv2 --json
```

### Response
#### Success Response (JSON Output)
- **path** (string) - Path to the memory file.
- **version** (string) - Version of the memory file format.
- **created_at** (string) - Timestamp of file creation.
- **frame_count** (integer) - Total number of frames in the memory file.
- **size_bytes** (integer) - Current size of the memory file in bytes.
- **size_limit_bytes** (integer) - The maximum size limit of the memory file in bytes.
- **indexes** (object) - Information about enabled indexes:
  - **lex** (object) - Lexical index details:
    - **enabled** (boolean) - Whether the lexical index is enabled.
    - **term_count** (integer) - Number of terms in the lexical index.
  - **vec** (object) - Vector index details:
    - **enabled** (boolean) - Whether the vector index is enabled.
    - **vector_count** (integer) - Number of vectors stored.
    - **dimension** (integer) - Dimensionality of the vectors.
  - **time** (object) - Time index details:
    - **enabled** (boolean) - Whether the time index is enabled.
    - **entry_count** (integer) - Number of entries in the time index.
- **tracks** (object) - Number of frames per track.
- **binding** (object) - Information about memory binding:
  - **memory_id** (string) - The ID of the bound memory.
  - **bound_at** (string) - Timestamp when the memory was bound.

#### Response Example (JSON)
```json
{
  "path": "my-memory.mv2",
  "version": "2.1.0",
  "created_at": "2024-01-15T10:30:00Z",
  "frame_count": 1234,
  "size_bytes": 47395430,
  "size_limit_bytes": 536870912,
  "indexes": {
    "lex": { "enabled": true, "term_count": 12456 },
    "vec": { "enabled": true, "vector_count": 1234, "dimension": 384 },
    "time": { "enabled": true, "entry_count": 1234 }
  },
  "tracks": {
    "default": 890,
    "meetings": 234,
    "emails": 110
  },
  "binding": {
    "memory_id": "mem_abc123",
    "bound_at": "2024-01-15T10:30:00Z"
  }
}
```
```

--------------------------------

### Conceptual SQL Query for Compound Search

Source: https://docs.memvid.com/introduction/frames

Demonstrates a conceptual SQL query that leverages multiple indexes (lexical, vector, time) for compound searches. This highlights how Memvid's co-located indexes allow for efficient querying within a single file I/O operation.

```sql
-- Conceptual query (not actual syntax)
SELECT frames
WHERE text MATCH 'budget'           -- Lexical index
  AND vector SIMILAR TO query_vec   -- Vector index
  AND timestamp > '2024-01-01'      -- Time index

-- All indexes in one file = one I/O operation
```

--------------------------------

### SDK Usage - Python

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Demonstrates how to use the adaptive retrieval feature within a Python application using the memvid SDK.

```APIDOC
## SDK Usage - Python

Adaptive retrieval can be seamlessly integrated into your Python applications using the memvid SDK.

### Basic Usage (Adaptive Default)

When `adaptive` is set to `True` (or not specified, as it defaults to `True`), the `find` method returns results based on adaptive retrieval.

```python
from memvid import use

mem = use('basic', 'memory.mv2')

# Adaptive retrieval enabled by default
results = mem.find("authentication patterns")
print(f"Returned {len(results)} relevant results")
```

### Customizing Adaptive Retrieval

You can customize the adaptive retrieval behavior by passing specific parameters to the `find` method:

*   **`adaptive`**: Boolean, `True` to enable, `False` to disable.
*   **`min_relevancy`**: Float, the minimum relevance score for results.
*   **`max_k`**: Integer, the maximum number of results to return.
*   **`adaptive_strategy`**: String, the strategy to use (e.g., `"cliff"`, `"combined"`).

```python
# With custom adaptive settings
results = mem.find(
    "security best practices",
    adaptive=True,
    min_relevancy=0.5,
    max_k=25,
    adaptive_strategy="cliff"
)
```

### Disabling Adaptive Retrieval

To disable adaptive retrieval and use a fixed `top_k` value, set `adaptive` to `False` and provide the `top_k` parameter.

```python
# Adaptive retrieval disabled, fixed top_k
results = mem.find(
    "config files",
    adaptive=False,
    top_k=10
)
```
```

--------------------------------

### Using Tools with Agents (Python)

Source: https://docs.memvid.com/frameworks/langchain

Demonstrates how to integrate Memvid tools with LangChain agents in Python for creating reactive agent behaviors.

```APIDOC
## Using Tools with Agents (Python)

```python
from memvid_sdk import use
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

# Get Memvid tools
mem = use('langchain', 'knowledge.mv2')
tools = mem.tools

# Create agent with LangGraph
llm = ChatOpenAI(model="gpt-4o")
agent = create_react_agent(llm, tools)

# Run
inputs = {"messages": [("user", "Search for information about authentication")]}
result = agent.invoke(inputs)
print(result["messages"][-1].content)
```
```

--------------------------------

### Crash Recovery Verification in Memvid

Source: https://docs.memvid.com/comparisons/vector-databases

Demonstrates how to use the 'memvid verify' command to ensure the integrity of a Memvid '.mv2' file after potential system issues like power failures. The embedded Write-Ahead Log (WAL) ensures zero data loss.

```bash
# Power failure? No problem.
memvid verify knowledge.mv2
#  File integrity verified
```

--------------------------------

### Memvid SDK Utility Functions with Python

Source: https://docs.memvid.com/python-sdk/overview

Demonstrates the usage of utility functions provided by the memvid_sdk, including retrieving SDK information, flushing analytics, checking telemetry status, and verifying files. Requires the memvid_sdk to be imported and potentially initialized.

```python
from memvid_sdk import info, flush_analytics, is_telemetry_enabled, verify_single_file

# Get SDK info
sdk_info = info()
print(sdk_info['version'], sdk_info['platform'])

# Flush analytics
flush_analytics()

# Check telemetry status
enabled = is_telemetry_enabled()

# Verify no auxiliary files
verify_single_file('project.mv2')
```

--------------------------------

### Complete Visual Document Search Workflow (Python)

Source: https://docs.memvid.com/concepts/visual-embeddings

This comprehensive Python snippet outlines a full workflow for visual document search using the Memvid SDK. It covers initializing a CLIP provider (OpenAI, local, or Gemini), creating a memory instance, enabling lexical search, ingesting a PDF file with metadata, generating text embeddings for search queries, and finally sealing the memory and displaying statistics.

```python
from pathlib import Path
from memvid_sdk import create
from memvid_sdk.clip import get_clip_provider

# Configuration
PROVIDER = 'openai'  # 'local', 'openai', 'gemini'
PDF_PATH = 'annual_report.pdf'
OUTPUT_PATH = 'visual_search.mv2'

# Initialize
clip = get_clip_provider(PROVIDER)
print(f"CLIP Provider: {clip.name} ({clip.dimension} dims)")

# Create memory
if Path(OUTPUT_PATH).exists():
    Path(OUTPUT_PATH).unlink()

mem = create(OUTPUT_PATH)
mem.enable_lex()

# Ingest PDF
frame_id = mem.put(
    title=Path(PDF_PATH).stem,
    label='report',
    metadata={'source': 'finance', 'year': 2024},
    file=PDF_PATH,
)
print(f"Stored PDF as frame {frame_id}")

# Visual search queries
queries = [
    'revenue growth charts',
    'organizational structure',
    'sustainability initiatives',
    'executive portraits',
]

print("\nGenerating embeddings for visual search:")
for query in queries:
    embedding = clip.embed_text(query)
    print(f"  '{query}' -> {len(embedding)} dims")

# Seal and show stats
mem.seal()
stats = mem.stats()
print(f"\nFinal: {stats.get('frame_count', 0)} frames")
```

--------------------------------

### Video Tutorial Library Management with Memvid

Source: https://docs.memvid.com/concepts/audio-video

Manage video tutorials using memvid. Create a memory file, ingest tutorials with frame extraction and clip embeddings enabled, find tutorials by spoken content, and find them by visual content.

```bash
# Create tutorial library
memvid create tutorials.mv2

# Ingest with frame extraction
memvid put tutorials.mv2 --input ./tutorials/ --extract-frames --clip-embeddings

# Find by spoken content
memvid find tutorials.mv2 --query "how to configure webpack"

# Find by visual content
memvid find tutorials.mv2 --query "terminal with npm commands" --mode clip
```

--------------------------------

### Next.js Configuration for Memvid SDK Native Bindings

Source: https://docs.memvid.com/node-sdk/overview

This configuration snippet is for `next.config.ts` to ensure the Memvid Node.js SDK's native bindings (.node files) are correctly bundled with Vercel's serverless functions. It utilizes `outputFileTracingIncludes` to specify the necessary directories, including platform-specific ones like `@memvid/sdk-linux-x64-gnu` and the general SDK directory.

```typescript
// next.config.ts
import type { NextConfig } from 'next';

const nextConfig: NextConfig = {
  experimental: {
    outputFileTracingIncludes: {
      '/api/*': [
        './node_modules/@memvid/sdk-linux-x64-gnu/**/*',
        './node_modules/@memvid/sdk/**/*',
      ],
    },
  },
};

export default nextConfig;
```

--------------------------------

### Configure LLM: Cloud Provider API Keys

Source: https://docs.memvid.com/cli/search-and-ask

Configuring memvid to use cloud-based LLM providers by setting API keys as environment variables. Supports OpenAI, Gemini, and Anthropic models.

```bash
# OpenAI
export OPENAI_API_KEY=your-key
memvid ask knowledge.mv2 --question "..." --use-model openai

# Gemini
export GEMINI_API_KEY=your-key
memvid ask knowledge.mv2 --question "..." --use-model "gemini-2.0-flash"

# Anthropic
export ANTHROPIC_API_KEY=your-key
memvid ask knowledge.mv2 --question "..." --use-model claude
```

--------------------------------

### Python KnowledgeBase Class for Memvid SDK

Source: https://docs.memvid.com/examples/knowledge-base

The KnowledgeBase class provides an interface to interact with the Memvid SDK for managing and querying a company knowledge base. It allows adding documents, files, and folders, as well as searching and asking questions. Dependencies include memvid_sdk, pathlib, typing, datetime, and json.

```python
from memvid_sdk import use
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import json

class KnowledgeBase:
    """Company knowledge base with Memvid."""

    def __init__(self, memory_path: str = "knowledge.mv2"):
        self.mem = use('basic', memory_path, mode='auto')
        self.sources = {}

    def add_document(
        self,
        title: str,
        content: str,
        category: str,
        tags: Optional[List[str]] = None,
        source: Optional[str] = None
    ) -> int:
        """Add a document to the knowledge base."""
        frame_id = self.mem.put({
            "title": title,
            "label": category,
            "text": content,
            "tags": tags or [],
            "metadata": {
                "source": source,
                "added_at": datetime.now().isoformat()
            }
        })
        return frame_id

    def add_file(
        self,
        filepath: str,
        category: str,
        tags: Optional[List[str]] = None
    ) -> int:
        """Add a file to the knowledge base."""
        path = Path(filepath)
        return self.mem.put({
            "title": path.name,
            "label": category,
            "file": str(path.absolute()),
            "tags": tags or [],
            "metadata": {
                "source": "file",
                "original_path": str(path)
            }
        })

    def add_folder(self, folder_path: str, category: str) -> int:
        """Add all documents from a folder."""
        count = 0
        for path in Path(folder_path).rglob("*"):
            if path.is_file() and path.suffix in ['.pdf', '.md', '.txt', '.docx']:
                self.add_file(str(path), category)
                count += 1
        return count

    def search(self, query: str, category: Optional[str] = None, k: int = 10) -> List[Dict]:
        """Search the knowledge base."""
        scope = f"label:{category}" if category else None
        results = self.mem.find(query, k=k, scope=scope)
        return [
            {
                "title": hit.title,
                "snippet": hit.snippet,
                "score": hit.score,
                "category": hit.label
            }
            for hit in results.hits
        ]

    def ask(self, question: str, category: Optional[str] = None) -> Dict:
        """Ask a question."""
        scope = f"label:{category}" if category else None
        answer = self.mem.ask(question, k=5, scope=scope)
        return {
            "answer": answer.get("answer"),
            "sources": [s.get("title") for s in (answer.get("sources") or [])],
            "confidence": getattr(answer, 'confidence', None)
        }

    def get_categories(self) -> List[str]:
        """Get all categories."""
        stats = self.mem.stats()
        # This is a simplified version
        return list(set(entry.label for entry in self.mem.timeline(limit=1000).entries))
```

--------------------------------

### Podcast Library Management with Memvid

Source: https://docs.memvid.com/concepts/audio-video

Organize and search a podcast library using memvid. Create a memory file, ingest episodes with metadata including show, episode number, and guests, search across episodes by query, and view a timeline of episodes.

```bash
# Create podcast memory
memvid create podcasts.mv2

# Ingest episodes with metadata
memvid put podcasts.mv2 --input episode-42.mp3 \
  --metadata '{"show": "Tech Talk", "episode": 42, "guests": ["Alice", "Bob"]}'

# Search across all episodes
memvid find podcasts.mv2 --query "machine learning trends"

# Timeline of episodes
memvid timeline podcasts.mv2 --reverse
```

--------------------------------

### Create React Agent with Memvid Tools (Python)

Source: https://docs.memvid.com/frameworks/langchain

Demonstrates creating a LangChain React Agent in Python using Memvid tools. It involves initializing the language model, fetching Memvid tools, and invoking the agent with a user query.

```python
from memvid_sdk import use
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

# Get Memvid tools
mem = use('langchain', 'knowledge.mv2')
tools = mem.tools

# Create agent with LangGraph
llm = ChatOpenAI(model="gpt-4o")
agent = create_react_agent(llm, tools)

# Run
inputs = {"messages": [("user", "Search for information about authentication")]}
result = agent.invoke(inputs)
print(result["messages"][-1].content)
```

--------------------------------

### Memvid Best Practice: Export for Integration

Source: https://docs.memvid.com/concepts/memory-cards

Explains how to export data from Memvid in various formats for integration with other tools and systems, such as graph databases.

```bash
# For graph databases
memvid export memory.mv2 --format ntriples --out facts.nt
```

--------------------------------

### Initialize Memvid with OpenAI Adapter (Node.js)

Source: https://docs.memvid.com/frameworks/openai

Initializes the Memvid SDK using the 'openai' adapter and specifies the knowledge base file. This makes Memvid's functions available in a format compatible with OpenAI.

```typescript
import { use } from '@memvid/sdk';

// Open with OpenAI adapter
const mem = await use('openai', 'knowledge.mv2');

// Access function schemas
const functions = mem.functions;  // Array of function schemas
```

--------------------------------

### Troubleshoot: No Tables Detected (Aggressive Mode)

Source: https://docs.memvid.com/concepts/table-extraction

When no tables are detected, try the 'aggressive' mode. This mode attempts more advanced detection techniques but may increase processing time.

```bash
# Try aggressive mode
memvid tables import memory.mv2 --input report.pdf --mode aggressive
```

--------------------------------

### Customer Information Management with Memvid

Source: https://docs.memvid.com/concepts/memory-cards

Demonstrates using Memvid to manage customer information by ingesting support tickets, extracting facts, and querying customer states.

```bash
# Ingest support tickets
memvid put customers.mv2 --input tickets/

# Extract customer facts
memvid enrich customers.mv2 --engine groq

# Get customer state
memvid state customers.mv2 --entity "customer_12345"
```

--------------------------------

### Sync Notion Pages to Knowledge Base

Source: https://docs.memvid.com/examples/knowledge-base

Python function to synchronize pages from a Notion database into a KnowledgeBase object. It utilizes the 'notion-client' library and requires a Notion API token. The function extracts page titles and content, categorizing them as 'notion'.

```python
from notion_client import Client

notion = Client(auth=os.environ["NOTION_TOKEN"])

def sync_notion_pages(database_id: str, kb: KnowledgeBase):
    """Sync pages from a Notion database."""
    pages = notion.databases.query(database_id=database_id)

    for page in pages["results"]:
        # Extract content
        title = page["properties"]["Name"]["title"][0]["plain_text"]
        blocks = notion.blocks.children.list(page["id"])
        content = extract_text_from_blocks(blocks)

        kb.add_document(
            title=title,
            content=content,
            category="notion",
            tags=["notion", "synced"]
        )
```

--------------------------------

### Use Local Embedding Models (No API Required)

Source: https://docs.memvid.com/python-sdk/overview

Demonstrates how to use local embedding models without requiring external API keys. It shows how to specify a local model during the `put` operation and lists the available local models with their dimensionality and characteristics.

```python
from memvid_sdk.embeddings import LOCAL_EMBEDDING_MODELS

mem.put(
    'Title', 'label', {},
    text='content',
    enable_embedding=True,
    embedding_model=LOCAL_EMBEDDING_MODELS['BGE_SMALL']  # 384d, fast
)

# Available local models
LOCAL_EMBEDDING_MODELS['BGE_SMALL']   # 384d - fastest
LOCAL_EMBEDDING_MODELS['BGE_BASE']    # 768d - balanced
LOCAL_EMBEDDING_MODELS['NOMIC']       # 768d - general purpose
LOCAL_EMBEDDING_MODELS['GTE_LARGE']   # 1024d - highest quality
```

--------------------------------

### Memvid Python SDK Usage

Source: https://docs.memvid.com/architecture/overview

Illustrates how to use the Memvid Python SDK for common tasks. Covers opening/creating files, adding single or batch documents, performing searches, asking questions using LLMs, and properly sealing the file.

```python
from memvid_sdk import use

# Open or create
mem = use('basic', 'notes.mv2')

# Add content
mem.put(text="Introduction to neural networks...", title="NN Intro")

# Batch add (100-200x faster)
mem.put_many([
    {'text': 'Chapter 1...', 'title': 'Ch 1'},
    {'text': 'Chapter 2...', 'title': 'Ch 2'},
])

# Search
results = mem.find('neural networks', k=5)

# Ask with LLM
answer = mem.ask('What is a neural network?', model='openai:gpt-4o')

# Close properly
mem.seal()
```

--------------------------------

### FastAPI Web API for Knowledge Base

Source: https://docs.memvid.com/examples/knowledge-base

This FastAPI application exposes endpoints to interact with the Memvid KnowledgeBase. It allows searching, asking questions, adding documents, and retrieving categories. Dependencies include FastAPI, Pydantic, and the KnowledgeBase class. Request and response models are defined using Pydantic.

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List

# Assuming KnowledgeBase class is defined in the same file or imported
# from .knowledge_base import KnowledgeBase 

app = FastAPI(title="Knowledge Base API")
kb = KnowledgeBase("company-kb.mv2")


class SearchRequest(BaseModel):
    query: str
    category: Optional[str] = None
    limit: int = 10


class AskRequest(BaseModel):
    question: str
    category: Optional[str] = None


class AddDocumentRequest(BaseModel):
    title: str
    content: str
    category: str
    tags: Optional[List[str]] = None


@app.post("/search")
async def search(request: SearchRequest):
    results = kb.search(request.query, request.category, request.limit)
    return {"results": results, "total": len(results)}


@app.post("/ask")
async def ask(request: AskRequest):
    return kb.ask(request.question, request.category)


@app.post("/documents")
async def add_document(request: AddDocumentRequest):
    frame_id = kb.add_document(
        request.title,
        request.content,
        request.category,
        request.tags
    )
    return {"frame_id": frame_id, "status": "added"}


@app.get("/categories")
async def get_categories():
    return {"categories": kb.get_categories()}
```

--------------------------------

### Memvid Embedding Providers Configuration

Source: https://docs.memvid.com/sdks/python

Demonstrates how to configure various embedding providers with the Memvid SDK, including OpenAI, Gemini, Mistral, and local models. It shows initializing embedding objects with API keys and how to specify a local embedding model when putting data.

```python
from memvid_sdk.embeddings import (
    OpenAIEmbeddings,
    GeminiEmbeddings,
    MistralEmbeddings,
    CohereEmbeddings,
    VoyageEmbeddings,
    NvidiaEmbeddings,
    LOCAL_EMBEDDING_MODELS
)
import os

# OpenAI
openai = OpenAIEmbeddings(api_key=os.environ['OPENAI_API_KEY'])

# Gemini
gemini = GeminiEmbeddings(api_key=os.environ['GEMINI_API_KEY'])

# Mistral
mistral = MistralEmbeddings(api_key=os.environ['MISTRAL_API_KEY'])

# Local models (no API required)
mem.put(
    title='Doc',
    label='test',
    metadata={},
    text='Content',
    enable_embedding=True,
    embedding_model=LOCAL_EMBEDDING_MODELS['BGE_SMALL']
)
```

--------------------------------

### Dockerizing Memvid CLI with Runtime Password and Query

Source: https://docs.memvid.com/concepts/encryption

This Dockerfile sets up a Memvid CLI environment. It copies an encrypted memory file and configures the container to accept the password and query as runtime environment variables. The CMD instruction first unlocks the memory and then performs a search query.

```dockerfile
FROM memvid/cli:latest

# Password passed at runtime
ENV MEMVID_PASSWORD=""

COPY memory.mv2e /data/

CMD echo "$MEMVID_PASSWORD" | memvid unlock /data/memory.mv2e --password-stdin --out /data/memory.mv2 && \
    memvid find /data/memory.mv2 --query "$QUERY"
```

```bash
docker run -e MEMVID_PASSWORD="secret" -e QUERY="search term" myimage
```

--------------------------------

### Initialize Memvid with OpenAI Adapter (Python)

Source: https://docs.memvid.com/frameworks/openai

Initializes the Memvid SDK in Python using the 'openai' adapter and specifies the knowledge base. This prepares Memvid's functions for use with OpenAI's API.

```python
from memvid_sdk import use

# Open with OpenAI adapter
mem = use('openai', 'knowledge.mv2')

# Access function schemas
functions = mem.functions  # OpenAI function schemas
```

--------------------------------

### Initialize Memvid with Vercel AI Adapter

Source: https://docs.memvid.com/frameworks/vercel-ai

Initializes Memvid using the 'vercel-ai' adapter and specifies the knowledge base file ('knowledge.mv2'). It then accesses the available Vercel AI tools.

```typescript
import { use } from '@memvid/sdk';

// Open with Vercel AI adapter
const mem = await use('vercel-ai', 'knowledge.mv2');

// Access Vercel AI tools
const tools = mem.tools;  // Object with tool definitions
```

--------------------------------

### Python: Manage Memvid Memory and Kernel

Source: https://docs.memvid.com/frameworks/semantic-kernel

Demonstrates how to initialize and manage memory and the kernel in Memvid using Python. It highlights the use of read-only mode for memory retrieval and proper sealing of the memory upon completion. This pattern is crucial for efficient resource management.

```python
mem = use('semantic-kernel', 'knowledge.mv2', read_only=True)
try:
    kernel = sk.Kernel()
    kernel.add_plugin(mem.as_plugin(), "memvid")
    # ... use kernel
finally:
    mem.seal()
```

--------------------------------

### Replay Session with Different Models (CLI)

Source: https://docs.memvid.com/concepts/time-travel-replay

Compares session replay results by using different LLM models with audit and diff flags. This allows for a direct comparison of model performance on the same session data.

```bash
memvid session replay knowledge.mv2 --session <id> --audit --use-model gemini:gemini-2.5-pro --diff
memvid session replay knowledge.mv2 --session <id> --audit --use-model claude:claude-4-sonnet --diff
```

--------------------------------

### Create a new Memvid memory (CLI)

Source: https://docs.memvid.com/concepts/capacity-and-plans

Creates a new memory file with default settings. The default capacity is 50 MB, suitable for most use cases. Subsequent commands show how to add content, search, and view stats.

```bash
memvid create knowledge.mv2
```

--------------------------------

### Incremental Enrichment with Memvid

Source: https://docs.memvid.com/concepts/memory-cards

Shows how Memvid performs incremental enrichment by default, processing only new frames. It also illustrates how to force a full re-enrichment.

```bash
# First run: processes all 100 frames
memvid enrich memory.mv2 --engine groq
# Enriched 100 frames, extracted 250 cards

# Add more documents
memvid put memory.mv2 --input new_docs/

# Second run: only processes new frames
memvid enrich memory.mv2 --engine groq
# Enriched 15 frames (85 already enriched), extracted 42 cards
```

```bash
memvid enrich memory.mv2 --engine claude --force
```

--------------------------------

### Verify File Integrity Before Sharing

Source: https://docs.memvid.com/cli/maintenance-and-tickets

Provides commands to ensure the integrity and consistency of a Memvid knowledge base file before sharing it. This includes deep verification and checking file statistics.

```bash
# Verify integrity
memvid verify knowledge.mv2 --deep

# Check single-file
memvid verify-single-file knowledge.mv2

# Review stats
memvid stats knowledge.mv2
```

--------------------------------

### Model A/B Testing Replay with Diff (CLI)

Source: https://docs.memvid.com/concepts/time-travel-replay

Compares outputs from different models using a replayed session with frozen context. The `--diff` flag highlights the differences between the original model's answer and the new model's answer, facilitating A/B testing.

```bash
# Original used GPT-4o-mini, replay with Claude
memvid session replay knowledge.mv2 --session <id> \
  --audit \
  --use-model claude:claude-3-5-sonnet \
  --diff
```

--------------------------------

### Persisting and Recovering Data with Memvid

Source: https://docs.memvid.com/introduction/why-memvid

This snippet demonstrates how to store critical data using Memvid's `put` function and how the `use` function with the 'basic' module automatically recovers uncommitted writes upon reopening. This ensures data durability.

```memvid
mem.put(title="Important", text="Critical data...")

mem = use('basic', 'knowledge.mv2')  # Auto-recovers uncommitted writes
```

--------------------------------

### API Fetch Configuration

Source: https://docs.memvid.com/cli/create-and-put

Defines the structure for a JSON configuration file used with the `memvid api-fetch` command. This file specifies API endpoint details, request methods, headers, pagination strategies, and how to map API response data to memvid frame attributes like title, text, and URI.

```json
{
  "url": "https://api.example.com/documents",
  "method": "GET",
  "headers": {
    "Authorization": "Bearer ${API_TOKEN}"
  },
  "pagination": {
    "type": "cursor",
    "cursor_param": "after",
    "cursor_path": "$.meta.next_cursor"
  },
  "items_path": "$.data",
  "mapping": {
    "title": "$.name",
    "text": "$.content",
    "uri": "$.id"
  }
}
```

--------------------------------

### Ingesting from Stdin

Source: https://docs.memvid.com/cli/create-and-put

Ingest data directly from standard input, useful for piping data from other commands or sources.

```APIDOC
## Ingesting from Stdin

### Description
Ingests data piped from standard input into a Memvid memory. Useful for integrating with other command-line tools.

### Method
PUT

### Endpoint
/websites/memvid

### Parameters
#### Path Parameters
- **knowledge.mv2** (string) - Required - The target memory file.

#### Query Parameters
- **--vector-compression** (boolean) - Optional - Enables vector compression.
- **--title** (string) - Optional - Sets a custom title for the ingested data.
- **--track** (string) - Optional - Assigns the ingested data to a specific track.

### Request Example (Pipe text content)
```bash
echo "Important note to remember" | memvid put knowledge.mv2 --vector-compression
```

### Request Example (Pipe from curl)
```bash
curl -s https://api.example.com/data | memvid put knowledge.mv2 --vector-compression --title "API Response"
```

### Request Example (Pipe from another command)
```bash
cat log.txt | grep "ERROR" | memvid put knowledge.mv2 --vector-compression --track "errors"
```
```

--------------------------------

### List Downloaded Ollama Models

Source: https://docs.memvid.com/concepts/local-models

Lists all the AI models that have been downloaded and are available locally through Ollama. This command helps in managing your local model cache.

```bash
ollama list
```

--------------------------------

### Set Memvid API Key Environment Variable

Source: https://docs.memvid.com/cli/tickets-and-capacity

Shows how to set the MEMVID_API_KEY and MEMVID_API_URL environment variables for global configuration, enabling syncing without explicitly passing the API key.

```bash
# Set API key globally
export MEMVID_API_KEY=mv_live_xxx

# Then sync without --api-key flag
memvid tickets sync knowledge.mv2 --memory-id abc-123
```

--------------------------------

### Python RAG with LangChain Adapter

Source: https://docs.memvid.com/frameworks/overview

Demonstrates using the Memvid LangChain adapter in Python for Retrieval Augmented Generation (RAG). It shows how to initialize the adapter, create a LangChain agent with Memvid tools, and perform direct find/ask operations. Requires `memvid_sdk`, `langgraph`, and `langchain_openai`.

```python
from memvid_sdk import use
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI

mem = use('langchain', 'knowledge.mv2')
agent = create_react_agent(ChatOpenAI(model="gpt-4o"), mem.tools)
result = agent.invoke({"messages": [{"role": "user", "content": "What is..."}]})

# Or use find() + ask() directly
results = mem.find("search query", k=5)
answer = mem.ask("What is the main concept?")
```

--------------------------------

### LLM Q&A with ask()

Source: https://docs.memvid.com/node-sdk/overview

Asks a question to a Large Language Model (LLM) using context from the memory. Supports various LLM settings, privacy controls like PII masking, time filtering, and adaptive retrieval. The response includes the answer text and source documents if requested.

```typescript
const answer = await mem.ask('What was decided about the budget?', {
  k: 8,
  mode: 'auto',

  // LLM settings
  model: 'gpt-4o-mini',
  modelApiKey: process.env.OPENAI_API_KEY,
  llmContextChars: 120000,

  // Privacy
  maskPii: true,

  // Time filters
  since: 1704067200,
  until: 1706745600,

  // Options
  contextOnly: false,   // Set true to skip synthesis
  returnSources: true,  // Include source documents

  // Adaptive retrieval
  adaptive: true,
  minRelevancy: 0.5
});

console.log(answer.text);
console.log(answer.sources);
```

--------------------------------

### Encrypt and Decrypt Files with Passwords

Source: https://docs.memvid.com/python-sdk/overview

Demonstrates how to encrypt a file into a `.mv2e` capsule using a password and optionally force overwriting. It also shows how to decrypt the capsule back to its original format. Functions to check who has a lock and to nudge stale locks are also included.

```python
from memvid_sdk import lock, unlock, lock_who, lock_nudge

# Encrypt to .mv2e capsule
encrypted_path = lock(
    'project.mv2',
    password='secret',
    force=True
)

# Decrypt back to .mv2
decrypted_path = unlock(
    'project.mv2e',
    password='secret'
)

# Check who has the lock
lock_info = lock_who('project.mv2')

# Nudge stale lock
released = lock_nudge('project.mv2')
```

--------------------------------

### Create React Agent with Memvid Tools (Node.js)

Source: https://docs.memvid.com/frameworks/langchain

Shows how to create a LangChain React Agent using Memvid's tools in a Node.js environment. It initializes the LLM, retrieves Memvid tools, and sets up the agent for streaming responses.

```typescript
import { use } from '@memvid/sdk';
import { ChatOpenAI } from '@langchain/openai';
import { createReactAgent } from '@langchain/langgraph/prebuilt';
import { HumanMessage } from '@langchain/core/messages';

// Get Memvid tools
const mem = await use('langchain', 'knowledge.mv2');
const tools = mem.tools;

// Create agent with LangGraph
const llm = new ChatOpenAI({ model: 'gpt-4o' });
const agent = createReactAgent({ llm, tools });

// Run
const inputs = { messages: [new HumanMessage('Search for authentication info')] };
const stream = await agent.stream(inputs, { streamMode: 'values' });

for await (const { messages } of stream) {
  const lastMsg = messages[messages.length - 1];
  if (lastMsg.content) {
    console.log(lastMsg.content);
  }
}
```

--------------------------------

### Query Memories with memvid_sdk in Python

Source: https://docs.memvid.com/python-sdk/querying

This Python code snippet demonstrates how to query memories using the memvid_sdk library. It showcases the use of `find()` for keyword search, `ask()` for question answering with context, and `timeline()` for chronological retrieval.  The output of these methods mirrors the CLI JSON output format.

```python
from memvid_sdk import use

mv = use("basic", "notes.mv2")
hits = mv.find("deterministic", k=5, mode="lex")
answer = mv.ask("Why is the WAL embedded?", mode="auto", context_only=True)
timeline = mv.timeline(since=1730000000, until=1730003600, limit=10)
```

--------------------------------

### Ingest Text Files

Source: https://docs.memvid.com/cli/create-and-put

Shows how to ingest plain text, markdown, and HTML files using the memvid CLI, with an option for vector compression.

```bash
# Plain text
memvid put knowledge.mv2 --input notes.txt --vector-compression

# Markdown
memvid put knowledge.mv2 --input README.md --vector-compression

# HTML
memvid put knowledge.mv2 --input page.html --vector-compression
```

--------------------------------

### Perform Timeline Queries

Source: https://docs.memvid.com/python-sdk/overview

Shows how to query memory events within a specific time range using `since` and `until` parameters. Options for limiting results, reversing order, and querying as of a specific frame are also available.

```python
timeline = mem.timeline(
    limit=50,
    since=1704067200,
    until=1706745600,
    reverse=True,
    as_of_frame=100
)
```

--------------------------------

### Initialize and Use Read-Only Memory Component

Source: https://docs.memvid.com/frameworks/haystack

Shows how to initialize a memory component in read-only mode and integrate it into a pipeline. This is crucial for efficient retrieval and ensures the memory is properly sealed after use.

```python
mem = use('haystack', 'knowledge.mv2', read_only=True)
try:
    pipeline = Pipeline()
    pipeline.add_component("retriever", mem.as_retriever(top_k=10))
    # ... build and run pipeline
finally:
    mem.seal()
```

--------------------------------

### Memvid CLI: Troubleshooting - Too Few Results

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Offers solutions for troubleshooting scenarios where Memvid's adaptive search returns too few results, suggesting adjustments to `min-relevancy`, `max-k`, and the `adaptive-strategy`.

```bash
1. Lower `min-relevancy`:
   memvid find memory.mv2 --query "term" \
     --min-relevancy 0.3

2. Increase `max-k`:
   memvid find memory.mv2 --query "term" \
     --max-k 100

3. Try `relative` strategy:
   memvid find memory.mv2 --query "term" \
     --adaptive-strategy relative
```

--------------------------------

### Using Tools with Agents (Node.js)

Source: https://docs.memvid.com/frameworks/langchain

Demonstrates how to integrate Memvid tools with LangChain agents in Node.js for creating reactive agent behaviors.

```APIDOC
## Using Tools with Agents (Node.js)

```typescript
import { use } from '@memvid/sdk';
import { ChatOpenAI } from '@langchain/openai';
import { createReactAgent } from '@langchain/langgraph/prebuilt';
import { HumanMessage } from '@langchain/core/messages';

// Get Memvid tools
const mem = await use('langchain', 'knowledge.mv2');
const tools = mem.tools;

// Create agent with LangGraph
const llm = new ChatOpenAI({ model: 'gpt-4o' });
const agent = createReactAgent({ llm, tools });

// Run
const inputs = { messages: [new HumanMessage('Search for authentication info')] };
const stream = await agent.stream(inputs, { streamMode: 'values' });

for await (const { messages } of stream) {
  const lastMsg = messages[messages.length - 1];
  if (lastMsg.content) {
    console.log(lastMsg.content);
  }
}
```
```

--------------------------------

### Integrating External Embedding Providers

Source: https://docs.memvid.com/node-sdk/overview

This snippet demonstrates how to initialize and use various external embedding providers like OpenAI, Gemini, Mistral, Cohere, Voyage, and NVIDIA. It shows how to create instances of these providers with their respective API keys and models, and how to use them with 'mem.putMany' and 'mem.find' methods. A factory function 'getEmbedder' is also provided.

```typescript
import {
  OpenAIEmbeddings,
  GeminiEmbeddings,
  MistralEmbeddings,
  CohereEmbeddings,
  VoyageEmbeddings,
  NvidiaEmbeddings,
  getEmbedder
} from '@memvid/sdk';

const openai = new OpenAIEmbeddings({
  apiKey: process.env.OPENAI_API_KEY,
  model: 'text-embedding-3-small'  // or 'text-embedding-3-large'
});

const gemini = new GeminiEmbeddings({
  apiKey: process.env.GEMINI_API_KEY,
  model: 'text-embedding-004'
});

const mistral = new MistralEmbeddings({
  apiKey: process.env.MISTRAL_API_KEY
});

const cohere = new CohereEmbeddings({
  apiKey: process.env.COHERE_API_KEY,
  model: 'embed-english-v3.0'
});

const voyage = new VoyageEmbeddings({
  apiKey: process.env.VOYAGE_API_KEY,
  model: 'voyage-3'
});

const nvidia = new NvidiaEmbeddings({
  apiKey: process.env.NVIDIA_API_KEY
});

const embedder = getEmbedder('openai', { apiKey: '...' });

await mem.putMany(docs, { embedder: openai });

await mem.find('query', { embedder: gemini });
```

--------------------------------

### Use Ollama Model with Memvid CLI

Source: https://docs.memvid.com/concepts/local-models

Queries a Memvid knowledge base using a locally hosted Ollama model. This command specifies the knowledge base, the question, and the Ollama model to use.

```bash
memvid ask knowledge.mv2 \
  --question "What is the main topic?" \
  --use-model "ollama:qwen2.5:1.5b"
```

--------------------------------

### Memvid CLI Q&A with Ollama

Source: https://docs.memvid.com/concepts/local-models

Performs basic Q&A on a knowledge base using a specified local Ollama model. Includes an option for JSON output.

```bash
# Ask with local model
memvid ask knowledge.mv2 \
  --question "What are the key findings?" \
  --use-model "ollama:qwen2.5:1.5b"

# With JSON output
memvid ask knowledge.mv2 \
  --question "Summarize the main points" \
  --use-model "ollama:qwen2.5:1.5b" \
  --json
```

--------------------------------

### Multi-File Search with memvid CLI

Source: https://docs.memvid.com/cli/search-and-ask

Shows how to query multiple memory files simultaneously using the 'memvid ask' command. This includes specifying multiple file paths directly or using glob patterns to include a range of files.

```bash
# Search multiple files
memvid ask docs.mv2 code.mv2 notes.mv2 \
  --question "How does authentication work?"

# Using glob patterns
memvid ask ./memories/*.mv2 \
  --question "What are the main features?"
```

--------------------------------

### Time-Travel Queries in Memvid

Source: https://docs.memvid.com/comparisons/vector-databases

Demonstrates how to query data as it existed at a specific past frame or point in time using Memvid's 'as_of_frame' parameter. This feature allows for historical data analysis without complex versioning.

```python
results = mem.find("budget", as_of_frame=1000)
```

--------------------------------

### Sync Google Drive Files to Knowledge Base

Source: https://docs.memvid.com/examples/knowledge-base

Python function to synchronize documents from a Google Drive folder into a KnowledgeBase. It uses the Google Drive API ('google-api-python-client') and requires valid credentials. The function downloads file content and adds it to the KB, categorizing it as 'google-drive'.

```python
from googleapiclient.discovery import build

def sync_google_drive(folder_id: str, kb: KnowledgeBase, creds):
    """Sync documents from Google Drive."""
    service = build('drive', 'v3', credentials=creds)

    results = service.files().list(
        q=f"'{folder_id}' in parents",
        fields="files(id, name, mimeType)"
    ).execute()

    for file in results.get('files', []):
        # Download and add to KB
        content = download_file(service, file['id'])
        kb.add_document(
            title=file['name'],
            content=content,
            category="google-drive"
        )
```

--------------------------------

### Docker Compose for Memvid API and Web Services

Source: https://docs.memvid.com/examples/knowledge-base

Docker Compose configuration to define and run the Memvid API and web frontend services. It specifies build contexts, port mappings, volume mounts for data persistence, and dependency management between services.

```yaml
version: '3.8'
services:
  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
    environment:
      - MEMVID_FILE=/app/data/knowledge.mv2

  web:
    build: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      - api
```

--------------------------------

### Memvid CLI Basic Document Ingestion Workflows

Source: https://docs.memvid.com/cli/index

Shows common workflows for ingesting documents into Memvid memory files. This includes creating a memory file, adding text files and PDFs, and adding documents with metadata like titles, tracks, tags, and labels.

```bash
# Create memory and add documents
memvid create project.mv2

# Add a text file
cat notes.txt | memvid put project.mv2 --title "Notes"

# Add a PDF
memvid put project.mv2 --input report.pdf --title "Report"

# Add with metadata
memvid put project.mv2 --input doc.txt \
  --title "Meeting Notes" \
  --track meetings \
  --tag "date=2024-01-15" \
  --tag "attendees=alice,bob" \
  --label important
```

--------------------------------

### Ingest Document Files

Source: https://docs.memvid.com/cli/create-and-put

Demonstrates ingesting various document types including PDF, DOCX, XLSX, and PPTX. Includes an option for extracting tables from PDFs.

```bash
# PDF files
memvid put knowledge.mv2 --input report.pdf --vector-compression

# PDF with table extraction
memvid put knowledge.mv2 --input invoice.pdf --tables --vector-compression

# Word documents
memvid put knowledge.mv2 --input document.docx --vector-compression

# Excel spreadsheets
memvid put knowledge.mv2 --input data.xlsx --vector-compression

# PowerPoint presentations
memvid put knowledge.mv2 --input slides.pptx --vector-compression
```

--------------------------------

### Segment Compaction for Storage Efficiency

Source: https://docs.memvid.com/introduction/frames

This diagram illustrates the segment compaction process in Memvid. Frames are periodically grouped into segments within a '.mv2' file to optimize storage. These segments, along with various indexes (TOC, Lexical, Vector, Time, WAL), form the structure of the '.mv2' file.

```mermaid
graph TB
    subgraph MV2[".mv2 FILE"]
        subgraph Segments
            direction LR
            S0["Segment 0<br/>Frames 0-99"]
            S1["Segment 1<br/>Frames 100-199"]
            S2["Segment 2<br/>Frames 200-299"]
        end
        subgraph Indexes
            direction LR
            TOC["TOC"]
            LEX["Lexical"]
            VEC["Vector"]
            TIME["Time"]
            WAL["WAL"]
        end
    end

    Segments --> Indexes

    style S0 fill:#FF9900,color:#000
    style S1 fill:#FF9900,color:#000
    style S2 fill:#FF9900,color:#000
    style MV2 stroke:#FF9900,stroke-width:2px
```

--------------------------------

### Framework Adapters for Memvid Node.js SDK

Source: https://docs.memvid.com/node-sdk/overview

Illustrates how to integrate the Memvid Node.js SDK with various frameworks and libraries like LangChain, Vercel AI, OpenAI, CrewAI, AutoGen, and others using the `use` function.

```typescript
import { use } from '@memvid/sdk';

// Available adapters
const mem = await use('basic', 'file.mv2');
const langchain = await use('langchain', 'file.mv2');
const llamaindex = await use('llamaindex', 'file.mv2');
const vercelai = await use('vercel-ai', 'file.mv2');
const openai = await use('openai', 'file.mv2');
const crewai = await use('crewai', 'file.mv2');
const autogen = await use('autogen', 'file.mv2');
const haystack = await use('haystack', 'file.mv2');
const langgraph = await use('langgraph', 'file.mv2');
const semantickernel = await use('semantic-kernel', 'file.mv2');
const googleadk = await use('google-adk', 'file.mv2');
```

--------------------------------

### Basic Table Extraction with Memvid CLI

Source: https://docs.memvid.com/concepts/table-extraction

Demonstrates the fundamental commands for extracting tables from PDF documents using the Memvid CLI. Includes options for basic extraction and for embedding rows to enable semantic search.

```bash
# Extract tables from PDF
memvid put memory.mv2 --input report.pdf --tables

# Extract and embed rows for semantic search
memvid put memory.mv2 --input financial.pdf --tables --embed-rows
```

--------------------------------

### Search with JSON Output

Source: https://docs.memvid.com/cli/search-and-ask

Perform a search query and receive results in JSON format. This is useful for programmatic integration. The command requires a database file and a query string.

```bash
memvid find knowledge.mv2 --query "database schema" --json
```

--------------------------------

### Python: Initialize Cohere Embeddings

Source: https://docs.memvid.com/concepts/embedding-models

Shows how to initialize Cohere embeddings using direct instantiation or a factory function. Supports different Cohere models for English and multilingual content.

```python
from memvid_sdk.embeddings import CohereEmbeddings, get_embedder

# Direct initialization
embedder = CohereEmbeddings(model='embed-english-v3.0')

# Or use factory
embedder = get_embedder('cohere', model='embed-multilingual-v3.0')

# Generate embeddings
embeddings = embedder.embed_documents(['Text 1', 'Text 2'])
query_vec = embedder.embed_query('search query')

```

--------------------------------

### Run Interactive Memvid Chatbot Loop with Python

Source: https://docs.memvid.com/examples/chatbot-memory

Provides a main function to run the Memvid chatbot interactively. It initializes the chatbot, handles user input for commands like adding knowledge, viewing history, or quitting, and processes chat messages.

```python
def main():
    print("Memvid Chatbot initialized")
    print("Commands: /add (add knowledge), /history (view recent), /quit (exit)")
    print("-" * 50)

    bot = MemvidChatbot()

    while True:
        user_input = input("\nYou: ").strip()

        if not user_input:
            continue

        if user_input.lower() == "/quit":
            print("Goodbye!")
            break

        elif user_input.lower() == "/history":
            messages = bot.memory.get_recent_messages(10)
            print("\nRecent conversation:")
            for msg in messages:
                print(f"  [{msg['role']}]: {msg['content'][:100]}...")

        elif user_input.lower().startswith("/add "):
            # Format: /add Title | Content
            parts = user_input[5:].split("|", 1)
            if len(parts) == 2:
                bot.add_knowledge(parts[0].strip(), parts[1].strip())
            else:
                print("Usage: /add Title | Content")

        else:
            response = bot.chat(user_input)
            print(f"\nBot: {response}")

if __name__ == "__main__":
    main()
```

--------------------------------

### Handling Multi-Page Tables with Memvid

Source: https://docs.memvid.com/concepts/table-extraction

Explains how Memvid handles tables that span across multiple pages, demonstrating options to enable or disable cross-page merging for table import.

```bash
# Enable cross-page merging (default)
memvid tables import memory.mv2 --input report.pdf --merge-pages

# Disable merging (treat as separate tables)
memvid tables import memory.mv2 --input report.pdf --no-merge-pages
```

--------------------------------

### Configure and Use OpenAI Embeddings with Memvid CLI

Source: https://docs.memvid.com/concepts/embedding-models

Sets up the OpenAI API key and demonstrates using OpenAI's embedding models (e.g., text-embedding-3-small or text-embedding-3-large) via the Memvid CLI for semantic search.

```bash
export OPENAI_API_KEY=sk-your-key-here

# Use OpenAI for embeddings
memvid put knowledge.mv2 --input document.pdf --embedding -m openai-small

# Specify exact model
memvid put knowledge.mv2 --input docs/ --embedding -m openai-large
```

--------------------------------

### CLI Usage - Adaptive Strategies

Source: https://docs.memvid.com/concepts/adaptive-retrieval

An overview of the different adaptive retrieval strategies available in the memvid CLI, including their best use cases and behavior.

```APIDOC
## Adaptive Strategies

Five strategies are available for adaptive retrieval, each suited for different search scenarios:

### Combined (Default)

*   **Description**: Balances multiple signals for optimal performance.
*   **Behavior**: Uses relative and absolute thresholds, with cliff detection as a secondary signal.
*   **Best For**: General-purpose search.
*   **Usage**: `memvid find memory.mv2 --query "term" --adaptive-strategy combined`

### Relative

*   **Description**: Cuts off results at a percentage of the top score.
*   **Behavior**: Default threshold is 50% of the highest score. Good for consistent corpora.
*   **Best For**: Consistent corpora.
*   **Usage**: `memvid find memory.mv2 --query "term" --adaptive-strategy relative`

### Absolute

*   **Description**: Uses a fixed score cutoff.
*   **Behavior**: Cuts off results at a specified minimum score. Predictable behavior.
*   **Best For**: When you know your quality threshold.
*   **Usage**: `memvid find memory.mv2 --query "term" --adaptive-strategy absolute --min-relevancy 0.5`

### Cliff

*   **Description**: Detects sharp drops in score distribution between consecutive results.
*   **Behavior**: Looks for score drops greater than 30%. Best for distinct topic clusters.
*   **Best For**: Clearly separated relevant documents.
*   **Usage**: `memvid find memory.mv2 --query "term" --adaptive-strategy cliff`

### Elbow

*   **Description**: Finds the inflection point in the score curve to identify natural groupings.
*   **Behavior**: Uses curve analysis for gradual score distributions.
*   **Best For**: Gradual score distributions and mathematically principled grouping.
*   **Usage**: `memvid find memory.mv2 --query "term" --adaptive-strategy elbow`

### Strategy Comparison

| Strategy   | Best For              | Behavior             |
| ---------- | --------------------- | -------------------- |
| `combined` | General use           | Balanced, adaptive   |
| `relative` | Consistent corpora    | % of top score       |
| `absolute` | Known thresholds      | Fixed cutoff         |
| `cliff`    | Clustered topics      | Sharp drop detection |
| `elbow`    | Gradual distributions | Curve inflection     |
```

--------------------------------

### Memvid Python SDK Usage

Source: https://docs.memvid.com/index

Shows how to utilize the Memvid Python SDK for creating a knowledge base, enabling lexical search, adding data, finding information, and retrieving entity states.

```python
from memvid_sdk import create

mem = create('knowledge.mv2')
mem.enable_lex()
mem.put(title='Team', label='info', metadata={}, text='Alice works at Anthropic...')
results = mem.find('who works at AI companies', k=5)
alice = mem.state('Alice')
# {'slots': {'employer': 'Anthropic', 'role': 'Senior Engineer'}}
```

--------------------------------

### Built-in RAG with Citations in Memvid

Source: https://docs.memvid.com/introduction/why-memvid

Offers integrated Retrieval-Augmented Generation (RAG) functionality, enabling users to ask questions directly and receive sourced answers. It automatically handles the retrieval process and provides citations for the generated answers, simplifying RAG implementation.

```python
answer = mem.ask("What was our Q4 revenue?")

print(answer["answer"])              # "Q4 revenue was $2.4M, up 15% YoY..."
print(answer.get("sources", [])[:1]) # [{"title": "...", "uri": "...", ...}]
```

--------------------------------

### Near-Duplicate Detection using SimHash (Bash)

Source: https://docs.memvid.com/concepts/deduplication

Demonstrates near-duplicate detection with SimHash using the memvid CLI. An original document is added, followed by a slightly modified version (punctuation and whitespace changes). Memvid identifies the second input as a near-duplicate and skips creating a new frame. A significantly different document is then added successfully.

```bash
# Original document
echo "The quick brown fox jumps over the lazy dog." | memvid put memory.mv2 --input -
# Output: Created frame_001

# Minor variation (punctuation + whitespace)
echo "The quick brown fox jumps over the lazy dog" | memvid put memory.mv2 --input -
# Output: Near-duplicate of frame_001 detected, skipping

# Different document (passes threshold)
echo "A slow red cat sleeps under the busy cat." | memvid put memory.mv2 --input -
# Output: Created frame_002
```

--------------------------------

### Complete Conversation Loop with Memvid and OpenAI

Source: https://docs.memvid.com/frameworks/openai

This snippet demonstrates a full conversation loop integrating Memvid SDK with OpenAI's chat completions. It handles system messages, user queries, tool calls (using `memvid_find`, `memvid_put`, `memvid_ask`), and processes tool outputs to continue the conversation. The loop breaks when no more tool calls are needed, printing the final response. Dependencies include the Memvid SDK and the OpenAI library. It takes user input and returns conversational responses, potentially augmented by data from the Memvid knowledge base.

```typescript
import { use } from '@memvid/sdk';
import OpenAI from 'openai';

const mem = await use('openai', 'knowledge.mv2');
const functions = mem.functions;

const client = new OpenAI();
const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
  { role: 'system', content: 'You have access to a knowledge base. Use the tools to help users.' },
  { role: 'user', content: 'What authentication methods are supported?' },
];

// Function to execute tool calls
async function executeFunction(name: string, args: any): Promise<any> {
  if (name === 'memvid_find') {
    return mem.find(args.query, { k: args.top_k || 5 });
  } else if (name === 'memvid_put') {
    return mem.put({ title: args.title, label: args.label, text: args.text });
  } else if (name === 'memvid_ask') {
    return mem.ask(args.question, { mode: args.mode || 'auto' });
  }
  return null;
}

// Conversation loop
while (true) {
  const response = await client.chat.completions.create({
    model: 'gpt-4o',
    messages,
    tools: functions.map((f: any) => ({ type: 'function' as const, function: f })),
    tool_choice: 'auto',
  });

  const message = response.choices[0].message;
  messages.push(message);

  if (message.tool_calls) {
    for (const toolCall of message.tool_calls) {
      const funcName = toolCall.function.name;
      const funcArgs = JSON.parse(toolCall.function.arguments);
      const result = await executeFunction(funcName, funcArgs);

      messages.push({
        role: 'tool',
        tool_call_id: toolCall.id,
        content: JSON.stringify(result) || 'Function executed',
      });
    }
  } else {
    // No more tool calls, print the response
    console.log(message.content);
    break;
  }
}

```

```python
from memvid_sdk import use
import openai
import json

mem = use('openai', 'knowledge.mv2')
functions = mem.functions

client = openai.OpenAI()
messages = [
    {"role": "system", "content": "You have access to a knowledge base. Use the tools to help users."},
    {"role": "user", "content": "What authentication methods are supported?"}
]

# Function to execute tool calls
def execute_function(name, args):
    if name == "memvid_find":
        return mem.find(args["query"], k=args.get("top_k", 5))
    elif name == "memvid_put":
        return mem.put(title=args["title"], label=args["label"], text=args["text"])
    elif name == "memvid_ask":
        return mem.ask(args["question"], mode=args.get("mode", "auto"))
    return None

# Conversation loop
while True:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        tools=[{"type": "function", "function": f} for f in functions],
        tool_choice="auto"
    )

    message = response.choices[0].message
    messages.append(message)

    if message.tool_calls:
        for tool_call in message.tool_calls:
            func_name = tool_call.function.name
            func_args = json.loads(tool_call.function.arguments)
            result = execute_function(func_name, func_args)

            messages.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": json.dumps(result) if result else "Function executed"
            })
    else:
        # No more tool calls, print the response
        print(message.content)
        break

```

--------------------------------

### Context Manager

Source: https://docs.memvid.com/python-sdk/overview

Demonstrates using the Memvid SDK with Python's `with` statement for automatic resource management (e.g., closing memory files).

```APIDOC
## Context Manager

```python
import memvid_sdk as memvid

# Automatically closes when done
with memvid.use('basic', 'memory.mv2') as mem:
    mem.put('Doc', 'test', {}, text='Content')
    results = mem.find('query')
```
```

--------------------------------

### Initialize and Chat with LlamaIndex

Source: https://docs.memvid.com/frameworks/llamaindex

Initializes the Memvid knowledge base using LlamaIndex, builds a vector store index, and creates a chat engine with memory. It then demonstrates two chat interactions, maintaining context between them.

```python
mem = use('llamaindex', 'knowledge.mv2', read_only=True)
vector_store = mem.as_vector_store()

index = VectorStoreIndex.from_vector_store(vector_store)

chat_engine = index.as_chat_engine(
    chat_mode="context",
    llm=OpenAI(model="gpt-4o"),
    memory=ChatMemoryBuffer.from_defaults(token_limit=3000)
)

response = chat_engine.chat("What is Memvid?")
print(response)

response = chat_engine.chat("How does search work?")
print(response)
```

--------------------------------

### Tickets and Capacity Management (Node.js, Python)

Source: https://docs.memvid.com/quickstart/sdk-recipes

Manages Memvid capacity by applying tickets and retrieving current capacity information. The 'applyTicket' function takes a base64 encoded ticket string, and 'getCapacity' returns the current usage details. Consistent across both SDKs.

```typescript
await mem.applyTicket("base64-ticket");
console.log(await mem.getCapacity());
```

```python
mem.apply_ticket("base64-ticket")           # expands capacity
print(mem.get_capacity())
```

--------------------------------

### Show Memvid Plan and Usage (CLI)

Source: https://docs.memvid.com/concepts/query-usage

Displays the current Memvid plan, usage statistics for queries and storage, and the billing period details. Can output in human-readable format or JSON for scripting.

```bash
# Show current plan and usage
memvid plan show
```

```bash
memvid plan show --json
```

--------------------------------

### Node.js: Basic and Custom Memvid Search

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Demonstrates how to initialize Memvid in Node.js and perform searches with default adaptive settings, custom parameters, and adaptive search disabled.

```typescript
import { use } from '@anthropics/memvid'

const mem = await use('basic', 'memory.mv2')

// Adaptive (default)
const results = await mem.find("authentication patterns")
console.log(`Returned ${results.length} relevant results`)

// With custom settings
const results = await mem.find("security", {
  adaptive: true,
  minRelevancy: 0.5,
  maxK: 25,
  adaptiveStrategy: "cliff"
})

// Disable adaptive
const results = await mem.find("config", {
  adaptive: false,
  topK: 10
})
```

--------------------------------

### Local CLIP Embedding Operations (Python/Node.js)

Source: https://docs.memvid.com/concepts/visual-embeddings

Demonstrates how to use the local CLIP provider (MobileCLIP-S2) for embedding images and text. It shows initialization via a factory function or direct instantiation and covers single and batch embedding of images, as well as text embedding for search queries. Note that `LocalClip` in Node.js might require a native build.

```python
from memvid_sdk.clip import get_clip_provider, LocalClip

# Using factory
clip = get_clip_provider('local')

# Or direct instantiation
clip = LocalClip(model='mobileclip-s2')

# Embed an image
image_embedding = clip.embed_image('photo.jpg')

# Embed text for search
text_embedding = clip.embed_text('sunset over ocean')

# Batch embed multiple images
embeddings = clip.embed_images(['img1.jpg', 'img2.jpg', 'img3.jpg'])
```

```typescript
import { getClipProvider, LocalClip } from '@memvid/sdk';

// Using factory
const clip = getClipProvider('local');

// Or direct instantiation
const clip = new LocalClip({ model: 'mobileclip-s2' });

// Embed an image
const imageEmbedding = await clip.embedImage('photo.jpg');

// Embed text for search
const textEmbedding = await clip.embedText('sunset over ocean');

// Batch embed multiple images
const embeddings = await clip.embedImages(['img1.jpg', 'img2.jpg', 'img3.jpg'])
```

--------------------------------

### Grounding Quality Check with memvid CLI and jq

Source: https://docs.memvid.com/cli/search-and-ask

Demonstrates how to extract and inspect the 'grounding' object from the JSON output of the 'memvid ask' command. This is useful for assessing the reliability of the LLM's answer against the retrieved context.

```bash
# Check grounding quality
memvid ask knowledge.mv2 \
  --question "What is the API endpoint?" \
  --use-model openai \
  --json | jq '.grounding'
```

--------------------------------

### Memvid CLI: Tuning for High-Recall Search

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Shows how to configure Memvid for high-recall search by lowering the `min-relevancy` and increasing the `max-k` limit, using the `relative` adaptive strategy.

```bash
# Lenient threshold with high cap
memvid find memory.mv2 --query "term" \
  --min-relevancy 0.3 \
  --max-k 100 \
  --adaptive-strategy relative
```

--------------------------------

### Troubleshoot Capacity Exceeded Errors

Source: https://docs.memvid.com/cli/create-and-put

Offers solutions for 'CapacityExceeded' errors in Memvid. This involves checking current usage statistics (`stats`), deleting specific frames (`delete`), and compacting the project file to reclaim space (`doctor --vacuum`).

```bash
# Check current usage
memvid stats knowledge.mv2

# Delete unused frames
memvid delete knowledge.mv2 --frame-id 42 --yes

# Compact the file
memvid doctor knowledge.mv2 --vacuum
```

--------------------------------

### Parallel Ingestion

Source: https://docs.memvid.com/cli/create-and-put

Ingest large datasets efficiently using multi-threaded processing and fine-tune parallel settings.

```APIDOC
## Parallel Ingestion

### Description
Enables multi-threaded processing for ingesting large datasets. Allows fine-tuning of parallel settings for optimal performance.

### Method
PUT

### Endpoint
/websites/memvid

### Parameters
#### Path Parameters
- **knowledge.mv2** (string) - Required - The target memory file.

#### Query Parameters
- **--input** (string) - Required - Path to the input directory or file.
- **--vector-compression** (boolean) - Optional - Enables vector compression.
- **--parallel-segments** (boolean) - Optional - Enables multi-threaded processing.
- **--parallel-threads** (integer) - Optional - Specifies the number of worker threads. Default is CPU count - 1.
- **--parallel-seg-tokens** (integer) - Optional - Sets the target number of tokens per segment.
- **--parallel-queue-depth** (integer) - Optional - Sets the queue size for workers.

### Request Example (Enable parallel ingestion)
```bash
memvid put knowledge.mv2 --input ./large-dataset/ --vector-compression \
  --parallel-segments \
  --parallel-threads 8
```

### Request Example (Fine-tune parallel settings)
```bash
memvid put knowledge.mv2 --input ./corpus/ --vector-compression \
  --parallel-segments \
  --parallel-seg-tokens 4000 \
  --parallel-threads 4 \
  --parallel-queue-depth 16
```

### Options Table

| Option                   | Description                      | Default       |
| ------------------------ | -------------------------------- | ------------- |
| `--parallel-segments`    | Enable multi-threaded processing | false         |
| `--parallel-threads`     | Number of worker threads         | CPU count - 1 |
| `--parallel-queue-depth` | Queue size for workers           | Auto          |
| `--parallel-seg-tokens`  | Target tokens per segment        | Auto          |
```

--------------------------------

### Batch Ingest Records (Node.js, Python)

Source: https://docs.memvid.com/quickstart/sdk-recipes

Ingests multiple records efficiently in a batch. Both SDKs accept an array of record objects. The Node.js SDK allows specifying compression level via an options object, while the Python SDK uses a nested 'opts' dictionary.

```typescript
const docs = [
  { title: "Doc 1", label: "kb", text: "First document" },
  { title: "Doc 2", label: "kb", text: "Second document" },
];
await mem.putMany(docs, { compressionLevel: 3 });
```

```python
docs = [
    {"title": "Doc 1", "label": "kb", "text": "First document"},
    {"title": "Doc 2", "label": "kb", "text": "Second document"},
]
mem.put_many(docs, opts={"compression_level": 3, "enable_embedding": True})
```

--------------------------------

### Memory Management with Memvid SDK (Python)

Source: https://docs.memvid.com/frameworks/autogen

Demonstrates the best practice of using read-only mode for retrieval agents and properly sealing the memory resource when done. It shows initializing the memory context and ensuring it's closed using a try-finally block, even if errors occur during agent execution.

```python
from memvid_sdk import use

mem = use('autogen', 'knowledge.mv2', read_only=True)
try:
    # Create agents and run conversation
    pass
finally:
    mem.seal()
```

--------------------------------

### Interactive CLI for Document Q&A System (Python)

Source: https://docs.memvid.com/examples/document-qa

This Python script implements an interactive command-line interface for a Document Q&A system. It utilizes argparse for command parsing, allowing users to ingest documents, ask questions, search content, and view statistics. The system relies on the DocumentQA class for core functionality and Path for file system operations.

```python
def main():
    import argparse

    parser = argparse.ArgumentParser(description="Document Q&A System")
    parser.add_argument("--memory", default="documents.mv2", help="Memory file path")

    subparsers = parser.add_subparsers(dest="command")

    # Ingest command
    ingest_parser = subparsers.add_parser("ingest", help="Ingest documents")
    ingest_parser.add_argument("path", help="File or folder path")
    ingest_parser.add_argument("--recursive", "-r", action="store_true")

    # Ask command
    ask_parser = subparsers.add_parser("ask", help="Ask a question")
    ask_parser.add_argument("question", help="Question to ask")

    # Search command
    search_parser = subparsers.add_parser("search", help="Search documents")
    search_parser.add_argument("query", help="Search query")

    # Stats command
    subparsers.add_parser("stats", help="Show statistics")

    args = parser.parse_args()
    qa = DocumentQA(args.memory)

    if args.command == "ingest":
        path = Path(args.path)
        if path.is_file():
            qa.ingest_file(str(path))
        else:
            qa.ingest_folder(str(path), recursive=args.recursive)
        print(f"\n Ingested: {qa.stats['ingested']}, Failed: {qa.stats['failed']}")

    elif args.command == "ask":
        result = qa.ask(args.question)
        print(f"\n Answer: {result['answer']}")
        print(f"\n Sources:")
        for s in result['sources'][:3]:
            print(f"  - {s['title']} (score: {s['score']:.2f})")

    elif args.command == "search":
        results = qa.search(args.query)
        print(f"\n Found {len(results)} results:")
        for r in results[:5]:
            print(f"\n   {r['title']} (score: {r['score']:.2f})")
            print(f"     {r['snippet'][:150]}...")

    elif args.command == "stats":
        stats = qa.get_stats()
        print(f"\n Document Store Statistics:")
        print(f"  Documents: {stats['total_documents']}")
        print(f"  Size: {stats['size_mb']} MB")

    else:
        parser.print_help()


if __name__ == "__main__":
    main()

```

--------------------------------

### Traditional VectorDB Data Insertion (Python)

Source: https://docs.memvid.com/introduction/frames

This Python code snippet demonstrates how data is typically inserted into a traditional vector database. It highlights the insertion of an ID, a vector, and some metadata. The snippet also points out the missing information like original document content, insertion time, and historical context.

```python
# Traditional VectorDB
vectordb.insert(
    id="doc-123",
    vector=[0.1, 0.2, ...],  # 1536 floats
    metadata={"title": "Meeting Notes"}
)
# Where's the original document?
# When was it added?
# What was the knowledge state before this?
# 
```

--------------------------------

### Meeting Minutes Analysis with Memvid

Source: https://docs.memvid.com/concepts/memory-cards

Shows how to use Memvid to process meeting minutes, extract action items and decisions, and query specific assignments.

```bash
# Transcribe and ingest
memvid put meetings.mv2 --input recording.mp3

# Enrich with Claude for best accuracy
memvid enrich meetings.mv2 --engine claude

# Find all action items assigned to John
memvid facts meetings.mv2 --entity "John" --predicate assigned_action
```

--------------------------------

### AI-Powered Q&A with Local Ollama

Source: https://docs.memvid.com/cli/search-and-ask

Ask questions against a knowledge base using a local Ollama model for AI-powered Q&A. This method is recommended for privacy and cost-effectiveness. It requires specifying the database file, the question, and the desired local model.

```bash
# Ask a question with local Ollama model (recommended)
memvid ask knowledge.mv2 --question "Why is determinism important?" --use-model "ollama:qwen2.5:1.5b"
```

--------------------------------

### Traditional VectorDB Architecture

Source: https://docs.memvid.com/introduction/frames

This diagram outlines the typical architecture of a traditional vector database. It involves a client application, the vector database components (Text Chunker, Embedding API, Vector Store, Metadata Store, Document Store), and external dependencies like OpenAI/Cohere API and PostgreSQL/S3.

```mermaid
graph TB
    subgraph Client["Your Application"]
        Doc[Document]
    end

    subgraph VectorDB["Traditional VectorDB"]
        Chunker[Text Chunker]
        Embedder[Embedding API]
        VecStore[Vector Store]
        MetaStore[Metadata Store]
        DocStore[Document Store]
    end

    subgraph External["External Dependencies"]
        API[OpenAI/Cohere API]
        PG[(PostgreSQL)]
        S3[(S3/Blob Storage)]
    end

    Doc --> Chunker
    Chunker --> Embedder
    Embedder --> API
    API --> VecStore
    VecStore --> MetaStore
    MetaStore --> PG
    Doc --> DocStore
    DocStore --> S3
```

--------------------------------

### Create Memory File

Source: https://docs.memvid.com/cli/create-and-put

Create new memory files (.mv2) with customizable options for capacity, indexing, and size.

```APIDOC
## CREATE /websites/memvid

### Description
Creates a new memory file with specified options.

### Method
`memvid create`

### Parameters
#### Path Parameters
* **<FILE>** (string) - Required - The name of the memory file to create (e.g., `my-knowledge.mv2`).

#### Query Parameters
* **--tier** (string) - Optional - Capacity tier (`free`, `dev`, `enterprise`). Defaults to `free`.
* **--size** (string) - Optional - Capacity override (e.g. `15MB`, capped at `50MB`). Defaults to `50MB`.
* **--no-lex** (boolean) - Optional - Disable lexical/full-text index. Defaults to enabled.
* **--no-vector** (boolean) - Optional - Disable vector index. Defaults to enabled.
* **--json** (boolean) - Optional - Output as JSON.

### Request Example
```bash
memvid create my-knowledge.mv2 --tier dev --size 15MB --no-lex
```

### Response
#### Success Response (JSON Output)
- **path** (string) - Path to the created memory file.
- **size_limit_bytes** (integer) - The size limit of the memory file in bytes.
- **lex_enabled** (boolean) - Indicates if the lexical index is enabled.
- **vec_enabled** (boolean) - Indicates if the vector index is enabled.
- **created_at** (string) - Timestamp when the file was created.

#### Response Example
```json
{
  "path": "my-memory.mv2",
  "size_limit_bytes": 536870912,
  "lex_enabled": true,
  "vec_enabled": true,
  "created_at": "2024-01-15T10:30:00Z"
}
```
```

--------------------------------

### Ingest Documents

Source: https://docs.memvid.com/cli/create-and-put

Add documents to a memory file as frames, with options for embeddings, compression, and metadata.

```APIDOC
## INGEST /websites/memvid

### Description
Ingests documents into a specified memory file, creating frames.

### Method
`memvid put`

### Endpoint
`memvid put <FILE> [OPTIONS]`

### Parameters
#### Path Parameters
* **<FILE>** (string) - Required - The path to the memory file to ingest into.

#### Query Parameters
**Core Options:**
* **--input PATH** (string) - Required - Path to the file or directory containing documents to ingest.
* **--uri URI** (string) - Optional - Custom URI for the frame.
* **--title TITLE** (string) - Optional - Document title.
* **--timestamp UNIX_TS** (integer) - Optional - POSIX timestamp for the frame.
* **--track TRACK** (string) - Optional - Track/collection name for the frame.
* **--kind KIND** (string) - Optional - Content type metadata for the frame.
* **--json** (boolean) - Optional - Output ingestion results as JSON.
* **--embedding** (boolean) - Optional - Generate semantic embeddings for the document.
* **--vector-compression** (boolean) - Optional - Apply PQ compression to generated vectors.

**Metadata Options:**
* **--tag KEY=VALUE** (string) - Optional - Add tags to the frame (repeatable).
* **--label LABEL** (string) - Optional - Add labels to the frame (repeatable).
* **--metadata JSON** (string) - Optional - Additional metadata provided as a JSON string.
* **--no-auto-tag** (boolean) - Optional - Disable automatic tag extraction.
* **--no-extract-dates** (boolean) - Optional - Disable automatic date extraction from content.

### Request Example
```bash
# Ingest a directory with semantic embeddings and tags
memvid put my-knowledge.mv2 --input ./data/ --embedding --tag source=web --tag category=news

# Ingest from stdin with a custom title and track
echo "Meeting notes content" | memvid put my-knowledge.mv2 --title "Team Sync" --track "meetings"
```

### Response
*The response format depends on the `--json` flag. Without it, progress and success messages are printed to the console. With `--json`, detailed results for each ingested document are provided.*
```

--------------------------------

### Memvid Context Manager for Memory Creation

Source: https://docs.memvid.com/python-sdk/overview

Shows how to use the `create` function within a context manager for automatic memory file closure. This is a convenient way to manage memory resources in the Memvid Python SDK.

```python
from memvid_sdk import create

# Automatically closes when done
with create('project.mv2') as mem:
    mem.put('Title', 'label', {}, text='Content')
    results = mem.find('query')
```

--------------------------------

### Optimize Top-K for Memvid CLI

Source: https://docs.memvid.com/concepts/query-usage

Demonstrates how to efficiently use the `--top-k` argument in the Memvid CLI to request only the necessary number of results, avoiding unnecessary data fetching.

```bash
# Fetching 100 results when you only need 5
memvid find memory.mv2 --query "term" --top-k 100  # Wasteful

# Better: Request what you need
memvid find memory.mv2 --query "term" --top-k 5
```

--------------------------------

### Pull Various Ollama Models

Source: https://docs.memvid.com/concepts/local-models

Commands to download different AI models supported by Ollama. These range from small and fast models to larger, more capable ones like Qwen2.5, Phi3, Gemma2, and Llama3.2.

```bash
# Small & fast
ollama pull qwen2.5:0.5b

# Recommended (best balance)
ollama pull qwen2.5:1.5b

# Higher quality
ollama pull qwen2.5:3b
ollama pull phi3:mini
ollama pull gemma2:2b

# Meta's Llama
ollama pull llama3.2:1b
ollama pull llama3.2:3b
```

--------------------------------

### Memvid SDK Framework Adapters

Source: https://docs.memvid.com/sdks/node

Demonstrates how to integrate the Memvid SDK with popular frameworks like Vercel AI SDK, LangChain.js, LlamaIndex, OpenAI Function Calling, Google ADK, and Semantic Kernel. This enables seamless use of Memvid memory within these ecosystems.

```typescript
import { use } from '@memvid/sdk';

// Vercel AI SDK
const vercel = await use('vercel-ai', 'knowledge.mv2');
const tools = vercel.tools;

// LangChain.js
const langchain = await use('langchain', 'knowledge.mv2');
const retriever = langchain.asRetriever();

// LlamaIndex
const llamaindex = await use('llamaindex', 'knowledge.mv2');

// OpenAI Function Calling
const openai = await use('openai', 'knowledge.mv2');
const functions = openai.functions;

// Google ADK
const googleAdk = await use('google-adk', 'knowledge.mv2');

// Semantic Kernel
const sk = await use('semantic-kernel', 'knowledge.mv2');
```

--------------------------------

### Memvid CLI Search and Retrieval Workflows

Source: https://docs.memvid.com/cli/index

Illustrates common workflows for searching and retrieving information from Memvid memory files. Includes simple keyword search, semantic search using embeddings, asking questions via LLM, and time-filtered searches.

```bash
# Simple search
memvid find project.mv2 --query "budget projections"

# Semantic search with embeddings
memvid find project.mv2 --query "financial outlook" --mode sem

# Ask questions with LLM
memvid ask project.mv2 --question "What was decided about the budget?" --use-model openai

# Time-filtered search
memvid find project.mv2 --query "status" --as-of-ts 1704067200
```

--------------------------------

### Enrichment with OpenAI Cloud Engine (CLI)

Source: https://docs.memvid.com/concepts/memory-cards

This command utilizes the OpenAI API for high-quality LLM-based enrichment. It requires an `OPENAI_API_KEY` environment variable to be set and offers a medium speed and quality tradeoff.

```bash
export OPENAI_API_KEY=sk-xxx
memvid enrich memory.mv2 --engine openai
```

--------------------------------

### Troubleshoot Model Not Found

Source: https://docs.memvid.com/concepts/local-models

Shows the command to pull a model that is reported as not found. This is a common error when trying to use a model that hasn't been downloaded.

```bash
# Pull the model first
ollama pull qwen2.5:1.5b
```

--------------------------------

### Basic Document Ingestion

Source: https://docs.memvid.com/cli

Demonstrates common workflows for ingesting documents into Memvid, including creating memory files, adding text files, PDFs, and documents with metadata.

```APIDOC
## Basic Document Ingestion

```bash
# Create memory and add documents
memvid create project.mv2

# Add a text file
cat notes.txt | memvid put project.mv2 --title "Notes"

# Add a PDF
memvid put project.mv2 --input report.pdf --title "Report"

# Add with metadata
memvid put project.mv2 --input doc.txt \
  --title "Meeting Notes" \
  --track meetings \
  --tag "date=2024-01-15" \
  --tag "attendees=alice,bob" \
  --label important
```
```

--------------------------------

### Ask Questions with Memvid (RAG) (Bash)

Source: https://docs.memvid.com/cli/cheat-sheet

Utilize Memvid's RAG capabilities to ask questions against memory content. Supports choosing synthesis models, retrieving context only, including sources, masking PII, and including memory cards.

```bash
# Ask a question
memvid ask memory.mv2 --question "What is the main topic?"

# Choose synthesis model
memvid ask memory.mv2 --question "..." --use-model tinyllama   # Local (default)
memvid ask memory.mv2 --question "..." --use-model openai      # GPT-4
memvid ask memory.mv2 --question "..." --use-model claude      # Claude
memvid ask memory.mv2 --question "..." --use-model groq        # Llama via Groq
memvid ask memory.mv2 --question "..." --use-model gemini      # Gemini

# Get context only (no synthesis)
memvid ask memory.mv2 --question "..." --context-only

# Include sources
memvid ask memory.mv2 --question "..." --sources

# Mask PII in response
memvid ask memory.mv2 --question "..." --mask-pii

# Include memory cards
memvid ask memory.mv2 --question "..." --memories
```

--------------------------------

### Default Deduplication Best Practice (Bash)

Source: https://docs.memvid.com/concepts/deduplication

This command shows the most common use case where deduplication is enabled by default. Simply adding content via `memvid put` will automatically handle deduplication, saving storage space by not storing identical content multiple times.

```bash
# Just add content - dedup handles the rest
memvid put memory.mv2 --input ./documents/
```

--------------------------------

### Memvid Ask Command

Source: https://docs.memvid.com/cli/search-and-ask

The `ask` command is used to query memory files. It supports various options for filtering, model selection, and output formatting.

```APIDOC
## memvid ask

### Description
Queries one or more memory files using a specified question and optional filters.

### Method
CLI Command

### Endpoint
`memvid ask <FILE> [<FILE>...] --question <QUESTION> [OPTIONS]`

### Parameters
#### Path Parameters
- **FILE** (string) - Required - Path to the memory file(s) to query.
- **QUESTION** (string) - Required - The question to ask the model.

#### Query Parameters
- **--scope** (string) - Filter by URI prefix.
- **--uri** (string) - Filter to a specific URI.
- **--start** (string) - Start date filter (e.g., "YYYY-MM-DD").
- **--end** (string) - End date filter (e.g., "YYYY-MM-DD").
- **--as-of-frame** (integer) - Time-travel to a specific frame ID.
- **--as-of-ts** (integer) - Time-travel to a specific timestamp.
- **--use-model** (string) - Specifies the LLM to use (e.g., "ollama:qwen2.5:1.5b", "openai", "gemini-2.0-flash").
- **--top-k** (integer) - Number of top results to consider for synthesis.
- **--context-only** (boolean) - If set, returns only the context without LLM synthesis.
- **--mask-pii** (boolean) - If set, masks personally identifiable information before sending to cloud LLMs.
- **--json** (boolean) - If set, outputs the response in JSON format.

### Request Example
```bash
memvid ask knowledge.mv2 --question "How do I configure authentication?"
memvid ask knowledge.mv2 --question "What happened in Q4?" --start "2024-10-01" --end "2024-12-31"
memvid ask docs.mv2 code.mv2 --question "How does authentication work?"
memvid ask ./memories/*.mv2 --question "What are the main features?"
```

### Response
#### Success Response
The response typically includes the answer, context, and potentially grounding information. When `--json` is used, the structure is detailed.

#### Response Example (JSON)
```json
{
  "question": "What is the architecture?",
  "answer": "The architecture follows a layered design with...",
  "mode": "hybrid",
  "context_only": false,
  "hits": [
    {
      "rank": 1,
      "frame_id": 124,
      "uri": "mv2://docs/arch.md",
      "title": "Architecture Overview",
      "score": 0.92,
      "text": "The system consists of..."
    }
  ],
  "grounding": {
    "score": 0.85,
    "label": "HIGH",
    "sentence_count": 3,
    "grounded_sentences": 3,
    "has_warning": false
  },
  "follow_up": {
    "needed": false
  },
  "stats": {
    "retrieval_ms": 5,
    "synthesis_ms": 1200,
    "latency_ms": 1205
  }
}
```

### Grounding & Hallucination Detection
When using `--json`, the response includes a `grounding` object:

- **score** (float) - Grounding score from 0.0 to 1.0.
- **label** (string) - Quality label: `LOW`, `MEDIUM`, or `HIGH`.
- **sentence_count** (integer) - Number of sentences in the answer.
- **grounded_sentences** (integer) - Sentences supported by context.
- **has_warning** (boolean) - True if the answer may be hallucinated.
- **warning_reason** (string) - Explanation if a warning is present.

The `follow_up` object indicates if further interaction is needed due to potential unreliability:

- **needed** (boolean) - True if follow-up is recommended.
- **reason** (string) - Explanation for the need for follow-up.
- **hint** (string) - A hint for rephrasing the query or selecting topics.
- **available_topics** (array of strings) - Topics available in the memory.
- **suggestions** (array of strings) - Suggested follow-up questions.

<Tip>When `follow_up.needed` is `true`, the answer may not be reliable. Consider using the suggested follow-up questions or rephrasing your query.</Tip>
```

--------------------------------

### Framework Adapters API

Source: https://docs.memvid.com/node-sdk/overview

Integrate Memvid with various popular frameworks and libraries using specific adapters. This allows seamless use of Memvid within your existing ecosystem.

```APIDOC
## Framework Adapters API

### Description
Provides adapters to integrate Memvid with different frameworks and libraries.

### Method
`use(adapter: string, filePath: string, options?: UseOptions)`

### Endpoint
N/A (This is a function call within the SDK)

### Parameters
- **adapter** (string) - Required - The name of the adapter to use (e.g., 'langchain', 'vercel-ai').
- **filePath** (string) - Required - The path to the Memvid memory file.
- **options** (object) - Optional - Adapter-specific options.

### Request Example
```typescript
import { use } from '@memvid/sdk';

// Use the basic file adapter
const memBasic = await use('basic', 'file.mv2');

// Use the LangChain adapter
const memLangchain = await use('langchain', 'file.mv2');

// Use the Vercel AI adapter
const memVercelAI = await use('vercel-ai', 'file.mv2');

// Use other adapters...
const memLlamaIndex = await use('llamaindex', 'file.mv2');
const memOpenAI = await use('openai', 'file.mv2');
const memCrewAI = await use('crewai', 'file.mv2');
const memAutoGen = await use('autogen', 'file.mv2');
const memHaystack = await use('haystack', 'file.mv2');
const memLangGraph = await use('langgraph', 'file.mv2');
const memSemanticKernel = await use('semantic-kernel', 'file.mv2');
const memGoogleADK = await use('google-adk', 'file.mv2');
```

### Response
Returns an instance of the memory object configured with the specified adapter.
```

--------------------------------

### Document Ingestion with Metadata

Source: https://docs.memvid.com/cli/create-and-put

Ingest documents into a Memvid memory, adding metadata tags, labels, and custom timestamps.

```APIDOC
## Add metadata tags

### Description
Ingests a document and adds category, version, and author tags.

### Method
PUT

### Endpoint
/websites/memvid

### Parameters
#### Path Parameters
- **knowledge.mv2** (string) - Required - The target memory file.

#### Query Parameters
- **--input** (string) - Required - Path to the input file (e.g., api-docs.md).
- **--vector-compression** (boolean) - Optional - Enables vector compression.
- **--tag** (string) - Optional - Adds a tag in key=value format (e.g., "category=documentation"). Can be specified multiple times.
- **--label** (string) - Optional - Adds a label to the document. Can be specified multiple times.
- **--timestamp** (integer) - Optional - Sets a custom Unix timestamp for the document.

### Request Example
```bash
memvid put knowledge.mv2 --input api-docs.md --vector-compression \
  --tag "category=documentation" \
  --tag "version=2.0" \
  --tag "author=team"
```

## Add labels

### Description
Ingests a PDF document and adds "quarterly" and "finance" labels.

### Method
PUT

### Endpoint
/websites/memvid

### Parameters
#### Path Parameters
- **knowledge.mv2** (string) - Required - The target memory file.

#### Query Parameters
- **--input** (string) - Required - Path to the input file (e.g., report.pdf).
- **--vector-compression** (boolean) - Optional - Enables vector compression.
- **--label** (string) - Required - Adds a label to the document (e.g., "quarterly"). Can be specified multiple times.

### Request Example
```bash
memvid put knowledge.mv2 --input report.pdf --vector-compression \
  --label "quarterly" \
  --label "finance"
```

## Set custom timestamp

### Description
Ingests a PDF document and sets a custom Unix timestamp.

### Method
PUT

### Endpoint
/websites/memvid

### Parameters
#### Path Parameters
- **knowledge.mv2** (string) - Required - The target memory file.

#### Query Parameters
- **--input** (string) - Required - Path to the input file (e.g., old-report.pdf).
- **--vector-compression** (boolean) - Optional - Enables vector compression.
- **--timestamp** (integer) - Required - Sets a custom Unix timestamp (e.g., 1686819000).

### Request Example
```bash
memvid put knowledge.mv2 --input old-report.pdf --vector-compression \
  --timestamp 1686819000
```

## Combine options

### Description
Ingests a PDF document with multiple options including track, title, tags, and labels.

### Method
PUT

### Endpoint
/websites/memvid

### Parameters
#### Path Parameters
- **knowledge.mv2** (string) - Required - The target memory file.

#### Query Parameters
- **--input** (string) - Required - Path to the input file (e.g., quarterly-report.pdf).
- **--vector-compression** (boolean) - Optional - Enables vector compression.
- **--track** (string) - Optional - Assigns the document to a specific track (e.g., "reports").
- **--title** (string) - Optional - Sets a custom title for the document (e.g., "Q3 2024 Report").
- **--tag** (string) - Optional - Adds a tag in key=value format (e.g., "quarter=Q3"). Can be specified multiple times.
- **--label** (string) - Optional - Adds a label to the document. Can be specified multiple times.

### Request Example
```bash
memvid put knowledge.mv2 --input quarterly-report.pdf --vector-compression \
  --track "reports" \
  --title "Q3 2024 Report" \
  --tag "quarter=Q3" \
  --tag "year=2024"
```
```

--------------------------------

### Python: Enable PII Masking for User-Facing Output

Source: https://docs.memvid.com/concepts/pii-masking

Demonstrates the best practice of enabling PII masking when displaying information to users. This Python code snippet shows how to call the `mem.ask` function with `mask_pii=True` and then display the sanitized response.

```python
# Always mask when displaying to users
response = mem.ask(question, mask_pii=True)
display_to_user(response.answer)
```

--------------------------------

### Troubleshoot: Poor Quality Extraction (Re-extract)

Source: https://docs.memvid.com/concepts/table-extraction

Re-extract tables with adjusted settings if quality is poor. Use 'conservative' mode and set a 'high' minimum quality threshold to prioritize accuracy.

```bash
# Re-extract with different settings
memvid tables import memory.mv2 --input report.pdf --mode conservative --min-quality high
```

--------------------------------

### Ask with PII Masking (Python SDK)

Source: https://docs.memvid.com/concepts/pii-masking

This Python code snippet demonstrates how to ask a question using the Memvid SDK and enable PII masking in the response. It utilizes the `use` function to initialize the memory and the `ask` method with `mask_pii=True`.

```python
from memvid import use

mem = use('basic', 'memory.mv2')

# Ask with PII masking
response = mem.ask(
    "What is the customer's contact information?",
    mask_pii=True
)
print(response.answer)
# "Customer can be reached at [EMAIL] or [PHONE]"
```

--------------------------------

### Environment Variables

Source: https://docs.memvid.com/cli/advanced-commands

Configuration settings for the Memvid environment.

```APIDOC
## Environment Variables

### MEMVID_MODELS_DIR

**Description:** Specifies the directory for storing Memvid models.

**Default:** `~/.memvid/models`

### MEMVID_OFFLINE

**Description:** When set to `1`, skips model downloads and uses only cached models.

**Value:** `1` (to enable offline mode)

### OPENAI_API_KEY

**Description:** Required API key for using OpenAI enrichment and models.

### ANTHROPIC_API_KEY

**Description:** Required API key for using Claude models.

### GEMINI_API_KEY

**Description:** Required API key for using Gemini models. For legacy systems, `GOOGLE_API_KEY` may also work.

### MISTRAL_API_KEY

**Description:** Required API key for using Mistral models.

### GROQ_API_KEY

**Description:** Required API key for using Groq inference.
```

--------------------------------

### Complete Memvid Chatbot Implementation in Python

Source: https://docs.memvid.com/examples/chatbot-memory

Provides the full implementation of the Memvid Chatbot, including conversation memory management, LLM interaction, knowledge addition, and a command-line interface for user interaction. It leverages Langchain for LLM capabilities and a persistent memory backend.

```python
#!/usr/bin/env python3
"""
Memvid Chatbot with Persistent Memory

A production-ready chatbot that remembers conversations
and learns from documents.
"""

from datetime import datetime
from typing import List, Dict, Optional
from memvid_sdk import use
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage


class ConversationMemory:
    """Manages conversation history and knowledge retrieval."""

    def __init__(self, memory_path: str):
        self.mem = use('langchain', memory_path, mode='auto')
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")

    def add_message(self, role: str, content: str, tags: Optional[List[str]] = None):
        self.mem.put(
            f"Conversation - {self.session_id}",
            "conversation",
            {
                "session_id": self.session_id,
                "role": role,
                "timestamp": datetime.now().isoformat()
            },
            text=f"[{role.upper()}]: {content}",
            tags=tags or []
        )

    def get_relevant_context(self, query: str, k: int = 5) -> str:
        results = self.mem.find(query, k=k)
        context_parts = []

        for hit in results.hits:
            prefix = "[Knowledge]" if hit.label == "knowledge" else "[History]"
            context_parts.append(f"{prefix} {hit.text}")

        return "\n\n".join(context_parts)

    def get_recent_messages(self, limit: int = 10) -> List[Dict]:
        timeline = self.mem.timeline(limit=limit)
        return [
            {
                "role": e.metadata.get("role", "unknown"),
                "content": e.text,
                "timestamp": e.metadata.get("timestamp")
            }
            for e in timeline.entries
            if e.label == "conversation"
        ]


class MemvidChatbot:
    """AI chatbot with persistent memory."""

    def __init__(self, memory_path: str = "chatbot-memory.mv2"):
        self.memory = ConversationMemory(memory_path)
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.7)
        self.system_prompt = """You are a helpful AI assistant with memory.
Use the provided context to give accurate answers.
Reference the knowledge base when relevant.
Remember details from previous conversations."""

    def chat(self, user_message: str) -> str:
        self.memory.add_message("user", user_message)
        context = self.memory.get_relevant_context(user_message)

        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=f"Context:\n{context}\n\nUser: {user_message}")
        ]

        response = self.llm.invoke(messages)
        self.memory.add_message("assistant", response.content)

        return response.content

    def add_knowledge(self, title: str, content: str):
        self.memory.mem.put(title, "knowledge", {}, text=content)


def main():
    print("Memvid Chatbot")
    print("Commands: /add Title | Content, /history, /quit")
    print("-" * 50)

    bot = MemvidChatbot()

    while True:
        user_input = input("\nYou: ").strip()

        if not user_input:
            continue
        if user_input == "/quit":
            break
        if user_input == "/history":
            for m in bot.memory.get_recent_messages(10):
                print(f"  [{m['role']}]: {m['content'][:80]}...")
            continue
        if user_input.startswith("/add "):
            parts = user_input[5:].split("|", 1)
            if len(parts) == 2:
                bot.add_knowledge(parts[0].strip(), parts[1].strip())
            continue

        print(f"\n{bot.chat(user_input)}")


if __name__ == "__main__":
    main()
```

--------------------------------

### Set OpenAI API Key

Source: https://docs.memvid.com/examples/chatbot-memory

Sets the OpenAI API key as an environment variable. This is a prerequisite for the chatbot to authenticate with OpenAI services for natural language processing tasks.

```bash
export OPENAI_API_KEY=your-api-key
```

--------------------------------

### Research Paper Knowledge Graph with Memvid

Source: https://docs.memvid.com/concepts/memory-cards

Illustrates building a knowledge graph from research papers using Memvid, including ingestion, enrichment, and exporting to standard graph formats.

```bash
# Ingest papers
memvid put research.mv2 --input papers/

# Extract entities and relationships
memvid enrich research.mv2 --engine groq

# Export to knowledge graph format
memvid export research.mv2 --format ntriples --out research.nt
```

--------------------------------

### FastAPI Service for Memvid Chatbot

Source: https://docs.memvid.com/examples/chatbot-memory

Sets up a FastAPI application to serve the Memvid chatbot. It defines Pydantic models for chat requests and responses, and exposes API endpoints for '/chat' and '/knowledge'. Dependencies include FastAPI and Pydantic.

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()
bot = MemvidChatbot()

class ChatRequest(BaseModel):
    message: str
    user_id: str = "default"

class ChatResponse(BaseModel):
    response: str

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    response = bot.chat(request.message)
    return ChatResponse(response=response)

@app.post("/knowledge")
async def add_knowledge(title: str, content: str):
    bot.add_knowledge(title, content)
    return {"status": "added"}
```

--------------------------------

### Memvid Hybrid Search Modes (CLI)

Source: https://docs.memvid.com/index

Demonstrates different search modes in Memvid CLI: lexical for exact matching, semantic for meaning-based retrieval, and hybrid (default) combining both.

```bash
# Lexical search (fast, exact matching)
memvid find knowledge.mv2 --query "budget" --mode lex

# Semantic search (meaning-based)
memvid find knowledge.mv2 --query "financial outlook" --mode sem

# Hybrid search (combines both - default)
memvid find knowledge.mv2 --query "Q4 projections"
```

--------------------------------

### Troubleshoot: Missing Rows/Columns (Lattice Method)

Source: https://docs.memvid.com/concepts/table-extraction

For tables with borders that might be causing issues with row/column detection, try the 'lattice' method. This method is specifically designed for bordered tables.

```bash
# Try lattice method for bordered tables
memvid tables import memory.mv2 --input report.pdf --method lattice
```

--------------------------------

### Viewing Table Contents with Memvid CLI

Source: https://docs.memvid.com/concepts/table-extraction

Illustrates how to view the contents of a specific table using its ID with the Memvid CLI. Demonstrates both the default formatted table output and JSON output for programmatic use.

```bash
# View table contents
memvid tables view memory.mv2 --table-id pdf_table_1_page1

# JSON output
memvid tables view memory.mv2 --table-id pdf_table_1_page1 --json
```

--------------------------------

### Node.js SDK: Create, Add, Search, and Seal Memory

Source: https://docs.memvid.com/concepts/no-vec-mode

This snippet demonstrates the basic usage of the Memvid Node.js SDK. It covers creating a new memory instance, adding content with metadata, performing immediate searches, and finally sealing the memory. This requires the '@memvid/sdk' package.

```typescript
import { create } from '@memvid/sdk';

// Create memory (no embeddings by default)
const mem = await create('memory.mv2');

// Add content - indexed instantly
await mem.put({
  title: 'My Document',
  label: 'note',
  text: 'Your document content...' 
});

// Search works immediately
const results = await mem.find('search query', { k: 5 });

// Seal when done
await mem.seal();
```

--------------------------------

### Memvid Entity Extraction and Querying (CLI)

Source: https://docs.memvid.com/index

Shows how to use Memvid CLI to enrich documents for fact extraction and perform O(1) lookups for entity states.

```bash
# Extract facts from documents
memvid enrich knowledge.mv2 --engine rules

# Query entity state (O(1) lookup)
memvid state knowledge.mv2 "Alice"
# employer: Anthropic
# role: Senior Engineer
# location: San Francisco
```

--------------------------------

### Memvid Data Insertion (Python)

Source: https://docs.memvid.com/introduction/frames

This Python code snippet illustrates how data is inserted using Memvid's frame-based approach. It shows the `mem.put` method which stores the title, label, metadata, and the full document text. This method ensures atomic storage of all data, including full content, metadata, timestamp, frame position, and crash-safe commit.

```python
# Memvid
mem.put(
    title="Meeting Notes",
    label="meeting",
    metadata={},
    text="Full document content here..."
)
# Stored atomically:
#  Full original content (compressed)
#  All metadata
#  Timestamp + frame position
#  Relationship to previous frames
#  Crash-safe commit
#  Embedding vector (optional - add when you need semantic search)
```

--------------------------------

### Initialize and Use Gemini CLIP Provider (TypeScript)

Source: https://docs.memvid.com/concepts/visual-embeddings

This snippet demonstrates how to initialize and use the Gemini CLIP provider for text embeddings in TypeScript. It shows how to obtain the provider using a factory function and then embed text for visual search.

```typescript
import { getClipProvider, GeminiClip } from '@memvid/sdk';

const clip = getClipProvider('gemini');
const embedding = await clip.embedText('data visualization dashboard');
```

--------------------------------

### Preview Memvid Media File

Source: https://docs.memvid.com/cli/timeline-and-view

Generates a preview for a media item referenced in a memvid file. This is useful for quickly assessing media content without full playback. It requires the memvid executable, the .mv2 file path, and the URI of the media.

```bash
memvid view media.mv2 --uri "mv2://photos/vacation.jpg" --preview
```

--------------------------------

### Use Memvid as a Haystack Document Store

Source: https://docs.memvid.com/frameworks/haystack

Demonstrates how to use Memvid as a document store within Haystack. It shows how to initialize the document store, write documents, and count the total number of documents.

```python
from memvid_sdk import use

# Use as a document store
mem = use('haystack', 'knowledge.mv2')

# Get document store interface
doc_store = mem.as_document_store()

# Write documents
doc_store.write_documents([
    {"content": "Document 1 content", "meta": {"title": "Doc 1"}},
    {"content": "Document 2 content", "meta": {"title": "Doc 2"}}
])

# Count documents
count = doc_store.count_documents()
print(f"Total documents: {count}")
```

--------------------------------

### Exact Deduplication using BLAKE3 Hash (Bash)

Source: https://docs.memvid.com/concepts/deduplication

Demonstrates exact duplicate detection using BLAKE3 hashing with the memvid CLI. The first 'put' creates a new frame, while subsequent 'put' operations with identical content return the existing frame ID without creating duplicates. This ensures data integrity by preventing identical content from being stored multiple times.

```bash
# First put - creates new frame
memvid put memory.mv2 --input document.pdf
# Output: Created frame_abc123

# Second put of same file - returns existing frame
memvid put memory.mv2 --input document.pdf
# Output: Duplicate detected, returning existing frame_abc123
```

--------------------------------

### Extract Entities with Node.js SDK

Source: https://docs.memvid.com/concepts/entity-extraction

Shows how to set up and use the Memvid Node.js SDK for entity extraction. It involves importing the necessary functions, initializing the extractor with a specific provider and entity types, and then processing text to identify entities, outputting their details.

```typescript
import { create, getEntityExtractor } from '@memvid/sdk';

// Initialize entity extractor
const ner = getEntityExtractor('openai', {
  entityTypes: ['COMPANY', 'PERSON', 'MONEY', 'DATE'],
});
console.log(`Provider: ${ner.name}`);
console.log(`Entity types: ${ner.entityTypes}`);

// Extract entities from text
const text = `
Microsoft CEO Satya Nadella announced a $50 million investment in Seattle.
The deal closes December 2024 with Pinnacle Financial as lead investor.
`;

const entities = await ner.extract(text, 0.5);
for (const entity of entities) {
  console.log(`  ${entity.name} (${entity.type}, ${entity.confidence.toFixed(2)})`);
}
```

--------------------------------

### Create and Manage Memvid Memories (Bash)

Source: https://docs.memvid.com/cli/cheat-sheet

Commands to create new memory files, specify options like index types and memory IDs, and view memory information.

```bash
# Create new memory
memvid create memory.mv2

# Create with options
memvid create memory.mv2 --size 25MB          # Custom size
memvid create memory.mv2 --no-vec             # No vector index
memvid create memory.mv2 --no-lex             # No lexical index
memvid create memory.mv2 --memory-id <UUID>   # Bind to dashboard

# View memory info
memvid open memory.mv2
memvid stats memory.mv2
memvid stats memory.mv2 --json
```

--------------------------------

### File Operations API

Source: https://docs.memvid.com/node-sdk/overview

This section details the functions for managing memory files, including creating, opening, verifying, and repairing them. It also provides information on retrieving SDK details.

```APIDOC
## File Operations API

### Description
Functions for creating, opening, closing, verifying, and repairing memory files.

### Methods
- `create(filePath: string, type?: string, options?: CreateOptions)`: Creates a new memory file.
- `open(filePath: string, options?: OpenOptions)`: Opens an existing memory file.
- `close(mem: Memvid)`: Closes a memory file.
- `use(adapter: string, filePath: string, options?: UseOptions)`: Uses a specific adapter for a memory file.
- `verifyMemvid(filePath: string, options?: VerifyOptions)`: Verifies the integrity of a memory file.
- `doctorMemvid(filePath: string, options?: DoctorOptions)`: Repairs and optimizes a memory file.
- `info()`: Retrieves information about the SDK.

### Parameters

#### `create` Parameters
- **filePath** (string) - Required - The path for the new memory file.
- **type** (string) - Optional - The type of memory file (e.g., 'basic').
- **options** (object) - Optional - Configuration options:
  - **enableLex** (boolean) - Enable lexical index.
  - **enableVec** (boolean) - Enable vector index.
  - **memoryId** (string) - Bind to a dashboard memory ID.

#### `open` Parameters
- **filePath** (string) - Required - The path to the existing memory file.
- **options** (object) - Optional - Configuration options.

#### `verifyMemvid` Parameters
- **filePath** (string) - Required - The path to the memory file to verify.
- **options** (object) - Optional - Verification options:
  - **deep** (boolean) - Perform a deep verification.

#### `doctorMemvid` Parameters
- **filePath** (string) - Required - The path to the memory file to repair.
- **options** (object) - Optional - Repair options:
  - **rebuildTimeIndex** (boolean) - Rebuild the time index.
  - **rebuildVecIndex** (boolean) - Rebuild the vector index.
  - **vacuum** (boolean) - Vacuum the file.

### Request Example
```typescript
import { create, open, verifyMemvid, doctorMemvid, info } from '@memvid/sdk';

// Create new memory file
const mem = await create('project.mv2');

// Open existing memory
const existing = await open('project.mv2');

// With options
const memWithOptions = await create('project.mv2', 'basic', {
  enableLex: true,    // Enable lexical index
  enableVec: true,    // Enable vector index
  memoryId: 'mem_abc' // Bind to dashboard
});

// Verify file integrity
await verifyMemvid('project.mv2', { deep: true });

// Repair and optimize
await doctorMemvid('project.mv2', {
  rebuildTimeIndex: true,
  rebuildVecIndex: true,
  vacuum: true
});

// Get SDK info
const sdkInfo = info();
console.log(sdkInfo);
```

### Response
(Varies based on the function called. Typically returns void or status information.)
```

--------------------------------

### Memvid Best Practice: Engine Selection

Source: https://docs.memvid.com/concepts/memory-cards

Provides guidance on selecting the appropriate enrichment engine for different stages of development and production, from quick testing to maximum accuracy.

```bash
# Development
memvid enrich memory.mv2 --engine rules --verbose

# Production
memvid enrich memory.mv2 --engine groq
```

--------------------------------

### Python SDK: Basic Memory Usage

Source: https://docs.memvid.com/concepts/graph-search

Demonstrates basic memory operations using the Memvid Python SDK. It covers initializing a memory instance, putting (adding) data with logic mesh enabled, listing entities, traversing relationships, and performing graph-filtered and hybrid searches.

```python
from memvid import use

mem = use('basic', 'memory.mv2')

# Enable logic mesh during put
mem.put(
    text="John Smith works at Acme Corp as a Senior Engineer.",
    logic_mesh=True
)

# List entities
entities = mem.get_entities()
for entity in entities:
    print(f"{entity.name} ({entity.kind}): {entity.relationship_count} relationships")

# Traverse from an entity
graph = mem.traverse(
    start="John Smith",
    link="works_at",
    hops=2,
    direction="outgoing"
)

for node in graph.nodes:
    print(f"{node.name}: {node.relationships}")

# Graph-filtered search
results = mem.find(
    "quarterly report",
    graph_pattern="?:works_at:Acme Corp"
)

# Hybrid search
results = mem.find(
    "machine learning",
    hybrid=True,
    graph_pattern="?:expertise:ML"
)
```

--------------------------------

### Troubleshoot: Missing Rows/Columns (Sensitivity)

Source: https://docs.memvid.com/concepts/table-extraction

If rows or columns are missing, adjust the detection sensitivity. Setting sensitivity to 'high' can help capture more detailed table structures.

```bash
# Adjust detection sensitivity
memvid tables import memory.mv2 --input report.pdf --sensitivity high
```

--------------------------------

### Directly Execute Memvid Tools

Source: https://docs.memvid.com/frameworks/vercel-ai

Demonstrates direct execution of Memvid's core functions: `memvid_put` for storing documents, `memvid_find` for searching, and `memvid_ask` for question answering with RAG.

```typescript
import { use } from '@memvid/sdk';

const mem = await use('vercel-ai', 'knowledge.mv2');
const tools = mem.tools;

// Store a document
const putResult = await tools.memvid_put.execute({
  title: 'API Documentation',
  label: 'docs',
  text: 'The API supports REST and GraphQL endpoints...',
});
console.log(putResult);  // "Document stored with frame_id: 2"

// Search for documents
const findResult = await tools.memvid_find.execute({
  query: 'API endpoints',
  top_k: 5,
});
console.log(findResult);

// Ask a question
const askResult = await tools.memvid_ask.execute({
  question: 'How do I authenticate with the API?',
  mode: 'auto',
});
console.log(askResult);
```

--------------------------------

### Use Memvid Tools with LlamaIndex Agents (Python)

Source: https://docs.memvid.com/frameworks/llamaindex

Demonstrates using Memvid tools with a LlamaIndex ReAct agent in Python. It initializes the Memvid adapter, sets up an OpenAI LLM, creates a ReAct agent with Memvid tools, and runs a query.

```python
from memvid_sdk import use
from llama_index.llms.openai import OpenAI
from llama_index.core.agent import ReActAgent
import asyncio

# Get Memvid tools
mem = use('llamaindex', 'knowledge.mv2')
tools = mem.tools

# Create ReAct agent
llm = OpenAI(model="gpt-4o")
agent = ReActAgent(
    name="MemvidResearcher",
    tools=tools,
    llm=llm,
    verbose=True
)

# Run agent
async def run():
    response = await agent.run("Search for information about vector stores")
    print(response)

asyncio.run(run())
```

--------------------------------

### Memvid SDK Asset Extraction with Python

Source: https://docs.memvid.com/python-sdk/overview

Shows how to extract assets like frames and raw binary data using the memvid_sdk. It utilizes the `frame` and `blob` methods with URIs. The SDK must be initialized and the provided URIs must be valid.

```python
# Get frame by URI
frame = mem.frame('mv2://docs/intro')

# Get raw binary data
data = mem.blob('mv2://docs/intro')
```

--------------------------------

### CLI Usage - Adaptive Retrieval

Source: https://docs.memvid.com/concepts/adaptive-retrieval

This section details how to use the adaptive retrieval feature via the command-line interface, including enabling it by default, disabling it, and tuning its behavior.

```APIDOC
## CLI Usage - Adaptive Retrieval

Adaptive retrieval is **enabled by default** for the `memvid find` command.

### Default Adaptive Mode

When adaptive mode is enabled by default, the command returns a variable number of results (typically 3-15) based on the relevance distribution of the search query.

```bash
memvid find memory.mv2 --query "authentication best practices"
```

### Disabling Adaptive Mode and Using Fixed Top-K

To disable adaptive retrieval and specify a fixed number of results, use the `--no-adaptive` and `--top-k` flags.

```bash
memvid find memory.mv2 --query "authentication" --no-adaptive --top-k 10
```

### Tuning Adaptive Behavior

Adaptive retrieval can be fine-tuned using the following parameters:

*   **`--min-relevancy`**: Sets a minimum relevance threshold (0.0-1.0). Only results meeting this threshold will be returned.
    ```bash
    memvid find memory.mv2 --query "security" --min-relevancy 0.6
    ```

*   **`--max-k`**: Sets a maximum cap on the number of results returned. The actual number returned may be less than this cap if adaptive detection cuts off earlier.
    ```bash
    memvid find memory.mv2 --query "config" --max-k 50
    ```

*   **`--adaptive-strategy`**: Specifies the algorithm used for adaptive retrieval. Available strategies include `combined` (default), `relative`, `absolute`, `cliff`, and `elbow`.
    ```bash
    memvid find memory.mv2 --query "api" --adaptive-strategy cliff
    ```
```

--------------------------------

### Bash for Memvid Stats and Vacuuming

Source: https://docs.memvid.com/introduction/frames

Provides bash commands to check the statistics of a Memvid file, specifically indicating the amount of space that can be reclaimed by vacuuming, and then executing the vacuum operation to reclaim that space.

```bash
# Check how much space can be reclaimed
memvid stats knowledge.mv2

# Reclaim deleted frame space
memvid doctor knowledge.mv2 --vacuum
```

--------------------------------

### Browse Memvid Media Timeline

Source: https://docs.memvid.com/cli/timeline-and-view

Displays a limited timeline of entries from a media-focused memvid file. This is useful for quickly browsing available media. It requires the memvid executable and the .mv2 file path.

```bash
memvid timeline media.mv2 --limit 20
```

--------------------------------

### Initialize Memvid with CrewAI Adapter (Python)

Source: https://docs.memvid.com/frameworks/crewai

Initializes the Memvid SDK using the 'crewai' adapter to access CrewAI-compatible tools for building AI agents.

```python
from memvid_sdk import use

# Open with CrewAI adapter
mem = use('crewai', 'knowledge.mv2')

# Access CrewAI tools
tools = mem.tools  # Returns CrewAI Tool objects
```

--------------------------------

### Memvid CLI Fact Extraction and Entity Query Workflows

Source: https://docs.memvid.com/cli/index

Demonstrates common workflows for fact extraction and entity querying within Memvid memory files. Covers offline and LLM-based fact enrichment, viewing all extracted memories, querying specific entity states, and exporting facts in JSON format.

```bash
# Extract facts (fast, offline)
memvid enrich project.mv2 --engine rules

# Extract facts (LLM, more accurate)
memvid enrich project.mv2 --engine openai

# View all facts
memvid memories project.mv2

# Query specific entity
memvid state project.mv2 "Alice"

# Export facts
memvid export project.mv2 -o facts.json --format json
```

--------------------------------

### Bash: Fixed Top-K Search (Problematic)

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Demonstrates the traditional fixed top-k approach in Bash, highlighting its limitations. This command always returns 10 results, regardless of whether fewer are relevant (causing noise) or more are relevant (causing missed information).

```bash
# Always returns 10 results, even if:
# - Only 3 are relevant (7 are noise)
# - 25 are relevant (15 are missed)
memvid find memory.mv2 --query "authentication" --top-k 10
```

--------------------------------

### Enable Parallel Ingestion

Source: https://docs.memvid.com/cli/create-and-put

Optimize data ingestion for large datasets by enabling multi-threaded processing. This involves using the '--parallel-segments' flag and optionally tuning the number of threads, segment tokens, and queue depth for efficient parallel ingestion.

```bash
# Enable parallel ingestion
memvid put knowledge.mv2 --input ./large-dataset/ --vector-compression \
  --parallel-segments \
  --parallel-threads 8
```

```bash
# Fine-tune parallel settings
memvid put knowledge.mv2 --input ./corpus/ --vector-compression \
  --parallel-segments \
  --parallel-seg-tokens 4000 \
  --parallel-threads 4 \
  --parallel-queue-depth 16
```

--------------------------------

### Check Memvid API Key Configuration (Bash)

Source: https://docs.memvid.com/concepts/query-usage

A command to verify the currently configured API key within the Memvid CLI. This is useful for troubleshooting issues related to API key authentication or quota tracking.

```bash
memvid config check
```

--------------------------------

### Fetch and ingest from HTTP

Source: https://docs.memvid.com/sdks/cli

Fetches data from an HTTP source and ingests it into a memvid memory file. Supports configuration files, different ingestion modes, and dry runs to preview the operation.

```bash
# Example: memvid api-fetch data.mv2 --config api_config.json --mode ingest --dry-run
```

--------------------------------

### Build Basic Haystack Pipeline with Memvid

Source: https://docs.memvid.com/frameworks/haystack

Constructs a basic Haystack pipeline that uses Memvid as a retriever and OpenAI as a generator. It connects the retriever's output to the generator's prompt for question answering.

```python
from haystack import Pipeline
from haystack.components.generators import OpenAIGenerator
from memvid_sdk import use

# Initialize with haystack adapter
mem = use('haystack', 'knowledge.mv2', read_only=True)

# Get retriever component
retriever = mem.as_retriever(top_k=5)

# Create generator
generator = OpenAIGenerator(model="gpt-4o")

# Build pipeline
pipeline = Pipeline()
pipeline.add_component("retriever", retriever)
pipeline.add_component("generator", generator)
pipeline.connect("retriever.documents", "generator.prompt")

# Run query
result = pipeline.run({
    "retriever": {"query": "What is the architecture?"}
})
print(result["generator"]["replies"][0])
```

--------------------------------

### React Knowledge Search Component

Source: https://docs.memvid.com/examples/knowledge-base

A React component for interacting with a knowledge base, allowing users to search for information and ask AI-driven questions. It manages local state for the query, search results, and AI answers, and makes API calls to '/api/search' and '/api/ask'.

```tsx
import { useState } from 'react';

export function KnowledgeSearch() {
  const [query, setQuery] = useState('');
  const [results, setResults] = useState([]);
  const [answer, setAnswer] = useState('');

  const handleSearch = async () => {
    const res = await fetch('/api/search', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ query, limit: 10 }),
    });
    const data = await res.json();
    setResults(data.results);
  };

  const handleAsk = async () => {
    const res = await fetch('/api/ask', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ question: query }),
    });
    const data = await res.json();
    setAnswer(data.answer);
  };

  return (
    <div className="knowledge-search">
      <input
        type="text"
        value={query}
        onChange={(e) => setQuery(e.target.value)}
        placeholder="Search or ask a question..."
      />
      <button onClick={handleSearch}>Search</button>
      <button onClick={handleAsk}>Ask AI</button>

      {answer && (
        <div className="answer-box">
          <h3>Answer</h3>
          <p>{answer}</p>
        </div>
      )}

      <div className="results">
        {results.map((r, i) => (
          <div key={i} className="result-card">
            <h4>{r.title}</h4>
            <p>{r.snippet}</p>
            <span className="category">{r.category}</span>
          </div>
        ))}
      </div>
    </div>
  );
}
```

--------------------------------

### MemVid Find API (Node.js)

Source: https://docs.memvid.com/concepts/adaptive-retrieval

This section describes how to use the `mem.find` method in Node.js to search for information within a memory store. It covers default adaptive retrieval and custom settings.

```APIDOC
## POST /memvid/memory/find

### Description
Searches for relevant information within a specified memory store using adaptive retrieval.

### Method
POST

### Endpoint
`/memvid/memory/find`

### Parameters
#### Path Parameters
None

#### Query Parameters
* `memory_store` (string) - Required - The identifier of the memory store to search within.

#### Request Body
* `query` (string) - Required - The search query string.
* `options` (object) - Optional - Configuration options for the search.
  * `adaptive` (boolean) - Optional - Whether to use adaptive retrieval (defaults to true).
  * `minRelevancy` (number) - Optional - Minimum relevancy score for results.
  * `maxK` (number) - Optional - Maximum number of results to return.
  * `adaptiveStrategy` (string) - Optional - Strategy for adaptive retrieval (e.g., 'cliff', 'absolute', 'relative', 'combined').
  * `topK` (number) - Optional - Fixed number of top results to return when adaptive is false.
  * `cursor` (string) - Optional - Cursor for paginating results.
  * `mode` (string) - Optional - Search mode (e.g., 'auto', 'lex', 'sem').
  * `start` (string) - Optional - Start date for time filtering (YYYY-MM-DD).
  * `end` (string) - Optional - End date for time filtering (YYYY-MM-DD).

### Request Example (Node.js)
```typescript
import { use } from '@anthropics/memvid'

const mem = await use('basic', 'memory.mv2')

// Adaptive (default)
const results = await mem.find("authentication patterns")
console.log(`Returned ${results.length} relevant results`)

// With custom settings
const results = await mem.find("security", {
  adaptive: true,
  minRelevancy: 0.5,
  maxK: 25,
  adaptiveStrategy: "cliff"
})

// Disable adaptive
const results = await mem.find("config", {
  adaptive: false,
  topK: 10
})
```

### Response
#### Success Response (200)
* `query` (string) - The original search query.
* `strategy` (string) - The adaptive strategy used.
* `results` (array) - An array of search result objects.
  * `frame_id` (string) - Unique identifier for the result frame.
  * `score` (number) - Relevancy score of the result.
  * `title` (string) - Title of the result.
  * `text` (string) - The content of the result (if applicable).
* `adaptive_info` (object) - Information about the adaptive retrieval process.
  * `total_candidates` (number) - Total number of candidate results before adaptive filtering.
  * `cutoff_score` (number) - The score at which results were cut off.
  * `cutoff_reason` (string) - The reason for the cutoff (e.g., 'cliff_detected').
  * `results_returned` (number) - The number of results actually returned.

#### Response Example
```json
{
  "query": "authentication",
  "strategy": "combined",
  "results": [
    {
      "frame_id": "frame_001",
      "score": 0.94,
      "title": "OAuth2 Implementation Guide"
    },
    {
      "frame_id": "frame_002",
      "score": 0.89,
      "title": "JWT Token Handling"
    },
    {
      "frame_id": "frame_003",
      "score": 0.85,
      "title": "Session Management"
    }
  ],
  "adaptive_info": {
    "total_candidates": 150,
    "cutoff_score": 0.72,
    "cutoff_reason": "cliff_detected",
    "results_returned": 3
  }
}
```
```

--------------------------------

### Add Metadata to Documents

Source: https://docs.memvid.com/cli/create-and-put

Add metadata tags, labels, or custom timestamps to documents ingested into a memvid knowledge base. This operation is performed using the 'put' command with specific options for tagging, labeling, or setting timestamps.

```bash
memvid put knowledge.mv2 --input api-docs.md --vector-compression \
  --tag "category=documentation" \
  --tag "version=2.0" \
  --tag "author=team"
```

```bash
memvid put knowledge.mv2 --input report.pdf --vector-compression \
  --label "quarterly" \
  --label "finance"
```

```bash
memvid put knowledge.mv2 --input old-report.pdf --vector-compression \
  --timestamp 1686819000
```

```bash
memvid put knowledge.mv2 --input quarterly-report.pdf --vector-compression \
  --track "reports" \
  --title "Q3 2024 Report" \
  --tag "quarter=Q3" \
  --tag "year=2024"
```

--------------------------------

### List Tables in memvid Project

Source: https://docs.memvid.com/cli

This command lists all available tables within a specified memvid project file. It's a fundamental operation for understanding the structure of your memory data.

```bash
memvid tables list project.mv2
```

--------------------------------

### Enrichment with Claude Cloud Engine (CLI)

Source: https://docs.memvid.com/concepts/memory-cards

This command uses the Anthropic Claude API for the highest quality LLM-based enrichment. It requires an `ANTHROPIC_API_KEY` environment variable to be set and generally has a higher cost.

```bash
export ANTHROPIC_API_KEY=sk-ant-xxx
memvid enrich memory.mv2 --engine claude
```

--------------------------------

### File Operations API

Source: https://docs.memvid.com/python-sdk/overview

Reference for core file operations: creating, opening, closing, and verifying memory files. Supports various adapters and configuration options.

```APIDOC
## File Operations

### `create(filename, **kwargs)`

Creates a new memory file.

- **filename** (str): The name of the memory file.
- **kwargs**:
    - `enable_vec` (bool): Enable vector index.
    - `enable_lex` (bool): Enable lexical index.
    - `memory_id` (str): Bind to dashboard.
    - `api_key` (str): Dashboard API key.

```python
from memvid_sdk import create

# Create new memory file
mem = create('project.mv2')

# With options
mem = create(
    'project.mv2',
    enable_vec=True,      # Enable vector index
    enable_lex=True,      # Enable lexical index
    memory_id='mem_abc',  # Bind to dashboard
    api_key='mv_live_...' # Dashboard API key
)
```

### `use(adapter, filename, mode='open')`

Opens an existing memory file with a specified adapter.

- **adapter** (str): The adapter to use (e.g., 'basic', 'langchain', 'llamaindex').
- **filename** (str): The name of the memory file.
- **mode** (str): The mode to open the file (e.g., 'open').

```python
# Open existing memory with adapter
mem = use('basic', 'project.mv2', mode='open')

# Available adapters
mem = use('langchain', 'file.mv2')
mem = use('llamaindex', 'file.mv2')
mem = use('crewai', 'file.mv2')
mem = use('autogen', 'file.mv2')
mem = use('haystack', 'file.mv2')
mem = use('langgraph', 'file.mv2')
mem = use('semantic-kernel', 'file.mv2')
mem = use('openai', 'file.mv2')
mem = use('google-adk', 'file.mv2')
```

### `info()`

Retrieves SDK information.

```python
from memvid_sdk import info
sdk_info = info()
```

### `verify(deep=False)`

Verifies file integrity.

- **deep** (bool): Perform a deep verification.

```python
result = mem.verify(deep=True)
```

### `doctor(rebuild_time_index=False, rebuild_vec_index=False, vacuum=False)`

Repairs and optimizes the memory file.

- **rebuild_time_index** (bool): Rebuild the time index.
- **rebuild_vec_index** (bool): Rebuild the vector index.
- **vacuum** (bool): Remove unused space.

```python
result = mem.doctor(
    rebuild_time_index=True,
    rebuild_vec_index=True,
    vacuum=True
)
```
```

--------------------------------

### Error Handling (Node.js, Python)

Source: https://docs.memvid.com/quickstart/sdk-recipes

Demonstrates how to handle specific Memvid errors like 'CapacityExceededError' and 'LockedError' using try-catch blocks. The Node.js SDK imports errors from '@memvid/sdk', while the Python SDK imports them from 'memvid_sdk'. Error messages provide specific guidance.

```typescript
import { CapacityExceededError, LockedError } from "@memvid/sdk";

try {
  await mem.put({ title: "Big", label: "kb", file: "huge.bin", enableEmbedding: false });
} catch (err) {
  if (err instanceof CapacityExceededError) console.log("Capacity exceeded");
  else if (err instanceof LockedError) console.log("File is locked");
}
```

```python
from memvid_sdk import CapacityExceededError, LockedError
try:
    mem.put("Big", "kb", {}, file="huge.bin", enable_embedding=False)
except CapacityExceededError:
    print("Upgrade or apply a ticket")
except LockedError:
    print("Another writer holds the lock")
```

--------------------------------

### WAL Commit: Ensuring Data Durability

Source: https://docs.memvid.com/introduction/frames

This section describes the Write-Ahead Log (WAL) commit process in Memvid. It ensures that data frames are durably stored and recoverable even in case of system crashes. Committed frames are persistent, while incomplete ones are discarded, preventing data corruption.

```mermaid
graph LR
    subgraph WAL["Write-Ahead Log"]
        direction LR
        E41[Entry 41<br/>Frame 41 ]
        E42[Entry 42<br/>Frame 42 ]
        E43[Entry 43<br/>pending...]
    end

    E41 --> E42 --> E43

    style E41 fill:#2ecc71,color:#000
    style E42 fill:#FF9900,color:#000
    style E43 fill:#666,color:#fff
```

--------------------------------

### Use OpenAI Embeddings with Memvid Python SDK

Source: https://docs.memvid.com/concepts/embedding-models

Demonstrates initializing OpenAI embeddings and using them with the Memvid Python SDK for storing documents and performing semantic searches. Requires the `OPENAI_API_KEY` environment variable.

```python
from memvid_sdk import create
from memvid_sdk.embeddings import OpenAIEmbeddings

# Initialize embedder
embedder = OpenAIEmbeddings(model='text-embedding-3-small')
print(f"Model: {embedder.model_name} ({embedder.dimension} dimensions)")

# Create memory with vector index
mem = create('knowledge.mv2', enable_vec=True, enable_lex=True)

# Store + embed in batch (vector index required for semantic search)
documents = [
    {"title": "Doc 1", "label": "kb", "text": "Machine learning fundamentals..."},
    {"title": "Doc 2", "label": "kb", "text": "Deep neural networks..."},
]
frame_ids = mem.put_many(documents, embedder=embedder)

# Search with query embedding
query = "How do neural networks work?"
results = mem.find(query, k=5, mode="sem", embedder=embedder)
```

--------------------------------

### Visual Search with CLIP in Memvid

Source: https://docs.memvid.com/introduction/why-memvid

Integrates CLIP for visual search, allowing users to find images and PDF pages based on their visual content by embedding text descriptions and performing similarity searches. This feature does not require external API keys when using local models.

```python
from memvid_sdk.clip import get_clip_provider

clip = get_clip_provider('local')  # No API keys needed
embedding = clip.embed_text("pie chart showing market share")

# Find visually similar content
results = mem.visual_search(embedding, k=10)
```

--------------------------------

### Add Metadata to Ingested Files

Source: https://docs.memvid.com/cli/create-and-put

Shows how to add metadata, specifically tracks, to ingested files using the memvid CLI. This helps in organizing and categorizing the data.

```bash
# Add to a specific track
memvid put knowledge.mv2 --input meeting-notes.md --vector-compression --track "meetings"
```

--------------------------------

### Ingest Documents into Memvid Memory File (Bash)

Source: https://docs.memvid.com/cli/create-and-put

Command to add documents to a Memvid memory file. Supports ingesting single files, directories, or standard input. Options include semantic embeddings, compression, and metadata customization.

```bash
# Ingest a single file (text-only)
memvid put my-knowledge.mv2 --input document.pdf

# Ingest a directory
memvid put my-knowledge.mv2 --input ./documents/

# Ingest with semantic embeddings (+16x PQ compression)
memvid put my-knowledge.mv2 --input document.pdf --embedding --vector-compression

# Ingest from stdin (text-only by default)
echo "Some text content" | memvid put my-knowledge.mv2
```

--------------------------------

### Find Code Implementations and Patterns (Lexical/Auto Mode)

Source: https://docs.memvid.com/cli/search-and-ask

Searching code files for specific function implementations or error handling patterns using lexical or automatic search modes. This helps in locating and understanding code segments.

```bash
# Find function implementations
memvid find code.mv2 --query "handleUserLogin" --mode lex
```

```bash
# Find error handling patterns
memvid find code.mv2 --query "try catch error handling" --mode auto
```

--------------------------------

### Handle Memvid Quota Exceeded Error (Python SDK)

Source: https://docs.memvid.com/concepts/query-usage

Demonstrates how to catch and handle the `QuotaExceededError` when a Memvid query operation exceeds the monthly plan limit using the Python SDK. Provides details on used queries, limits, reset dates, and upgrade links.

```python
from memvid import use, QuotaExceededError, MemvidError

mem = use('basic', 'memory.mv2')

try:
    results = mem.find("search query")
except QuotaExceededError as e:
    print(f"Quota exceeded!")
    print(f"Used: {e.used} / {e.limit}")
    print(f"Resets: {e.reset_date}")
    print(f"Upgrade at: {e.upgrade_url}")
except MemvidError as e:
    print(f"Other error: {e}")
```

--------------------------------

### Memvid CLI: Combining Adaptive Search with Time Filtering

Source: https://docs.memvid.com/concepts/adaptive-retrieval

Shows how Memvid's adaptive search respects time-based filters, ensuring that results are not only relevant but also fall within the specified date range.

```bash
# Adaptive respects filters
memvid find memory.mv2 --query "report" \
  --start 2024-01-01 \
  --end 2024-06-30 \
  --adaptive-strategy cliff
```

--------------------------------

### Memvid OpenAI Adapter Functions

Source: https://docs.memvid.com/frameworks/openai

The Memvid OpenAI adapter exposes three primary functions for interacting with your knowledge base: storing documents, searching for information, and asking questions with RAG-style answer synthesis.

```APIDOC
## Available Functions

The OpenAI adapter provides three functions:

| Function      | Description                                           |
| ------------- | ----------------------------------------------------- |
| `memvid_put`  | Store documents in memory with title, label, and text |
| `memvid_find` | Search for relevant documents by query                |
| `memvid_ask`  | Ask questions with RAG-style answer synthesis         |
```

--------------------------------

### Create a new .mv2 file

Source: https://docs.memvid.com/sdks/cli

Creates a new memvid memory file (`.mv2`). Key flags allow specifying the storage tier (`--tier`), size (`--size`), and disabling lexical (`--no-lex`) or vector (`--no-vector`) indexes.

```bash
# Example: memvid create data.mv2 --tier free --size 1GB
# Example: memvid create no_indexes.mv2 --no-lex --no-vector
```

--------------------------------

### Embed Text for Visual Search (Python)

Source: https://docs.memvid.com/concepts/visual-embeddings

This snippet demonstrates how to embed text for visual search using the OpenAI CLIP provider in Python. It initializes the CLIP provider and then generates an embedding for a given text query. The output shows the dimensions of the generated embedding.

```python
import clip

# Embed text for visual search
embedding = clip.embed_text('executive team photo')
print(f"Dimensions: {len(embedding)}")
```

--------------------------------

### Basic Enrichment with Rules Engine (CLI)

Source: https://docs.memvid.com/concepts/memory-cards

This command uses the 'rules' engine for basic pattern-based extraction using regex. It is fast and free but has limited extraction capabilities, focusing on emails, phone numbers, dates, and URLs.

```bash
memvid enrich memory.mv2 --engine rules
```

--------------------------------

### Add Knowledge to Memvidbot

Source: https://docs.memvid.com/examples/chatbot-memory

Enables adding new knowledge entries to the Memvid Chatbot's knowledge base.

```APIDOC
## POST /knowledge

### Description
Add a new title and content to the Memvid Chatbot's knowledge base.

### Method
POST

### Endpoint
/knowledge

### Parameters
#### Query Parameters
- **title** (str) - Required - The title of the knowledge entry.
- **content** (str) - Required - The content of the knowledge entry.

### Response
#### Success Response (200)
- **status** (str) - Indicates the status of the operation, e.g., "added".

#### Response Example
```json
{
  "status": "added"
}
```
```

--------------------------------

### Use Memvid Tools with generateText

Source: https://docs.memvid.com/frameworks/vercel-ai

Demonstrates using Memvid's tools (memvid_put, memvid_find, memvid_ask) within the Vercel AI SDK's generateText function. Allows for multiple tool calls to perform complex queries and information retrieval.

```typescript
import { use } from '@memvid/sdk';
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

// Get Memvid tools
const mem = await use('vercel-ai', 'knowledge.mv2');

// Use with generateText
const result = await generateText({
  model: openai('gpt-4o-mini'),
  tools: mem.tools,
  maxSteps: 5,  // Allow multiple tool calls
  system: 'You are a helpful assistant with access to a knowledge base.',
  prompt: 'Search for information about authentication and summarize it.',
});

// Access the result
console.log(result.text);

// View tool calls made
for (const step of result.steps) {
  if (step.toolCalls) {
    for (const call of step.toolCalls) {
      console.log(`Tool: ${call.toolName}, Args: ${JSON.stringify(call.args)}`);
    }
  }
}
```

--------------------------------

### Memvid Node.js SDK: Ask with PII Masking

Source: https://docs.memvid.com/concepts/pii-masking

Illustrates using the Node.js SDK to ask questions with PII masking. The `maskPii` option in the configuration object enables PII redaction.

```APIDOC
## Node.js SDK: Ask with PII Masking

### Description
Uses the Memvid Node.js SDK to perform an 'ask' operation with PII masking enabled. Sensitive information in the response is automatically redacted.

### Method
```typescript
mem.ask(question, { maskPii: true })
```

### Parameters
*   `question` (string) - The question to ask the AI.
*   `options` (object) - Optional configuration object.
    *   `maskPii` (boolean) - If `true`, PII will be masked in the response. Defaults to `false`.

### Request Example
```typescript
import { use } from '@anthropics/memvid'

const mem = await use('basic', 'memory.mv2')

const response = await mem.ask(
  "What is the customer's contact information?",
  { maskPii: true }
)
console.log(response.answer)
```

### Response Example (Masked)
```
Customer can be reached at [EMAIL] or [PHONE]
```
```

--------------------------------

### Apply Vector Compression (CLI)

Source: https://docs.memvid.com/faq/general

Command-line interface command to enable vector compression when ingesting data into a Memvid memory file. Vector compression significantly reduces vector size with minimal quality loss.

```bash
memvid put knowledge.mv2 --input docs/ --vector-compression
```

--------------------------------

### Inspect Memvid Memory File (Bash)

Source: https://docs.memvid.com/cli/create-and-put

Command to inspect metadata and manifests of an existing Memvid memory file. The `--json` option provides output in JSON format for scripting.

```bash
memvid open my-memory.mv2
memvid open my-memory.mv2 --json
```

--------------------------------

### Verify Memvid Search and Content (Bash)

Source: https://docs.memvid.com/resources/troubleshooting

These Bash commands help troubleshoot search issues in Memvid by checking frame counts and attempting searches with different modes. Ensure your content exists and the search mode is appropriate.

```bash
memvid stats knowledge.mv2  # Check frame count
memvid find knowledge.mv2 --query "test" --mode lex  # Try lexical
```

--------------------------------

### Enrichment with Candle Engine (Local LLM) (CLI)

Source: https://docs.memvid.com/concepts/memory-cards

The 'candle' engine utilizes the Phi-3.5-mini model locally via HuggingFace Candle. It offers good quality without API keys and can run on a CPU, though it's slower than API-based engines. The model is downloaded automatically on the first run.

```bash
memvid enrich memory.mv2 --engine candle
```

--------------------------------

### View and Export Extracted Tables

Source: https://docs.memvid.com/cli/create-and-put

Manage extracted tables from PDFs using the 'tables' command. This includes listing all tables within a memory, viewing a specific table by its ID, and exporting tables to CSV or JSON formats for further analysis.

```bash
# List all tables in a memory
memvid tables list knowledge.mv2
```

```bash
# View a specific table
memvid tables view knowledge.mv2 --table-id pdf_table_1_page1
```

```bash
# Export to CSV
memvid tables export knowledge.mv2 --table-id pdf_table_1_page1 --format csv > data.csv
```

```bash
# Export to JSON
memvid tables export knowledge.mv2 --table-id pdf_table_1_page1 --format json
```

--------------------------------

### Enrichment with Verbose Output (CLI)

Source: https://docs.memvid.com/concepts/memory-cards

This command enriches memory cards with the Groq engine and enables verbose output, providing more detailed information during the enrichment process.

```bash
memvid enrich memory.mv2 --engine groq --verbose
```

--------------------------------

### Grounding & Hallucination Detection with ask()

Source: https://docs.memvid.com/node-sdk/overview

Analyzes the grounding quality of an LLM answer, indicating how well it's supported by the provided context. It returns a score, a label (LOW, MEDIUM, HIGH), sentence counts, and a warning flag for potential hallucinations. It also provides follow-up suggestions if confidence is low.

```typescript
const answer = await mem.ask('What is the API endpoint?', {
  model: 'gpt-4o-mini',
  modelApiKey: process.env.OPENAI_API_KEY
});

// Check grounding quality
console.log(answer.grounding);
// {
//   score: 0.85,
//   label: 'HIGH',           // 'LOW', 'MEDIUM', or 'HIGH'
//   sentence_count: 3,
//   grounded_sentences: 3,
//   has_warning: false,
//   warning_reason: undefined
// }

// Check if follow-up is needed
if (answer.follow_up?.needed) {
  console.log('Low confidence:', answer.follow_up.reason);
  console.log('Try these instead:', answer.follow_up.suggestions);
}
```

--------------------------------

### Node.js: OpenAI Embeddings and Memory Management

Source: https://docs.memvid.com/concepts/embedding-models

Demonstrates initializing OpenAI embeddings, creating a memory instance, batch storing and embedding documents, and performing semantic searches. Requires OPENAI_API_KEY environment variable.

```typescript
import { create, OpenAIEmbeddings } from '@memvid/sdk';

// Initialize embedder (uses OPENAI_API_KEY env var)
const embedder = new OpenAIEmbeddings({ model: 'text-embedding-3-small' });
console.log(`Model: ${embedder.modelName} (${embedder.dimension} dimensions)`);

// Create memory
const mem = await create('knowledge.mv2');

// Store + embed in batch (vector index required for semantic search)
await mem.putMany(
  [
    { title: 'Doc 1', text: 'Machine learning fundamentals...' },
    { title: 'Doc 2', text: 'Deep neural networks...' },
  ],
  { embedder }
);
await mem.seal();

// Query using the same embedder (keeps dimensions consistent)
const results = await mem.find('How do neural networks work?', { mode: 'sem', k: 5, embedder });

```

--------------------------------

### Memvid SDK Error Handling in Python

Source: https://docs.memvid.com/sdks/python

Demonstrates how to handle various Memvid SDK errors using Python's try-except blocks. It catches specific exceptions like CapacityExceededError, LockedError, VecDimensionMismatchError, EmbeddingFailedError, and a general MemvidError, providing informative messages for each.

```python
from memvid_sdk import (
    CapacityExceededError,
    LockedError,
    VecDimensionMismatchError,
    EmbeddingFailedError,
    MemvidError
)

try:
    mem.put(title='Doc', label='test', metadata={}, text='Content')
except CapacityExceededError:
    print('Storage limit reached')
except LockedError:
    print('File locked by another process')
except VecDimensionMismatchError:
    print('Embedding dimension mismatch')
except EmbeddingFailedError:
    print('Embedding generation failed')
except MemvidError as e:
    print(f'Error [{e.code}]: {e.message}')
```

--------------------------------

### Check Memvid memory statistics (CLI)

Source: https://docs.memvid.com/concepts/capacity-and-plans

Displays detailed statistics about a Memvid memory file, including document count, size, capacity, utilization percentage, and enabled indexes. This is useful for monitoring storage usage.

```bash
memvid stats knowledge.mv2
```

--------------------------------

### Python for Accessing Frames by ID or URI

Source: https://docs.memvid.com/introduction/frames

Shows how to retrieve frame data using either its internal integer Frame ID or its human-readable URI. This flexibility aids in accessing specific data points within the Memvid system.

```python
# Access by frame ID
frame = mem.frame(124)

# Access by URI
frame = mem.frame('mv2://docs/api.md')
```

--------------------------------

### Lecture Archive Management with Memvid

Source: https://docs.memvid.com/concepts/audio-video

Utilize memvid to manage lecture archives. Create a memory file, ingest lecture videos potentially using a specified Whisper model for transcription, search for topics, and ask study-related questions.

```bash
# Create lecture memory
memvid create lectures.mv2

# Ingest lecture videos
memvid put lectures.mv2 --input ./cs101/ --whisper-model medium

# Search for topics
memvid find lectures.mv2 --query "binary search algorithm"

# Ask study questions
memvid ask lectures.mv2 --question "Explain the time complexity of quicksort"
```

--------------------------------

### Timeline and Stats (Node.js, Python)

Source: https://docs.memvid.com/quickstart/sdk-recipes

Retrieves timeline information and statistics about the Memvid instance. The 'timeline' function accepts parameters like 'limit' and 'asOfFrame', while 'stats' provides general usage statistics. Both are available in Node.js and Python SDKs.

```typescript
await mem.timeline({ limit: 10, asOfFrame: 200 });
const stats = await mem.stats();
```

```python
mem.timeline(limit=10, as_of_frame=200)
stats = mem.stats()
```

--------------------------------

### Initialize Entity Extractor Factory Function (Python)

Source: https://docs.memvid.com/concepts/entity-extraction

Demonstrates the usage of the `get_entity_extractor` factory function in Python to initialize an entity extractor. It supports various providers like 'local', 'openai', 'claude', and 'gemini', and allows specifying custom entity types and API keys.

```python
# Python
from memvid_sdk.entities import get_entity_extractor

ner = get_entity_extractor(
    provider,           # 'local', 'openai', 'claude', 'gemini', 'openai:model-name'
    entity_types=None,  # Custom entity types (cloud providers only)
    api_key=None,       # Override env var
)
```

--------------------------------

### Using Cliff Strategy for Adaptive Search

Source: https://docs.memvid.com/concepts/adaptive-retrieval

When adaptive search returns too many results, switching to the 'cliff' strategy can help. This strategy modifies how adaptive search prunes results, potentially leading to a more manageable number of relevant outputs. It's applied via a command-line option.

```bash
--adaptive-strategy cliff
```

--------------------------------

### Ingest Data from Stdin

Source: https://docs.memvid.com/cli/create-and-put

Ingest data directly from standard input (stdin), which is useful for piping data from other commands or scripts. This allows for flexible data sourcing, including text content, curl requests, or filtered command outputs.

```bash
# Pipe text content
echo "Important note to remember" | memvid put knowledge.mv2 --vector-compression
```

```bash
# Pipe from curl
curl -s https://api.example.com/data | memvid put knowledge.mv2 --vector-compression --title "API Response"
```

```bash
# Pipe from another command
cat log.txt | grep "ERROR" | memvid put knowledge.mv2 --vector-compression --track "errors"
```

--------------------------------

### Troubleshoot Embedding Model Issues

Source: https://docs.memvid.com/cli/create-and-put

Provides solutions for errors related to loading embedding models in Memvid. This includes setting the directory for model storage using the `MEMVID_MODELS_DIR` environment variable or enabling offline mode with pre-cached models using `MEMVID_OFFLINE`.

```bash
# Set model directory
export MEMVID_MODELS_DIR=~/.memvid/models

# Or use offline mode with pre-cached models
export MEMVID_OFFLINE=1
```

--------------------------------

### Node.js: Initialize Cohere Embeddings

Source: https://docs.memvid.com/concepts/embedding-models

Demonstrates initializing Cohere embeddings in Node.js, both directly and via a factory. Supports various Cohere embedding models.

```typescript
import { CohereEmbeddings, getEmbedder } from '@memvid/sdk';

// Direct initialization
const embedder = new CohereEmbeddings({ model: 'embed-english-v3.0' });

// Or use factory
const embedder2 = getEmbedder('cohere', { model: 'embed-multilingual-v3.0' });

const embeddings = await embedder.embedDocuments(['Text 1', 'Text 2']);

```

--------------------------------

### Build Compound Queries

Source: https://docs.memvid.com/concepts/query-usage

Constructs compound queries by joining multiple search terms with 'OR'. This allows for more flexible and comprehensive searches.

```python
results = mem.find(" OR ".join(search_terms))
```

--------------------------------

### Memvid CLI: File Repair and Optimization

Source: https://docs.memvid.com/architecture/overview

CLI tools for diagnosing and repairing Memvid database files. Commands include previewing fixes ('--plan-only'), rebuilding indexes ('--rebuild-time-index', '--rebuild-lex-index'), and compacting deleted data ('--vacuum'). Requires the Memvid CLI.

```bash
# Preview what would be fixed
memvid doctor notes.mv2 --plan-only

# Rebuild corrupted time index
memvid doctor notes.mv2 --rebuild-time-index

# Rebuild lexical index
memvid doctor notes.mv2 --rebuild-lex-index

# Compact deleted frames
memvid doctor notes.mv2 --vacuum
```

--------------------------------

### Initialize Memvid with Google ADK (Node.js)

Source: https://docs.memvid.com/frameworks/google-adk

Initializes Memvid using the 'google-adk' adapter, specifying a memory file ('knowledge.mv2'). It demonstrates how to access native ADK function declarations (tools) and their corresponding executors.

```typescript
import { use } from '@memvid/sdk';

// Open with Google ADK adapter
const mem = await use('google-adk', 'knowledge.mv2');

// Access ADK function declarations
const tools = mem.tools;       // FunctionDeclaration[] for Gemini API
const executors = mem.functions; // Function executors by name
```

--------------------------------

### Extract Entities with Python SDK

Source: https://docs.memvid.com/concepts/entity-extraction

Demonstrates how to initialize an entity extractor using the Memvid Python SDK, specifying the provider and entity types, and then extracting entities from a given text. The output includes the extracted entity's name, type, and confidence score.

```python
from memvid_sdk import create
from memvid_sdk.entities import get_entity_extractor

# Initialize entity extractor
ner = get_entity_extractor('openai', entity_types=['COMPANY', 'PERSON', 'MONEY', 'DATE'])
print(f"Provider: {ner.name}")
print(f"Entity types: {ner.entity_types}")

# Extract entities from text
text = """
Microsoft CEO Satya Nadella announced a $50 million investment in Seattle.
The deal closes December 2024 with Pinnacle Financial as lead investor.
"""

entities = ner.extract(text, min_confidence=0.5)
for entity in entities:
    print(f"  {entity['name']} ({entity['type']}, {entity['confidence']:.2f})")
```

--------------------------------

### Accessing Memvid Tools in Python

Source: https://docs.memvid.com/frameworks/overview

This snippet demonstrates how to access the core Memvid tools (put, find, ask) within a Python environment, likely when using a framework integration.

```python
# Access tools (framework-specific format)
tools = mem.tools
```

--------------------------------

### Memvid Python SDK: Ask with PII Masking

Source: https://docs.memvid.com/concepts/pii-masking

Demonstrates how to use the Python SDK to ask questions with PII masking enabled. The `mask_pii` parameter controls the redaction of sensitive data in the answer.

```APIDOC
## Python SDK: Ask with PII Masking

### Description
Uses the Memvid Python SDK to perform an 'ask' operation with PII masking enabled. Sensitive information in the response is automatically redacted.

### Method
```python
mem.ask(question, mask_pii=True)
```

### Parameters
*   `question` (str) - The question to ask the AI.
*   `mask_pii` (bool) - If `True`, PII will be masked in the response. Defaults to `False`.

### Request Example
```python
from memvid import use

mem = use('basic', 'memory.mv2')

response = mem.ask(
    "What is the customer's contact information?",
    mask_pii=True
)
print(response.answer)
```

### Response Example (Masked)
```
Customer can be reached at [EMAIL] or [PHONE]
```
```

--------------------------------

### Python DocumentQA Class for Document Processing

Source: https://docs.memvid.com/examples/document-qa

The DocumentQA class handles the ingestion and querying of documents using the memvid_sdk. It supports multiple file formats like PDF, DOCX, TXT, MD, and HTML. The class utilizes hashing for unique document identification and provides methods for ingesting files/folders, asking questions, searching, and retrieving statistics. Dependencies include memvid_sdk, pathlib, typing, and hashlib.

```python
from memvid_sdk import use
from pathlib import Path
from typing import List, Optional
import hashlib

class DocumentQA:
    """Document Q&A system with Memvid."""

    SUPPORTED_FORMATS = {'.pdf', '.docx', '.txt', '.md', '.html'}

    def __init__(self, memory_path: str = "documents.mv2"):
        self.mem = use('basic', memory_path, mode='auto')
        self.stats = {"ingested": 0, "failed": 0}

    def ingest_file(self, filepath: str, metadata: Optional[dict] = None) -> bool:
        """Ingest a single file."""
        path = Path(filepath)

        if path.suffix.lower() not in self.SUPPORTED_FORMATS:
            print(f" Unsupported format: {path.suffix}")
            return False

        try:
            # Generate unique ID based on content hash
            content_hash = hashlib.md5(path.read_bytes()).hexdigest()[:8]

            self.mem.put({
                "title": path.name,
                "label": "document",
                "file": str(path.absolute()),
                "metadata": {
                    "path": str(path),
                    "size": path.stat().st_size,
                    "hash": content_hash,
                    **(metadata or {})
                }
            })

            self.stats["ingested"] += 1
            print(f" Ingested: {path.name}")
            return True

        except Exception as e:
            self.stats["failed"] += 1
            print(f" Failed: {path.name} - {e}")
            return False

    def ingest_folder(self, folder_path: str, recursive: bool = True) -> dict:
        """Ingest all documents from a folder."""
        folder = Path(folder_path)

        pattern = "**/*" if recursive else "*"
        files = [f for f in folder.glob(pattern)
                 if f.is_file() and f.suffix.lower() in self.SUPPORTED_FORMATS]

        print(f" Found {len(files)} documents to ingest...")

        for filepath in files:
            self.ingest_file(str(filepath))

        return self.stats

    def ask(self, question: str, k: int = 5) -> dict:
        """Ask a question about the documents."""
        result = self.mem.ask(question, k=k)

        return {
            "answer": result.text,
            "sources": [
                {
                    "title": s.title,
                    "snippet": s.snippet,
                    "score": s.score
                }
                for s in result.sources
            ],
            "confidence": result.confidence if hasattr(result, 'confidence') else None
        }

    def search(self, query: str, k: int = 10) -> List[dict]:
        """Search documents without generating an answer."""
        results = self.mem.find(query, k=k)

        return [
            {
                "title": hit.title,
                "snippet": hit.snippet,
                "score": hit.score,
                "metadata": hit.metadata
            }
            for hit in results.hits
        ]

    def get_stats(self) -> dict:
        """Get document store statistics."""
        stats = self.mem.stats()
        return {
            "total_documents": stats.get("frame_count", 0),
            "size_bytes": stats.get("size_bytes", 0),
            "size_mb": round(stats.get("size_bytes", 0) / 1024 / 1024, 2)
        }

```

--------------------------------

### Command-Line: Hybrid Search Configuration

Source: https://docs.memvid.com/concepts/no-vec-mode

These bash commands illustrate how to configure Memvid for hybrid search using the command-line interface. They show adding content with and without vector embeddings, enabling semantic search for specific directories, and skipping embeddings for bulk imports.

```bash
# Main content - lexical is enough
memvid put memory.mv2 --input ./code/
memvid put memory.mv2 --input ./logs/
```

```bash
# Documentation - add semantic search
memvid put memory.mv2 --input ./docs/ --enable-vec
```

```bash
# Bulk import without embeddings
memvid put memory.mv2 --input ./large-archive/ --embedding-skip
```

--------------------------------

### Exact Deduplication using BLAKE3 Hash (Python)

Source: https://docs.memvid.com/concepts/deduplication

Illustrates exact duplicate detection via BLAKE3 hashing using the Memvid Python SDK. When the same content string is 'put' twice, the SDK returns the same frame ID, confirming that no new duplicate frame was created. This is essential for maintaining efficient storage.

```python
# Python SDK
frame_id_1 = mem.put("The quick brown fox")
frame_id_2 = mem.put("The quick brown fox")  # Same content

assert frame_id_1 == frame_id_2  # True - no duplicate created
```

--------------------------------

### Paginated Search Results

Source: https://docs.memvid.com/cli/search-and-ask

Retrieve search results in pages for large datasets. The first request fetches the initial set of results, and subsequent requests use a cursor obtained from the previous response to fetch the next page.

```bash
# First page
memvid find knowledge.mv2 --query "api" --top-k 10 --json

# Next page using cursor from previous response
memvid find knowledge.mv2 --query "api" --top-k 10 --cursor "eyJvZmZzZXQiOjEwfQ"
```

--------------------------------

### Troubleshoot: Poor Quality Extraction (Check Scores)

Source: https://docs.memvid.com/concepts/table-extraction

To diagnose poor extraction quality, list tables and check their 'quality' scores. This helps identify which tables might need re-extraction.

```bash
# Check quality scores
memvid tables list memory.mv2 --json | jq '.[] | {id, quality}'
```

--------------------------------

### Enabling PII Masking via CLI and Python

Source: https://docs.memvid.com/concepts/pii-masking

Demonstrates how to enable PII masking. For command-line interface (CLI) usage, the `--mask-pii` flag is used. In Python, the `mask_pii=True` parameter is passed to the relevant function.

```bash
# CLI: use --mask-pii flag
memvid ask memory.mv2 -q "..." --mask-pii
```

```python
# Python: mask_pii=True parameter
mem.ask("...", mask_pii=True)
```

--------------------------------

### Streaming Responses

Source: https://docs.memvid.com/frameworks/google-adk

Demonstrates how to stream responses from the Gemini API using the Memvid SDK.

```APIDOC
## Streaming Responses

```python
from google import genai
from memvid_sdk import use

mem = use('google-adk', 'knowledge.mv2', read_only=True)
client = genai.Client()

# Stream response
for chunk in client.models.generate_content_stream(
    model="gemini-2.0-flash",
    contents="Explain the architecture based on the knowledge base",
    config=types.GenerateContentConfig(
        tools=mem.tools,
        system_instruction="You have access to a knowledge base."
    )
):
    print(chunk.text, end="")
```
```

--------------------------------

### Node.js SDK: Basic Memory Usage

Source: https://docs.memvid.com/concepts/graph-search

Demonstrates basic memory operations using the Memvid Node.js SDK. It covers initializing a memory instance, putting (adding) data with logic mesh enabled, listing entities, and performing graph-filtered searches.

```typescript
import { use } from '@anthropics/memvid'

const mem = await use('basic', 'memory.mv2')

// Enable logic mesh during put
await mem.put({
  content: "John Smith works at Acme Corp as a Senior Engineer.",
  logicMesh: true
})

// List entities
const entities = await mem.getEntities()
for (const entity of entities) {
  console.log(`${entity.name} (${entity.kind}): ${entity.relationshipCount} relationships`)
}

// Traverse from an entity
const graph = await mem.traverse({
  start: "John Smith",
  link: "works_at",
  hops: 2,
  direction: "outgoing"
})

// Graph-filtered search
const results = await mem.find("quarterly report", {
  graphPattern: "?:works_at:Acme Corp"
})
```