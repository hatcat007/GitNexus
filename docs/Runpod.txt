### Install Runpod SDK

Source: https://docs.runpod.io/serverless/quickstart

Installs the Runpod SDK using pip, which is necessary for building and deploying Serverless workers. This command should be run after activating the virtual environment.

```sh
pip install runpod
```

--------------------------------

### Install Runpod Network Volume Storage Tool

Source: https://docs.runpod.io/community-solutions/runpod-network-volume-storage-tool

Clones the GitHub repository and installs project dependencies using 'uv sync'. This is the initial setup step for using the tool.

```bash
git clone https://github.com/justinwlin/Runpod-Network-Volume-Storage-Tool.git
cd Runpod-Network-Volume-Storage-Tool

# Install dependencies with uv
uv sync
```

--------------------------------

### Example Docker Arguments Configuration

Source: https://docs.runpod.io/sdks/graphql/configurations

Shows how to override the default container start command using `dockerArgs`. If not provided, the container relies on the start command from the Docker image. For example, `sleep infinity` keeps the container running.

```json
{
  "dockerArgs": "sleep infinity"
}
```

--------------------------------

### SSH and SCP Connection Examples (Bash)

Source: https://docs.runpod.io/pods/configuration/use-ssh

These examples demonstrate how to connect to your RunPod instance via SSH and transfer files using SCP after the password-based SSH setup is complete. The examples use the detected external SSH port and provide commands for copying files to and from the pod.

```bash
========================================
SSH CONNECTION
========================================
Connect using: ssh root@38.80.152.73 -p 32061
Password: helloworld

========================================
FILE TRANSFER EXAMPLES (SCP)
========================================
Copy file TO pod:
scp -P 32061 yourfile.txt root@38.80.152.73:/workspace/

Copy file FROM pod:
scp -P 32061 root@38.80.152.73:/workspace/yourfile.txt .

Copy entire folder TO pod:
scp -P 32061 -r yourfolder root@38.80.152.73:/workspace/
```

--------------------------------

### Python Client Library Example

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

Example of how to use the OpenAI-compatible API with the Python client library.

```APIDOC
## Python Client Library Usage

### Description
This example demonstrates how to interact with the RunPod API using the official OpenAI Python client library.

### Code Example
```python
from openai import OpenAI

# Replace with your actual model name deployed on Runpod
MODEL_NAME = "YOUR_MODEL_NAME"

# Replace ENDPOINT_ID and RUNPOD_API_KEY with your actual values
client = OpenAI(
    api_key="RUNPOD_API_KEY",
    base_url="https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1"
)

try:
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"}
        ]
    )
    print(response.choices[0].message.content)
except Exception as e:
    print(f"An error occurred: {e}")
```
```

--------------------------------

### Clone Example Repository (Bash)

Source: https://docs.runpod.io/tutorials/pods/build-docker-images

Clones the example GitHub repository containing code for building Docker images with Bazel. This is the first step to get the necessary project files.

```bash
git clone https://github.com/therealadityashankar/build-docker-in-runpod.git && cd build-docker-in-runpod
```

--------------------------------

### Example Container Start Commands for Pod Templates

Source: https://docs.runpod.io/pods/templates/manage-templates

Customize the command that runs when your Pod starts. This can be a simple bash command or a JSON object specifying the entrypoint and command. Ensure the command is compatible with your container image.

```bash
bash -c 'mkdir /workspace && /start.sh'
```

```json
{"cmd": ["python", "app.py"], "entrypoint": ["bash", "-c"]}
```

--------------------------------

### Install Runpod Go SDK

Source: https://docs.runpod.io/sdks/go/overview

Installs the Runpod SDK package for Go projects using the 'go get' command. It also includes a command to tidy up dependencies.

```sh
go get github.com/runpod/go-sdk
go mod tidy
```

--------------------------------

### Local Server Setup with Python

Source: https://docs.runpod.io/tutorials/serverless/generate-sdxl-turbo

Provides a command to start a local HTTP server using Python's built-in module. This allows you to serve your `index.html` file and other static assets from your local machine, enabling you to test the web application in a browser environment.

```bash
python -m http.server 8000
```

--------------------------------

### Complete Serverless Function Code Example

Source: https://docs.runpod.io/tutorials/sdks/python/101/local-server-testing

The full Python code for the serverless string reversal function, combining imports, utility functions, the handler, and the worker start command. This provides a complete, runnable example.

```python
import runpod


def reverse_string(s):
    return s[::-1]


def handler(job):
    print(f"string-reverser | Starting job {job['id']}")
    job_input = job["input"]

    input_string = job_input.get("text", "")

    if not input_string:
        return {"error": "No input text provided"}

    reversed_string = reverse_string(input_string)

    job_output = {"original_text": input_string, "reversed_text": reversed_string}

    return job_output


runpod.serverless.start({"handler": handler})
```

--------------------------------

### Get Runpod CLI Help

Source: https://docs.runpod.io/runpodctl/overview

Displays the main help message for the Runpod CLI, listing available top-level commands. This is a starting point for exploring the CLI's capabilities.

```shell
runpodctl help
```

--------------------------------

### Install Runpod Python SDK

Source: https://docs.runpod.io/sdks/python/overview

Installs the Runpod Python SDK using pip. This command should be run after activating a virtual environment.

```bash
python -m pip install runpod
```

--------------------------------

### Sample JSON Response from Serverless Endpoint

Source: https://docs.runpod.io/serverless/quickstart

An example of a JSON response received after sending a request to a deployed Serverless endpoint. It includes details like 'delayTime', 'executionTime', 'output', and 'status'. The 'output' field reflects the result of the worker's execution, in this case, echoing the input prompt.

```json
{
    "delayTime": 15088,
    "executionTime": 60,
    "id": "04f01223-4aa2-40df-bdab-37e5caa43cbe-u1",
    "output": "Hello World",
    "status": "COMPLETED",
    "workerId": "uhbbfre73gqjwh"
}
```

--------------------------------

### Install Ollama and Serve Models (Bash)

Source: https://docs.runpod.io/tutorials/pods/run-ollama

Installs Ollama using a curl script and starts the Ollama server in the background. It redirects server output to an ollama.log file. This command is essential for setting up Ollama on the GPU Pod.

```bash
(curl -fsSL https://ollama.com/install.sh | sh && ollama serve > ollama.log 2>&1) &
```

--------------------------------

### Verify Runpod SDK Installation (Python)

Source: https://docs.runpod.io/tutorials/sdks/python/get-started/prerequisites

Verify that the Runpod SDK has been installed correctly by checking its version. This command executes a short Python script to import the library and print its version number.

```python
python -c "import runpod; print(runpod.__version__)"
```

--------------------------------

### Install RunPod Go SDK

Source: https://docs.runpod.io/serverless/endpoints/send-requests

Installs the RunPod Go SDK and updates Go modules. This is a prerequisite for using the Go examples.

```bash
go get github.com/runpod/go-sdk && go mod tidy
```

--------------------------------

### Python 'Hello World' Serverless Function for Runpod

Source: https://docs.runpod.io/tutorials/sdks/python/get-started/hello-world

This Python script defines a simple 'Hello, World!' serverless function for Runpod. It imports the 'runpod' library, defines a handler function to process job inputs and return a personalized greeting, and then starts the serverless worker using `runpod.serverless.start()`.

```python
import runpod


def handler(job):
    job_input = job["input"]

    return f"Hello {job_input['name']}!"


runpod.serverless.start({"handler": handler})
```

--------------------------------

### Dockerfile for Runpod Serverless Worker

Source: https://docs.runpod.io/serverless/quickstart

A Dockerfile to containerize the Python handler function for deployment. It uses a slim Python base image, installs the runpod SDK, and copies the handler file into the container.

```dockerfile
FROM python:3.10-slim

WORKDIR / 

# Install dependencies
RUN pip install --no-cache-dir runpod

# Copy your handler file
COPY handler.py / 

```

--------------------------------

### OpenAI API Compatibility - Initialize OpenAI Client

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

Provides a Python code example for initializing the OpenAI client with your Runpod API key and endpoint URL.

```APIDOC
## OpenAI API Compatibility - Initialize OpenAI Client

Before you can send API requests, set up an OpenAI client with your Runpod API key and endpoint URL:

```python
from openai import OpenAI

MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.2"  # Use your deployed model

# Replace ENDPOINT_ID and RUNPOD_API_KEY with your actual values
client = OpenAI(
    api_key="RUNPOD_API_KEY",
    base_url="https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1",
)
```
```

--------------------------------

### Example SSH Connection Command

Source: https://docs.runpod.io/pods/configuration/use-ssh

This is an example of the SSH command used to connect to a Runpod instance. It specifies the username, the Pod's public IP address, the SSH port, and the path to the local private key file for authentication.

```sh
ssh root@213.173.108.12 -p 17445 -i ~/.ssh/id_ed25519
```

--------------------------------

### Install and Run CopyParty (Background with tmux)

Source: https://docs.runpod.io/community-solutions/copyparty-file-manager/overview

Installs tmux and then runs CopyParty in a detached tmux session, allowing it to continue running even after the terminal is closed. This is useful for long-running processes on RunPod. The command first updates apt, installs tmux, creates a new session, installs and runs CopyParty within it, and then attaches to the session.

```bash
apt-get update && apt-get install tmux -y && tmux new-session -d -s copyparty 'curl -LsSf https://astral.sh/uv/install.sh | sh && source $HOME/.local/bin/env && uv tool run copyparty -p 8000 --allow-csrf' && tmux attach -t copyparty
```

--------------------------------

### Run Model Training with TMUX

Source: https://docs.runpod.io/tips-and-tricks/tmux

A practical example demonstrating a typical workflow for running long-duration model training using TMUX. It covers starting a new named session, navigating to the project directory, starting the training script, detaching from the session, and reattaching later.

```bash
# Start a new TMUX session
tmux new -s model_training

# Navigate to your project directory
cd /workspace/my_project

# Start your training script
python train.py --epochs 100 --batch-size 32

# Detach from the session with Ctrl+B, then D
# You can now safely disconnect from the Pod

# Later, reconnect to the Pod and reattach
tmux attach -t model_training
```

--------------------------------

### Install Runpod SDK using npm or yarn

Source: https://docs.runpod.io/sdks/javascript/overview

Installs the Runpod SDK package into your Node.js project. This command adds the 'runpod-sdk' to your project's dependencies, making it available for use in your JavaScript code. Ensure Node.js and npm/yarn are installed.

```bash
npm install --save runpod-sdk
# or
yarn add runpod-sdk
```

--------------------------------

### Get Runpod SDK Version (Pip)

Source: https://docs.runpod.io/sdks/python/overview

Retrieves detailed information about the installed 'runpod' package using pip, including its version. Useful for verifying installation.

```bash
pip show runpod
```

--------------------------------

### Get Runpod SDK Version (Shell)

Source: https://docs.runpod.io/sdks/python/overview

Prints the installed Runpod Python SDK version directly to the terminal using a one-liner Python command. Requires Python and the runpod package.

```bash
python3 -c "import runpod; print(runpod.__version__)"
```

--------------------------------

### Configure SSH Daemon in Custom Docker Template

Source: https://docs.runpod.io/pods/configuration/use-ssh

This bash script installs and starts the OpenSSH server within a custom Docker template. It ensures the .ssh directory is set up correctly and adds a provided public key for authentication. This is necessary if your custom template does not have SSH pre-configured.

```bash
bash -c 'apt update; 
DEBIAN_FRONTEND=noninteractive apt-get install openssh-server -y;
mkdir -p ~/.ssh;
cd ~/.ssh;
chmod 700 ~/.ssh; echo "$PUBLIC_KEY" >> authorized_keys;
chmod 700 authorized_keys;
service ssh start;
sleep infinity'
```

--------------------------------

### Test Serverless Function via Command-line

Source: https://docs.runpod.io/tutorials/sdks/python/101/local-server-testing

Demonstrates how to test the serverless function locally using command-line arguments. It shows the command to run and an example of the expected output, including job processing and results.

```bash
python your_script.py --test_input '{"input": {"text": "Hello, Runpod!"}}'
```

--------------------------------

### Sample JSON Request for Serverless Endpoint

Source: https://docs.runpod.io/serverless/quickstart

A sample JSON payload demonstrating the expected input format for a Serverless endpoint. This structure includes an 'input' object, which in this case contains a 'prompt' key with the value 'Hello World'. This is used for testing the deployed worker.

```json
{
    "input": {
        "prompt": "Hello World"
    }
}
```

--------------------------------

### Install OhMyRunPod using pip

Source: https://docs.runpod.io/community-solutions/ohmyrunpod/overview

This command installs the OhMyRunPod Python package using pip. Ensure you have Python 3.6 or higher installed. This package facilitates SFTP setup and file transfers on Runpod.

```bash
pip install OhMyRunPod
```

--------------------------------

### Test Input JSON for Handler

Source: https://docs.runpod.io/serverless/quickstart

A sample JSON file used for locally testing the handler function. It provides the expected input structure, including a nested 'input' object with a 'prompt' field.

```json
{
    "input": {
        "prompt": "Hey there!"
    }
}
```

--------------------------------

### Example JSON Output for GPU Availability

Source: https://docs.runpod.io/sdks/graphql/manage-pods

These JSON examples illustrate the expected output when querying GPU type availability. The first example shows a 'High' stock status, while the second demonstrates a 'Low' stock status, highlighting differences in pricing and available counts.

```json
{
  "data": {
    "gpuTypes": [
      {
        "lowestPrice": {
          "minimumBidPrice": 0.2,
          "uninterruptablePrice": 0.35,
          "minVcpu": 2,
          "minMemory": 8,
          "stockStatus": "High",
          "compliance": null,
          "maxUnreservedGpuCount": 5,
          "availableGpuCounts": [1, 2, 4],
          "__typename": "GpuTypeLowestPrice"
        },
        "id": "NVIDIA RTX A4000",
        "displayName": "RTX A4000",
        "memoryInGb": 16,
        "securePrice": 0.35,
        "communityPrice": 0.2,
        "oneMonthPrice": 200,
        "oneWeekPrice": 60,
        "threeMonthPrice": 500,
        "sixMonthPrice": 900,
        "secureSpotPrice": 0.18,
        "__typename": "GpuType"
      }
    ]
  }
}
```

```json
{
  "data": {
    "gpuTypes": [
      {
        "lowestPrice": {
          "minimumBidPrice": 0.16,
          "uninterruptablePrice": 0.24,
          "minVcpu": 5,
          "minMemory": 31,
          "stockStatus": "Low",
          "compliance": null,
          "maxUnreservedGpuCount": 7,
          "availableGpuCounts": [1,2,3,4,5,6,7],
          "__typename": "LowestPrice"
        },
        "id": "NVIDIA RTX A4000",
        "displayName": "RTX A4000",
        "memoryInGb": 16,
        "securePrice": 0.24,
        "communityPrice": 0.17,
        "oneMonthPrice": null,
        "oneWeekPrice": null,
        "threeMonthPrice": 0.204,
        "sixMonthPrice": 0.192,
        "secureSpotPrice": 0.16,
        "__typename": "GpuType"
      }
    ]
  }
}
```

--------------------------------

### Activate Python Virtual Environment

Source: https://docs.runpod.io/serverless/quickstart

Activates the previously created Python virtual environment. This command differs based on the operating system.

```sh
source venv/bin/activate
```

```sh
venv\Scripts\activate
```

--------------------------------

### Clone Slurm Demo Files and Install Slurm Packages (Bash)

Source: https://docs.runpod.io/instant-clusters/slurm

Clones the Slurm demo repository and installs necessary Slurm packages (slurm-wlm, slurm-client, munge) on each Pod. This is a prerequisite for setting up the Slurm cluster.

```bash
git clone https://github.com/pandyamarut/slurm_example.git && cd slurm_example
```

```bash
apt update && apt install -y slurm-wlm slurm-client munge
```

--------------------------------

### Complete Python Serverless Function Example

Source: https://docs.runpod.io/tutorials/sdks/python/101/hello

Provides the complete Python code for a basic serverless function that checks if a number is even. This includes importing the library, defining the handler function, and starting the serverless execution.

```python
import runpod


def is_even(job):
    job_input = job["input"]
    the_number = job_input["number"]

    if not isinstance(the_number, int):
        return {"error": "Please provide an integer."}

    return the_number % 2 == 0


runpod.serverless.start({"handler": is_even})
```

--------------------------------

### Start Spot Pod with cURL and GraphQL

Source: https://docs.runpod.io/sdks/graphql/manage-pods

Starts a spot pod instance on RunPod using the API. This method involves bidding for resources, specifying the pod ID, bid per GPU, and GPU count. The request is made via cURL with a GraphQL mutation, and the response is a JSON object detailing the started spot pod.

```bash
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { podBidResume( input: { podId: \"d62t7qg9n5vtan\", bidPerGpu: 0.2, gpuCount: 1 } ) { id desiredStatus imageName env machineId machine { podHostId } } }"}'
```

```graphql
mutation {
  podBidResume(input: {podId: "d62t7qg9n5vtan", bidPerGpu: 0.2, gpuCount: 1}) {
    id
    desiredStatus
    imageName
    env
    machineId
    machine {
      podHostId
    }
  }
}
```

--------------------------------

### Start Local Test Server for Serverless Function

Source: https://docs.runpod.io/tutorials/sdks/python/101/local-server-testing

Provides the command to launch a local test server for the Runpod serverless function. This server allows for more comprehensive testing by simulating HTTP requests to the function endpoint.

```bash
python your_script.py --rp_serve_api
```

--------------------------------

### Example Docker Image Configuration

Source: https://docs.runpod.io/sdks/graphql/configurations

Shows how to specify the Docker image to be used for the container. The `imageName` argument requires the repository name and tag. For example, `"nginx:latest"`.

```json
{
  "imageName": "nginx:latest"
}
```

--------------------------------

### Set up Environment with Runpod and Asyncio

Source: https://docs.runpod.io/serverless/workers/concurrent-handler

Installs the necessary Python packages, 'runpod' and 'asyncio', within a virtual environment for building concurrent handlers. This is a prerequisite for the subsequent steps.

```shell
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install runpod asyncio
```

--------------------------------

### Dockerfile for vLLM Load Balancer Container

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

Defines the build steps for creating a Docker image to host the vLLM load balancer application. It starts from an NVIDIA CUDA base image, installs Python and pip, sets up the Python environment, installs vLLM and FlashInfer with specific versions for stability and performance, copies the application source code, and sets the entry point.

```dockerfile
FROM nvidia/cuda:12.1.0-base-ubuntu22.04 

RUN apt-get update -y \
    && apt-get install -y python3-pip

RUN ldconfig /usr/local/cuda-12.1/compat/

# Install Python dependencies
RUN --mount=type=cache,target=/root/.cache/pip \
    python3 -m pip install --upgrade pip && \
    python3 -m pip install --upgrade -r /requirements.txt

# Pin vLLM version for stability - 0.9.1 is latest stable as of 2024-07
# FlashInfer provides optimized attention for better performance
ARG VLLM_VERSION=0.9.1
ARG CUDA_VERSION=cu121
ARG TORCH_VERSION=torch2.3

RUN python3 -m pip install vllm==${VLLM_VERSION} && \
    python3 -m pip install flashinfer -i https://flashinfer.ai/whl/${CUDA_VERSION}/${TORCH_VERSION}

ENV PYTHONPATH="/:/vllm-workspace"

COPY src /src

WORKDIR /src

CMD ["python3", "handler.py"]
```

--------------------------------

### Navigate to Tutorial Directory (Bash)

Source: https://docs.runpod.io/integrations/dstack

Changes the current directory to the 'runpod-dstack-tutorial' folder. This is a prerequisite for subsequent commands.

```bash
cd runpod-dstack-tutorial
```

--------------------------------

### Push Docker Image to Container Registry

Source: https://docs.runpod.io/serverless/quickstart

Pushes the locally built Docker image to a container registry, making it accessible for deployment on RunPod Serverless. Ensure you have authenticated with your container registry before running this command. The ':latest' tag indicates the most recent version of the image.

```sh
docker push [YOUR_USERNAME]/serverless-test:latest
```

--------------------------------

### Start Spot Pod

Source: https://docs.runpod.io/sdks/graphql/manage-pods

This endpoint allows you to start a spot pod by bidding for resources. You need to provide the pod ID, bid per GPU, and GPU count.

```APIDOC
## POST /graphql

### Description
Starts a spot pod by bidding for resources. You specify the pod ID, your bid per GPU, and the number of GPUs required.

### Method
POST

### Endpoint
`https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}`

### Parameters
#### Query Parameters
- **api_key** (string) - Required - Your RunPod API key.

#### Request Body
- **query** (string) - Required - The GraphQL mutation to bid for and resume a spot pod.
  - **input** (object) - Required - Input for the `podBidResume` mutation.
    - **podId** (string) - Required - The ID of the pod to bid for.
    - **bidPerGpu** (number) - Required - Your bid price per GPU.
    - **gpuCount** (integer) - Required - The number of GPUs to bid for.

### Request Example
```json
{
  "query": "mutation { podBidResume( input: { podId: \"d62t7qg9n5vtan\", bidPerGpu: 0.2, gpuCount: 1 } ) { id desiredStatus imageName env machineId machine { podHostId } } }"
}
```

### Response
#### Success Response (200)
- **data.podBidResume.id** (string) - The ID of the bid pod.
- **data.podBidResume.desiredStatus** (string) - The desired status of the pod (e.g., "RUNNING").
- **data.podBidResume.imageName** (string) - The name of the Docker image used for the pod.
- **data.podBidResume.env** (array) - An array of environment variables set for the pod.
- **data.podBidResume.machineId** (string) - The ID of the machine the pod is running on.
- **data.podBidResume.machine.podHostId** (string) - The host ID of the machine.

#### Response Example
```json
{
  "data": {
    "podBidResume": {
      "id": "d62t7qg9n5vtan",
      "desiredStatus": "RUNNING",
      "imageName": "runpod/tensorflow",
      "env": [
        "JUPYTER_PASSWORD=vunw9ybnzqwpia2795p2"
      ],
      "machineId": "hpvdausak8xb",
      "machine": {
        "podHostId": "d62t7qg9n5vtan-64410065"
      }
    }
  }
}
```
```

--------------------------------

### Example Training Output

Source: https://docs.runpod.io/instant-clusters/axolotl

Illustrates the expected output during and after the LLM training process using Axolotl. This includes loss, learning rate, epoch progression, and final training statistics, indicating successful completion.

```csharp
...
{'loss': 1.2569, 'grad_norm': 0.11112671345472336, 'learning_rate': 5.418275829936537e-06, 'epoch': 0.9}
{'loss': 1.2091, 'grad_norm': 0.11100614815950394, 'learning_rate': 3.7731999690749585e-06, 'epoch': 0.92}
{'loss': 1.2216, 'grad_norm': 0.10450132936239243, 'learning_rate': 2.420361737256438e-06, 'epoch': 0.93}
{'loss': 1.223, 'grad_norm': 0.10873789340257645, 'learning_rate': 1.3638696597277679e-06, 'epoch': 0.95}
{'loss': 1.2529, 'grad_norm': 0.1063728854060173, 'learning_rate': 6.069322682050516e-07, 'epoch': 0.96}
{'loss': 1.2304, 'grad_norm': 0.10996092110872269, 'learning_rate': 1.518483566683826e-07, 'epoch': 0.98}
{'loss': 1.2334, 'grad_norm': 0.10642101615667343, 'learning_rate': 0.0, 'epoch': 0.99}
{'train_runtime': 61.7602, 'train_samples_per_second': 795.189, 'train_steps_per_second': 1.085, 'train_loss': 1.255359119443751, 'epoch': 0.99}

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [01:00<00:00,  1.11it/s]
[2025-04-01 19:24:22,603] [INFO] [axolotl.train.save_trained_model:211] [PID:1009] [RANK:0] Training completed! Saving pre-trained model to ./outputs/lora-out.
```

--------------------------------

### GitBuildState Enum Value Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a value for the GitBuildState enum, representing the current status of a build. 'PENDING' indicates that the build has not yet started.

```plaintext
"PENDING"
```

--------------------------------

### Get Specific Runpod CLI Command Help

Source: https://docs.runpod.io/runpodctl/overview

Retrieves detailed help information for a specific Runpod CLI command. Replace `[command]` with the actual command you need help with.

```shell
runpodctl [command] help
```

--------------------------------

### Example Python Dependencies for requirements.txt

Source: https://docs.runpod.io/serverless/workers/create-dockerfile

List your Python package dependencies in the 'requirements.txt' file. Ensure compatibility with your handler function.

```txt
# Example requirements.txt
runpod~=1.7.6
torch==2.0.1
pillow==9.5.0
transformers==4.30.2
```

--------------------------------

### Navigate to Project Directory (Bash)

Source: https://docs.runpod.io/tutorials/sdks/python/get-started/prerequisites

Change the current directory to your project's root folder. This is a common command-line operation required before executing other project-specific commands.

```bash
cd path/to/your/project
```

--------------------------------

### Runpod Serverless Handler Function

Source: https://docs.runpod.io/serverless/quickstart

A Python handler function for Runpod Serverless endpoints. It processes incoming JSON events, extracts a 'prompt' and an optional 'seconds' value, simulates work by sleeping, and returns the prompt. It uses the runpod SDK to start the serverless function.

```python
import runpod
import time  

def handler(event):
#   This function processes incoming requests to your Serverless endpoint.
#
#    Args:
#        event (dict): Contains the input data and request metadata
#       
#    Returns:
#       Any: The result to be returned to the client
    
    # Extract input data
    print(f"Worker Start")
    input = event['input']
    
    prompt = input.get('prompt')  
    seconds = input.get('seconds', 0)  

    print(f"Received prompt: {prompt}")
    print(f"Sleeping for {seconds} seconds...")
    
    # You can replace this sleep call with your own Python code
    time.sleep(seconds)  
    
    return prompt 

# Start the Serverless function when the script is run
if __name__ == '__main__':
    runpod.serverless.start({'handler': handler })
```

--------------------------------

### Install Dependencies (Bash)

Source: https://docs.runpod.io/tutorials/pods/build-docker-images

Installs necessary dependencies within the Runpod container, including updating package lists, installing sudo, and setting up Docker using the official convenience script. It also includes instructions for logging into Docker Hub.

```bash
apt update && apt install -y sudo
curl -fsSL https://get.docker.com -o get-docker.sh && sudo sh get-docker.sh
docker login -u <your-username>
```

```bash
wget https://github.com/bazelbuild/bazelisk/releases/download/v1.20.0/bazelisk-linux-amd64
chmod +x bazelisk-linux-amd64
sudo cp ./bazelisk-linux-amd64 /usr/local/bin/bazel
```

--------------------------------

### AWS S3API CLI Example

Source: https://docs.runpod.io/serverless/storage/s3-api

An example of using the AWS s3api CLI to download an object from a Runpod network volume.

```APIDOC
## AWS S3API CLI Example

### Description
Demonstrates using the `aws s3api get-object` command to download an object from a Runpod network volume.

### Command Example
```bash
aws s3api get-object --bucket NETWORK_VOLUME_ID \
    --key REMOTE_FILE \
    --region DATACENTER \
    --endpoint-url https://s3api-DATACENTER.runpod.io/ \
    LOCAL_FILE
```

### Parameters
- `--bucket` (string): Your `NETWORK_VOLUME_ID`.
- `--key` (string): The `REMOTE_FILE` path on the network volume.
- `--region` (string): The `DATACENTER` identifier.
- `--endpoint-url` (string): The S3 API endpoint URL for the datacenter.
- `LOCAL_FILE`: The desired local path and filename for the downloaded file (e.g., `~/local-dir/my-file.txt`).

### Further Information
For a comprehensive list of `s3api` commands, refer to the [AWS s3api reference](https://docs.aws.amazon.com/cli/latest/reference/s3api/).
```

--------------------------------

### Create Dockerfile for FastAPI Application

Source: https://docs.runpod.io/serverless/load-balancing/build-a-worker

This Dockerfile sets up the environment for the FastAPI application. It starts from an NVIDIA CUDA base image, installs Python and pip, copies the requirements file to install dependencies, and then copies the application code. Finally, it defines the command to run the FastAPI application using uvicorn.

```dockerfile
FROM nvidia/cuda:12.1.0-base-ubuntu22.04 

RUN apt-get update -y \
    && apt-get install -y python3-pip

RUN ldconfig /usr/local/cuda-12.1/compat/

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY app.py .

# Start the handler
CMD ["python3", "app.py"]
```

--------------------------------

### Get Website Details

Source: https://docs.runpod.io/api-reference/pods/GET/pods/podId

Retrieves detailed information about a specific RunPod website, including its ID, pod ID, and start time.

```APIDOC
## GET /websites/runpod_io

### Description
Retrieves detailed information about a specific RunPod website.

### Method
GET

### Endpoint
/websites/runpod_io

### Parameters
#### Query Parameters
- **example** (string) - Optional - An example identifier for the website.
- **podId** (string) - Optional - The ID of the associated pod.
- **startTime** (string) - Optional - The start time of the website's operation in ISO 8601 format.

### Request Example
```
GET /websites/runpod_io?example=clkrb4qci0000mb09c7sualzo&podId=xedezhzb9la3ye&startTime=2024-05-12T19:14:40.144Z
```

### Response
#### Success Response (200)
- **example** (string) - An example identifier for the website.
- **podId** (string) - The ID of the associated pod.
- **startTime** (string) - The start time of the website's operation in ISO 8601 format.

#### Response Example
```json
{
  "example": "clkrb4qci0000mb09c7sualzo",
  "podId": "xedezhzb9la3ye",
  "startTime": "2024-05-12T19:14:40.144Z"
}
```
```

--------------------------------

### Example Environment Variables Configuration

Source: https://docs.runpod.io/sdks/graphql/configurations

Demonstrates how to configure environment variables for a container. This is useful for setting application-specific configurations or credentials. The `env` argument accepts a dictionary or object.

```json
{
  "env": {
    "DATABASE_URL": "postgres://user:password@localhost/dbname"
  }
}
```

--------------------------------

### Build Docker Image for RunPod Deployment

Source: https://docs.runpod.io/serverless/quickstart

Builds a Docker image specifically for RunPod deployment, ensuring compatibility with the 'linux/amd64' platform. Replace '[YOUR_USERNAME]' with your Docker Hub username. This command is essential for preparing your worker for the container registry.

```sh
docker build --platform linux/amd64 --tag [YOUR_USERNAME]/serverless-test .
```

--------------------------------

### Install and Run CopyParty (Standard Terminal)

Source: https://docs.runpod.io/community-solutions/copyparty-file-manager/overview

Installs CopyParty using a curl script and runs it on a specified port. This method keeps CopyParty active only as long as the terminal session is open. Ensure the chosen port is not already in use.

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh && source $HOME/.local/bin/env && uv tool run copyparty -p 8000 --allow-csrf
```

--------------------------------

### Create Python Virtual Environment

Source: https://docs.runpod.io/serverless/quickstart

Creates a Python virtual environment named 'venv' to isolate project dependencies. This is a standard practice for Python projects to manage packages effectively.

```sh
# Create a Python virtual environment
python3 -m venv venv
```

--------------------------------

### Basic SSH Connection Command (Shell)

Source: https://docs.runpod.io/pods/configuration/use-ssh

This is an example of the SSH command you would run in your local terminal to connect to a Runpod Pod using key authentication. It specifies the user, the SSH server address, and the path to your private key.

```sh
ssh 8y5rumuyb50m78-6441103b@ssh.runpod.io -i ~/.ssh/id_ed25519
```

--------------------------------

### Install Runpod CLI on Linux via direct download

Source: https://docs.runpod.io/runpodctl/overview

Installs the Runpod CLI on Linux systems by downloading the binary directly. This command downloads the executable, makes it executable, and copies it to the system's PATH.

```shell
wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd64 -O runpodctl && chmod +x runpodctl && sudo cp runpodctl /usr/bin/runpodctl
```

--------------------------------

### Start Runpod Agent (Bash)

Source: https://docs.runpod.io/hosting/burn-testing

This command restarts the Runpod agent service using systemctl after burn tests are completed. It is essential for the agent to be running for normal operation.

```bash
sudo systemctl start runpod
```

--------------------------------

### Start Script - Bash

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

This Bash script acts as the entrypoint for the Docker container, managing different operational modes based on the `MODE_TO_RUN` environment variable. It sets the workspace directory, and includes functions to start Nginx and execute scripts.

```bash
#!/bin/bash
set -e  # Exit the script if any statement returns a non-true return value

# Set workspace directory from env or default
WORKSPACE_DIR="${WORKSPACE_DIR:-/workspace}"

# Start nginx service
start_nginx() {
    echo "Starting Nginx service..."
    service nginx start
}

# Execute script if exists
execute_script() {
    local script_path=$1
    local script_msg=$2
    if [[ -f ${script_path} ]]; then
        echo "${script_msg}"
        bash ${script_path}
    fi
}
```

--------------------------------

### System Update and Utility Installation (Bash)

Source: https://docs.runpod.io/tutorials/pods/run-ollama

Updates the package list and installs the 'lshw' utility. 'lshw' (list hardware) can be useful for inspecting hardware details, potentially aiding in GPU detection for Ollama.

```bash
apt update
apt install lshw
```

--------------------------------

### Setup SSH Access in Runpod

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

Configures SSH access by creating the .ssh directory, adding the public key to authorized_keys, setting permissions, generating host keys, and starting the SSH service. It requires the PUBLIC_KEY environment variable to be set.

```bash
setup_ssh() {
    if [[ $PUBLIC_KEY ]]; then
        echo "Setting up SSH..."
        mkdir -p ~/.ssh
        echo "$PUBLIC_KEY" >> ~/.ssh/authorized_keys
        chmod 700 -R ~/.ssh
        # Generate SSH host keys if not present
        generate_ssh_keys
        service ssh start
        echo "SSH host keys:"
        cat /etc/ssh/*.pub
    fi
}

# Generate SSH host keys
generate_ssh_keys() {
    ssh-keygen -A
}

```

--------------------------------

### Get Help for Docker Commands (CLI)

Source: https://docs.runpod.io/tutorials/introduction/containers

The --help flag provides documentation for any Docker command. This is useful for understanding command options and usage without leaving the terminal.

```bash
docker --help
```

--------------------------------

### Verify Runpod CLI Installation

Source: https://docs.runpod.io/runpodctl/overview

Checks if the Runpod CLI has been installed correctly and displays the installed version. This command is used to confirm a successful installation.

```shell
runpodctl version
```

--------------------------------

### Run GPU Burn Test with Docker (Bash)

Source: https://docs.runpod.io/hosting/burn-testing

This command initiates a GPU burn test using a Docker container. It utilizes all available GPUs and runs for a specified duration (172800 seconds in this example).

```bash
docker run --gpus all --rm jorghi21/gpu-burn-test 172800
```

--------------------------------

### Dockerfile Structure for RunPod Worker

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

This Dockerfile sets up a RunPod worker environment. It starts from a base PyTorch image, configures the workspace, installs dependencies, copies application files, and defines the entrypoint script. The WORKSPACE_DIR can be set at build time.

```dockerfile
FROM runpod/pytorch:2.0.1-py3.10-cuda11.8.0-devel-ubuntu22.04
ARG WORKSPACE_DIR=/workspace
ENV WORKSPACE_DIR=${WORKSPACE_DIR}
WORKDIR $WORKSPACE_DIR
COPY requirements.txt ./requirements.txt
RUN pip install \
    # Add your dependencies here
    # Example:
    # torch \
    # torchvision \
    # torchaudio \
    # transformers \
    # accelerate
COPY . .
ENV MODE_TO_RUN="pod"
CMD ["$WORKSPACE_DIR/start.sh"]
```

--------------------------------

### Start a Pod with runpodctl

Source: https://docs.runpod.io/runpodctl/reference/runpodctl-start-pod

This command starts a stopped Pod, resuming compute and billing. It requires the Pod ID as an argument and optionally accepts a bid price for spot instances on Community Cloud Pods.

```sh
runpodctl start pod <podId> [flags]
runpodctl start pod abc123xyz456 --bid 0.50
```

--------------------------------

### Stream Job Results with JavaScript SDK

Source: https://docs.runpod.io/serverless/endpoints/operations

This JavaScript example utilizes the RunPod SDK to start a job and then stream its output. It uses `async/await` and `for await...of` for handling asynchronous streaming. Ensure RUNPOD_API_KEY and ENDPOINT_ID are configured in your environment variables.

```javascript
const { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;
import runpodSdk from "runpod-sdk";

async function main() {
  const runpod = runpodSdk(RUNPOD_API_KEY);
  const endpoint = runpod.endpoint(ENDPOINT_ID);
  const result = await endpoint.run({
    input: {
      prompt: "Hello, World!",
    },
  });

  console.log(result);

  const { id } = result;
  for await (const result of endpoint.stream(id)) {
    console.log(`${JSON.stringify(result, null, 2)}`);
  }
  console.log("done streaming");
}

main();

```

--------------------------------

### Start On-Demand Pod

Source: https://docs.runpod.io/sdks/graphql/manage-pods

This endpoint allows you to start an on-demand pod. You need to provide the pod ID and the desired GPU count.

```APIDOC
## POST /graphql

### Description
Starts an on-demand pod with the specified pod ID and GPU count.

### Method
POST

### Endpoint
`https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}`

### Parameters
#### Query Parameters
- **api_key** (string) - Required - Your RunPod API key.

#### Request Body
- **query** (string) - Required - The GraphQL mutation to resume a pod.
  - **input** (object) - Required - Input for the `podResume` mutation.
    - **podId** (string) - Required - The ID of the pod to resume.
    - **gpuCount** (integer) - Required - The number of GPUs to allocate to the pod.

### Request Example
```json
{
  "query": "mutation { podResume( input: { podId: \"inzk6tzuz833h5\", gpuCount: 1 } ) { id desiredStatus imageName env machineId machine { podHostId } } }"
}
```

### Response
#### Success Response (200)
- **data.podResume.id** (string) - The ID of the resumed pod.
- **data.podResume.desiredStatus** (string) - The desired status of the pod (e.g., "RUNNING").
- **data.podResume.imageName** (string) - The name of the Docker image used for the pod.
- **data.podResume.env** (array) - An array of environment variables set for the pod.
- **data.podResume.machineId** (string) - The ID of the machine the pod is running on.
- **data.podResume.machine.podHostId** (string) - The host ID of the machine.

#### Response Example
```json
{
  "data": {
    "podResume": {
      "id": "inzk6tzuz833h5",
      "desiredStatus": "RUNNING",
      "imageName": "runpod/tensorflow",
      "env": [
        "JUPYTER_PASSWORD=ywm4c9r15j1x6gfrds5n"
      ],
      "machineId": "hpvdausak8xb",
      "machine": {
        "podHostId": "inzk6tzuz833h5-64410065"
      }
    }
  }
}
```
```

--------------------------------

### AWS S3 CLI Examples

Source: https://docs.runpod.io/serverless/storage/s3-api

Examples of using the AWS s3 CLI to interact with Runpod network volumes, including listing objects, transferring files, removing files, and syncing directories.

```APIDOC
## AWS S3 CLI Examples

### Description
Examples demonstrating how to use the `aws s3` CLI commands with the Runpod S3-compatible API. Remember to specify the `--endpoint-url` and `--region` flags.

### General Usage Notes
- Use `--endpoint-url https://s3api-DATACENTER.runpod.io/`
- Use `--region DATACENTER`
- Object names correspond to file paths.
- Special characters in object names may require URL encoding.
- `ls` and `ListObjects` operations will list empty directories.
- `ls` operations can be slow on large directories.

### List Objects
Lists objects in a network volume directory.
```bash
aws s3 ls --region DATACENTER \
    --endpoint-url https://s3api-DATACENTER.runpod.io/ \
    s3://NETWORK_VOLUME_ID/REMOTE_DIR
```

### Transfer Files (Upload)
Copies a local file to a network volume.
```bash
aws s3 cp --region DATACENTER \
    --endpoint-url https://s3api-DATACENTER.runpod.io/ \
    LOCAL_FILE \
    s3://NETWORK_VOLUME_ID
```

### Transfer Files (Download)
Copies a file from a network volume to a local directory.
```bash
aws s3 cp --region DATACENTER \
    --endpoint-url https://s3api-DATACENTER.runpod.io/ \
    s3://NETWORK_VOLUME_ID/remote-file.txt LOCAL_DIR
```

### Remove Files
Removes a file from a network volume.
```bash
aws s3 rm --region DATACENTER \
    --endpoint-url https://s3api-DATACENTER.runpod.io/ \
    s3://NETWORK_VOLUME_ID/remote-file.txt
```

### Error Handling (502 Bad Gateway)
If you encounter a 502 error, consider increasing retry attempts:
```bash
export AWS_RETRY_MODE=standard
export AWS_MAX_ATTEMPTS=10
```

### Sync Directories
Syncs a local directory to a network volume directory.
```bash
aws s3 sync --region DATACENTER \
    --endpoint-url https://s3api-DATACENTER.runpod.io/ \
    LOCAL_DIR \
    s3://NETWORK_VOLUME_ID/REMOTE_DIR
```
```

--------------------------------

### Start or Resume a Pod

Source: https://docs.runpod.io/api-reference/pods/POST/pods/podId/start

This endpoint allows you to start or resume a specific Pod using its unique ID.

```APIDOC
## POST /pods/{podId}/start

### Description
Start or resume a Pod.

### Method
POST

### Endpoint
/pods/{podId}/start

### Parameters
#### Path Parameters
- **podId** (string) - Required - Pod ID to start.

### Request Example
```json
{
  "example": "Not applicable for this endpoint, as it uses path parameters and no request body."
}
```

### Response
#### Success Response (200)
- **message** (string) - Pod successfully started.

#### Error Response (400)
- **message** (string) - Invalid Pod ID.

#### Error Response (401)
- **message** (string) - Unauthorized.

#### Response Example
```json
{
  "message": "Pod successfully started."
}
```
```

--------------------------------

### Install Runpod SDKs

Source: https://docs.runpod.io/serverless/endpoints/operations

Installs the Runpod SDK for Python, JavaScript, and Go. These SDKs are necessary for interacting with Runpod endpoints programmatically.

```bash
# Python
python -m pip install runpod

# JavaScript
npm install --save runpod-sdk

# Go
go get github.com/runpod/go-sdk && go mod tidy

```

--------------------------------

### Example Output for Sentiment Analysis Test (Bash)

Source: https://docs.runpod.io/tutorials/sdks/python/101/aggregate

This bash output illustrates the expected console log when running the sentiment analysis test locally. It shows the serverless worker starting, processing the job, generating results for each input item, and aggregating these results into a final output list.

```bash
---	Starting Serverless Worker |	 Version 1.6.2 ---
INFO	|	test_input set, using test_input as job input.
DEBUG	|	Retrieved local job: {'input': {'task_type': 'sentiment', 'items': ['I love this product!', 'The service was terrible.', 'It was okay, nothing special.']}, 'id': 'local_test'}
INFO	|	local_test |	Started.
DEBUG	|	local_test |	Handler output: ['Sentiment: Positive, Score: 0.85', 'Sentiment: Negative, Score: -0.72', 'Sentiment: Neutral, Score: 0.12']
DEBUG	|	local_test |	run_job return: {'output': ['Sentiment: Positive, Score: 0.85', 'Sentiment: Negative, Score: -0.72', 'Sentiment: Neutral, Score: 0.12']}
INFO	|	Job local_test completed successfully.
INFO	|	Job result: {'output': ['Sentiment: Positive, Score: 0.85', 'Sentiment: Negative, Score: -0.72', 'Sentiment: Neutral, Score: 0.12']}
INFO	|	Local testing complete, exiting.

```

--------------------------------

### SignUpMethod Enum Value Example

Source: https://docs.runpod.io/references/graphql-spec

An example of a value from the SignUpMethod enumeration, indicating how a user signed up, such as 'GOOGLE'.

```json
"GOOGLE"
```

--------------------------------

### Get Runpod SDK Version (Python Script)

Source: https://docs.runpod.io/sdks/python/overview

Retrieves and prints the Runpod Python SDK version from within a Python script. This method uses the SDK's built-in version retrieval function.

```python
import runpod

version = runpod.version.get_version()

print(f"Runpod version number: {version}")
```

--------------------------------

### DataCenter Object Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows the DataCenter object, which contains information about a data center, including its ID, name, location, storage details, and GPU availability. It also includes arguments for filtering GPU availability.

```json
{
  "id": "xyz789",
  "name": "abc123",
  "location": "abc123",
  "storage": DataCenterStorage,
  "s3apiEnabled": true,
  "globalNetwork": true,
  "storageClusters": [StorageClusterInfo],
  "storageSupport": true,
  "listed": true,
  "gpuAvailability": [GpuAvailability],
  "compliance": ["GDPR"],
  "region": "NORTH_AMERICA",
  "pools": [Pool]
}
```

--------------------------------

### Local Handler Test Execution

Source: https://docs.runpod.io/serverless/quickstart

Command to execute the Python handler script locally for testing purposes. This command runs the handler and simulates a request using the default or specified input.

```sh
python handler.py
```

--------------------------------

### Submit Asynchronous Job Request Example

Source: https://docs.runpod.io/serverless/endpoints/send-requests

This example shows how to submit an asynchronous job request to a RunPod queue-based endpoint using Python. It sends a POST request to the `/run` endpoint, and the response contains a job ID. To retrieve the results, a subsequent GET request to the `/status` endpoint with the job ID is required. This is suitable for long-running tasks.

```python
import requests
import time

endpoint_url = "YOUR_ENDPOINT_URL"
api_key = "YOUR_API_KEY"

headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {api_key}"
}

payload = {
    "input": {
        "prompt": "Generate an image of a cat wearing a hat."
    }
}

try:
    # Submit the asynchronous job
    response = requests.post(f"{endpoint_url}/run", headers=headers, json=payload)
    response.raise_for_status()
    job_info = response.json()
    job_id = job_info.get("id")
    print(f"Asynchronous job submitted with ID: {job_id}")

    # Poll for results (example: check every 10 seconds)
    while True:
        status_response = requests.get(f"{endpoint_url}/status/{job_id}", headers=headers)
        status_response.raise_for_status()
        status_data = status_response.json()
        
        if status_data["status"] == "COMPLETED":
            print("Asynchronous job completed:", status_data["result"])
            break
        elif status_data["status"] == "FAILED":
            print("Asynchronous job failed:", status_data)
            break
        else:
            print(f"Job status: {status_data['status']}. Waiting...")
            time.sleep(10)

except requests.exceptions.RequestException as e:
    print(f"Error sending request: {e}")

```

--------------------------------

### Create Python Virtual Environment (Bash)

Source: https://docs.runpod.io/tutorials/sdks/python/get-started/prerequisites

Create an isolated Python environment named 'venv' for your project. This command uses Python's built-in 'venv' module to manage project dependencies separately.

```bash
python -m venv venv
```

--------------------------------

### Configure Destination Pod and Prepare rsync Command

Source: https://docs.runpod.io/storage/network-volumes

Installs rsync, retrieves the destination Pod's public IP and SSH port, and adds the source Pod's public SSH key to the authorized_keys file. It also displays the rsync command needed for the source Pod.

```bash
apt update && apt install -y vim rsync && \
ip=$(printenv RUNPOD_PUBLIC_IP) && \
port=$(printenv RUNPOD_TCP_PORT_22) && \
echo "rsync -avzP --inplace -e \"ssh -p $port\" /workspace/ root@$ip:/workspace" && \
vi ~/.ssh/authorized_keys
```

--------------------------------

### Install Axolotl Dependencies

Source: https://docs.runpod.io/instant-clusters/axolotl

Installs the necessary Python packages for Axolotl, including build tools and specific features like flash-attn and deepspeed for optimized training. Ensure you are in the 'axolotl' directory before running these commands.

```shell
pip3 install -U packaging setuptools wheel ninja
pip3 install --no-build-isolation -e '.[flash-attn,deepspeed]'
```

--------------------------------

### Create Project Directory and Files (Bash)

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

Initializes the project structure by creating a directory and essential files like Dockerfile, requirements.txt, and Python source files for the vLLM worker.

```bash
mkdir vllm_worker
cd vllm_worker
touch Dockerfile
touch requirements.txt
mkdir src
touch src/handler.py
touch src/models.py
touch src/utils.py
```

--------------------------------

### Start Slurm Services (Bash)

Source: https://docs.runpod.io/instant-clusters/slurm

Starts the Slurm control daemon (slurmctld) on the primary node and the Slurm daemon (slurmd) on both primary and secondary nodes. The '-D' flag keeps the services running in the foreground, requiring separate terminals for each command.

```bash
slurmctld -D
```

```bash
slurmd -D
```

--------------------------------

### MachineMaintenance Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object details a scheduled maintenance for a machine. It includes the maintenance ID, the associated machine ID, start and end times for the maintenance, and any relevant notes. This is important for operational planning.

```json
{
  "id": "xyz789",
  "machineId": "xyz789",
  "maintenanceStart": "2007-12-03T10:15:30Z",
  "maintenanceEnd": "2007-12-03T10:15:30Z",
  "maintenanceNote": "abc123"
}
```

--------------------------------

### GpuTelemetry JSON Example

Source: https://docs.runpod.io/references/graphql-spec

This example demonstrates the GpuTelemetry object, which provides real-time performance metrics for a GPU. It includes utilization percentages, temperature, memory usage, and power consumption.

```json
{
  "id": "abc123",
  "percentUtilization": 987.65,
  "temperatureCelcius": 987.65,
  "memoryUtilization": 987.65,
  "powerWatts": 987.65
}
```

--------------------------------

### Project Setup - Shell

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

This shell script creates the directory structure and essential files needed for a dual-mode Runpod worker project. It sets up the project directory, navigates into it, and creates the handler, startup script, Dockerfile, and requirements file.

```shell
mkdir dual-mode-worker
cd dual-mode-worker
touch handler.py start.sh Dockerfile requirements.txt
```

--------------------------------

### Create Dockerfile for Custom Image

Source: https://docs.runpod.io/tutorials/introduction/containers/create-dockerfiles

This Dockerfile defines a custom Docker image. It starts from the `busybox` image, copies a local `entrypoint.sh` script into the image, makes the script executable, and sets it as the default command to run when a container starts from this image.

```dockerfile
FROM busybox
COPY entrypoint.sh /
RUN chmod +x /entrypoint.sh
ENTRYPOINT ["/entrypoint.sh"]
```

--------------------------------

### Expected Local Test Output for Sentiment Analysis

Source: https://docs.runpod.io/tutorials/sdks/python/102/huggingface-models

This is an example of the output you should expect when running the sentiment analysis function locally. It shows the worker starting, processing the input, and returning the sentiment analysis result, including the predicted sentiment and its confidence score.

```bash
--- Starting Serverless Worker |  Version 1.6.2 ---
INFO   | Using test_input.json as job input.
DEBUG  | Retrieved local job: {"input": {"text": "I love using Runpod for serverless machine learning!"}, "id": "local_test"}
INFO   | local_test | Started.
model.safetensors: 100%|█████████████████████████| 268M/268M [00:02<00:00, 94.9MB/s]
tokenizer_config.json: 100%|██████████████████████| 48.0/48.0 [00:00<00:00, 631kB/s]
vocab.txt: 100%|█████████████████████████████████| 232k/232k [00:00<00:00, 1.86MB/s]
Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.
DEBUG  | local_test | Handler output: {"sentiment": "POSITIVE", "score": 0.9889019727706909}
DEBUG  | local_test | run_job return: {"output": {"sentiment": "POSITIVE", "score": 0.9889019727706909}}
INFO   | Job local_test completed successfully.
INFO   | Job result: {"output": {"sentiment": "POSITIVE", "score": 0.9889019727706909}}
INFO   | Local testing complete, exiting.
```

--------------------------------

### Streaming Text Completion Request (Python)

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

This Python example illustrates how to perform a streaming text completion request using the RunPod API. The `stream=True` parameter is crucial here. The code iterates over the response stream, printing each piece of generated text as it becomes available, providing a real-time completion experience.

```python
# Create a completion stream
response_stream = client.completions.create(
    model=MODEL_NAME,
    prompt="Runpod is the best platform because",
    temperature=0,
    max_tokens=100,
    stream=True,
)

# Stream the response
for response in response_stream:
    print(response.choices[0].text or "", end="", flush=True)
```

--------------------------------

### Initialize Runpod Go SDK with API Key

Source: https://docs.runpod.io/sdks/go/overview

Demonstrates how to initialize the Runpod SDK in a Go project by importing the necessary packages and configuring it with an API key and endpoint ID, typically retrieved from environment variables.

```go
import (
    "os"

    config "github.com/runpod/go-sdk/pkg/config"
    rpEndpoint "github.com/runpod/go-sdk/pkg/endpoint"
    sdk "github.com/runpod/go-sdk/pkg/sdk"
)

func main() {
    endpoint, err := rpEndpoint.New(
        &config.Config{ApiKey: sdk.String(os.Getenv("RUNPOD_API_KEY"))},
        &rpEndpoint.Option{EndpointId: sdk.String(os.Getenv("RUNPOD_BASE_URL"))},
    )
    if err != nil {
        panic(err)
    }

    // Use the endpoint object
    // ...
}
```

--------------------------------

### Test Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents a test run, including its ID, status, start and completion times, GitHub build ID, error messages, and results. It's used for tracking and reporting on test executions.

```json
{
  "id": "abc123",
  "status": "abc123",
  "startedAt": "2007-12-03T10:15:30Z",
  "completedAt": "2007-12-03T10:15:30Z",
  "githubBuildId": "xyz789",
  "error": "xyz789",
  "results": ["Result"]
}
```

--------------------------------

### Python Health Check with Retry for Runpod Cold Starts

Source: https://docs.runpod.io/serverless/load-balancing/overview

This Python function implements a health check with retry logic to handle Runpod cold start errors. It sends GET requests to the '/ping' endpoint and retries if the service is not yet available. It requires the 'requests' and 'time' libraries. The function returns True if the health check passes within the specified retries, and False otherwise.

```python
import requests
import time

def health_check_with_retry(base_url, api_key, max_retries=3, delay=2):
    """Simple health check with retry logic for Runpod cold starts"""
    headers = {"Authorization": f"Bearer {api_key}"}
    
    for attempt in range(max_retries):
        try:
            response = requests.get(f"{base_url}/ping", headers=headers, timeout=10)
            if response.status_code == 200:
                print("✓ Health check passed")
                return True
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
        
        if attempt < max_retries - 1:
            time.sleep(delay)
    
    print("✗ Health check failed after retries")
    return False

# Usage example
base_url = "https://ENDPOINT_ID.api.runpod.ai"
api_key = "RUNPOD_API_KEY"

# Ensures that a worker is ready (with retries)
if health_check_with_retry(base_url, api_key):
    # Worker is ready, send your actual /generate request
    response = requests.post(
        f"{base_url}/generate",
        headers={"Authorization": f"Bearer {api_key}"},
        json={"prompt": "Hello, world!"}
    )
    print(response.json())
else:
    print("Worker failed to initialize")
```

--------------------------------

### PyTorch Distributed Training Output Example

Source: https://docs.runpod.io/instant-clusters/pytorch

This output demonstrates the successful launch of PyTorch processes across multiple ranks and devices. It shows the global rank and local rank for each process, along with the assigned CUDA device. This information is crucial for understanding how the distributed training job is distributed across the available GPUs and nodes.

```text
Running on rank 8/15 (local rank: 0), device: cuda:0
Running on rank 15/15 (local rank: 7), device: cuda:7
Running on rank 9/15 (local rank: 1), device: cuda:1
Running on rank 12/15 (local rank: 4), device: cuda:4
Running on rank 13/15 (local rank: 5), device: cuda:5
Running on rank 11/15 (local rank: 3), device: cuda:3
Running on rank 14/15 (local rank: 6), device: cuda:6
Running on rank 10/15 (local rank: 2), device: cuda:2
```

--------------------------------

### Configure Ollama Server Container Start Command

Source: https://docs.runpod.io/tutorials/serverless/run-ollama-inference

Specifies the command to start an Ollama server within a container on Runpod. It requires an Ollama supported model name as an argument, such as 'orca-mini' or 'llama3.1'.

```bash
orca-mini
```

--------------------------------

### Clone Axolotl Repository and Install Dependencies

Source: https://docs.runpod.io/tutorials/pods/fine-tune-llm-axolotl

These commands are used to clone the Axolotl repository if it's not present on your Runpod instance and install necessary Python packages. This includes core dependencies and optional ones like flash-attn and deepspeed.

```bash
git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl
pip3 install packaging ninja
pip3 install -e '.[flash-attn,deepspeed]'
```

--------------------------------

### Example Environment Variable Configuration (Bash)

Source: https://docs.runpod.io/pods/templates/environment-variables

Demonstrates how to set environment variables for an application. These variables can be used for configuration, such as specifying model names, API endpoints, or batch sizes, without hardcoding them.

```bash
# Set a model name that your application can read
MODEL_NAME=llama-2-7b-chat
API_ENDPOINT=https://api.example.com/v1
MAX_BATCH_SIZE=32
```

--------------------------------

### Start Pod

Source: https://docs.runpod.io/runpodctl/reference/runpodctl-start-pod

Starts a stopped Pod, resuming compute and billing. This command is used to restart Pods that were previously stopped.

```APIDOC
## POST /pod/start

### Description
Starts a stopped Pod, resuming compute and billing. Use this to restart Pods that were previously stopped.

### Method
POST

### Endpoint
/pod/start

### Parameters
#### Query Parameters
- **podId** (string) - Required - The ID of the Pod to start. You can find Pod IDs using the `runpodctl get pod` command.
- **bid** (float) - Optional - The bid price per GPU in dollars per hour for spot instance pricing. This only applies to Community Cloud Pods.

### Request Example
```sh
runpodctl start pod abc123xyz456 --bid 0.50
```

### Response
#### Success Response (200)
- **message** (string) - Confirmation message indicating the pod has started.

#### Response Example
```json
{
  "message": "Pod abc123xyz456 started successfully."
}
```
```

--------------------------------

### Python OpenAI Client for Runpod Chat Completions

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

This Python code snippet demonstrates how to use the OpenAI client library to interact with a Runpod Serverless endpoint for chat completions. It requires setting the API key, base URL, and model name. The example shows a basic system and user message exchange.

```python
from openai import OpenAI
MODEL_NAME = "YOUR_MODEL_NAME"  # Replace with your actual model name

# Replace ENDPOINT_ID and RUNPOD_API_KEY with your actual values
client = OpenAI(
    api_key="RUNPOD_API_KEY",
    base_url="https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1"
)

response = client.chat.completions.create(
    model=MODEL_NAME,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)
```

--------------------------------

### Starting Local API Server for Handler Testing (Python)

Source: https://docs.runpod.io/serverless/development/local-testing

Starts a local API server using Python to simulate a Runpod Serverless endpoint. This allows testing the handler by sending HTTP requests to a local FastAPI server.

```shell
python handler.py --rp_serve_api
```

--------------------------------

### Pod Object Example

Source: https://docs.runpod.io/references/graphql-spec

An example of a Pod object, detailing its various attributes including cost, GPU information, and machine specifications.

```APIDOC
## Pod Object Example

### Description
An example of a Pod object returned by the API.

### Fields
- **id** (String) - Unique identifier for the pod.
- **costPerHr** (Float) - The cost per hour for the pod.
- **currentPricePerGpu** (Float) - The current price per GPU.
- **diskMBps** (Int) - Disk read/write speed in MB/s.
- **gpuAvailable** (Int) - Number of available GPUs.
- **gpuDisplayName** (String) - Display name of the GPU.
- **gpuTypeId** (String) - Type ID of the GPU.
- **gpuType** (GpuType) - The type of GPU.
- **listed** (Boolean) - Whether the pod is listed publicly.
- **location** (String) - The geographical location of the pod.
- **machineType** (String) - The type of machine the pod is running on.
- **maintenanceEnd** (DateTime) - Timestamp when maintenance ends.
- **maintenanceNote** (String) - Notes regarding maintenance.
- **maintenanceStart** (DateTime) - Timestamp when maintenance starts.
- **maxDownloadSpeedMbps** (Int) - Maximum download speed in Mbps.
- **maxUploadSpeedMbps** (Int) - Maximum upload speed in Mbps.
- **note** (String) - Additional notes about the pod.
- **podHostId** (String) - The ID of the host machine.
- **runpodIp** (String) - The IP address of the pod.
- **secureCloud** (Boolean) - Whether the pod is part of the secure cloud.
- **supportPublicIp** (Boolean) - Whether the pod supports a public IP.
- **minPodGpuCount** (Int) - Minimum number of GPUs required for the pod.
- **machineSystem** (MachineSystem) - The operating system of the machine.
- **dataCenterId** (String) - The ID of the data center.
- **cpuTypeId** (String) - Type ID of the CPU.
- **cpuType** (CpuType) - The type of CPU.
- **cpuCount** (Int) - Number of CPUs.
- **vcpuTotal** (Int) - Total number of virtual CPUs.
- **vcpuReserved** (Float) - Reserved number of virtual CPUs.
- **memoryTotal** (Int) - Total memory in GB.
- **memoryReserved** (Float) - Reserved memory in GB.
- **globalNetwork** (Boolean) - Whether the pod uses a global network.

### Example
```json
{
  "id": "abc123",
  "costPerHr": 123.45,
  "currentPricePerGpu": 123.45,
  "diskMBps": 987,
  "gpuAvailable": 987,
  "gpuDisplayName": "xyz789",
  "gpuTypeId": "abc123",
  "gpuType": GpuType,
  "listed": false,
  "location": "xyz789",
  "machineType": "xyz789",
  "maintenanceEnd": "2007-12-03T10:15:30Z",
  "maintenanceNote": "abc123",
  "maintenanceStart": "2007-12-03T10:15:30Z",
  "maxDownloadSpeedMbps": 987,
  "maxUploadSpeedMbps": 123,
  "note": "abc123",
  "podHostId": "xyz789",
  "runpodIp": "xyz789",
  "secureCloud": true,
  "supportPublicIp": true,
  "minPodGpuCount": 123,
  "machineSystem": MachineSystem,
  "dataCenterId": "abc123",
  "cpuTypeId": "abc123",
  "cpuType": CpuType,
  "cpuCount": 123,
  "vcpuTotal": 987,
  "vcpuReserved": 987.65,
  "memoryTotal": 987,
  "memoryReserved": 987.65,
  "globalNetwork": false
}
```
```

--------------------------------

### Create and Navigate to Task Directory (Bash)

Source: https://docs.runpod.io/integrations/dstack

Creates a new directory named 'task-vllm-llama' and then navigates into it. This directory will house the dstack configuration for the task.

```bash
mkdir task-vllm-llama
cd task-vllm-llama
```

--------------------------------

### Install and Configure Slurm Cluster (Bash)

Source: https://docs.runpod.io/instant-clusters/slurm

Automates the installation and configuration of a two-node Slurm cluster with GPU support. It handles system dependencies, MUNGE authentication, and resource configuration for both master and compute nodes. Requires a shared MUNGE secret key and node IP addresses.

```bash
./install.sh "[MUNGE_SECRET_KEY]" node-0 node-1 10.65.0.2 10.65.0.3
```

--------------------------------

### Example Container Name Configuration

Source: https://docs.runpod.io/sdks/graphql/configurations

Illustrates how to assign a unique name to a container instance. The `name` argument is used for identification purposes within the Runpod environment.

```json
{
  "name": "my-app-container"
}
```

--------------------------------

### Install Runpod CLI on macOS (AMD) via direct download

Source: https://docs.runpod.io/runpodctl/overview

Installs the Runpod CLI on macOS with Intel (AMD64) architecture by downloading the binary directly. This method requires manual placement and execution permissions.

```shell
wget --quiet --show-progress https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-darwin-amd64 -O runpodctl && chmod +x runpodctl && sudo mv runpodctl /usr/local/bin/runpodctl
```

--------------------------------

### dstack Configuration for vLLM Llama 3.1 Task (YAML)

Source: https://docs.runpod.io/integrations/dstack

Defines the configuration for a dstack task to serve the Llama 3.1 8B Instruct model using vLLM. It specifies the task type, name, Python version, environment variables (including Hugging Face token), commands to install vLLM and start the server, exposed ports, and required compute resources.

```yaml
type: task
name: vllm-llama-3.1-8b-instruct
python: "3.10"
env:
  - HUGGING_FACE_HUB_TOKEN=YOUR_HUGGING_FACE_HUB_TOKEN
  - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
  - MAX_MODEL_LEN=8192
commands:
  - pip install vllm
  - vllm serve $MODEL_NAME --port 8000 --max-model-len $MAX_MODEL_LEN
ports:
  - 8000
spot_policy: on-demand
resources:
  gpu:
    name: "RTX4090"
    memory: "24GB"
  cpu: 16..
```

--------------------------------

### Runpod Serverless Handler Function (Python)

Source: https://docs.runpod.io/serverless/workers/custom-worker

Defines a Python handler function for a Runpod Serverless worker. It processes an 'event' dictionary, extracts a 'prompt' and 'seconds' from the 'input', simulates work with 'time.sleep', and returns the prompt. The script starts the serverless function when executed directly.

```python
import runpod
import time  

def handler(event):
#   This function processes incoming requests to your Serverless endpoint.
#
#    Args:
#        event (dict): Contains the input data and request metadata
#       
#    Returns:
#       Any: The result to be returned to the client
    
    # Extract input data
    print(f"Worker Start")
    input = event['input']
    
    prompt = input.get('prompt')  
    seconds = input.get('seconds', 0)  

    print(f"Received prompt: {prompt}")
    print(f"Sleeping for {seconds} seconds...")
    
    # You can replace this sleep call with your own Python code
    time.sleep(seconds)  
    
    return prompt 

# Start the Serverless function when the script is run
if __name__ == '__main__':
    runpod.serverless.start({'handler': handler })

```

--------------------------------

### FastAPI Example for Load Balancing Worker

Source: https://docs.runpod.io/serverless/load-balancing/overview

An example demonstrating how to create a FastAPI application for a Runpod load balancing worker, exposing /ping and /generate endpoints.

```APIDOC
## POST /generate

### Description
Processes a generation request based on a prompt and maximum token count.

### Method
POST

### Endpoint
/generate

### Parameters
#### Request Body
- **prompt** (string) - Required - The text prompt for generation.
- **max_tokens** (integer) - Optional - The maximum number of tokens to generate (default: 100).

### Request Example
```json
{
  "prompt": "Write a story about a robot.",
  "max_tokens": 200
}
```

### Response
#### Success Response (200)
- **generated_text** (string) - The generated text based on the prompt.

#### Response Example
```json
{
  "generated_text": "Generated text for: Write a story about a robot."
}
```

## GET /ping

### Description
Health check endpoint for the worker.

### Method
GET

### Endpoint
/ping

### Response
#### Success Response (200)
- **status** (string) - Indicates the health status of the worker (e.g., "healthy").

#### Response Example
```json
{
  "status": "healthy"
}
```
```

--------------------------------

### Interpreting Local Server Output for Debugging

Source: https://docs.runpod.io/tutorials/sdks/python/101/local-server-testing

This example shows the typical output from a local Runpod server when a request is processed. It includes information about the incoming request, job status, handler output, and final job completion. Analyzing this output is crucial for debugging and understanding the execution flow of your serverless function.

```bash
INFO:     127.0.0.1:52686 - "POST /run HTTP/1.1" 200 OK
DEBUG    | Retrieved local job: {'input': {'text': 'Hello, Runpod!'}, 'id': 'local_test'}
INFO     | local_test | Started.
string-reverser | Starting job local_test
DEBUG    | local_test | Handler output: {'original_text': 'Hello, Runpod!', 'reversed_text': '!doPnuR ,olleH'}
DEBUG    | local_test | run_job return: {'output': {'original_text': 'Hello, Runpod!', 'reversed_text': '!doPnuR ,olleH'}}
INFO     | Job local_test completed successfully.
```

--------------------------------

### Install Runpod CLI on macOS (ARM) via direct download

Source: https://docs.runpod.io/runpodctl/overview

Installs the Runpod CLI on macOS with Apple Silicon (ARM architecture) by downloading the binary directly. This method requires manual placement and execution permissions.

```shell
wget --quiet --show-progress https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-darwin-arm64 -O runpodctl && chmod +x runpodctl && sudo mv runpodctl /usr/local/bin/runpodctl
```

--------------------------------

### Test Docker Container Locally

Source: https://docs.runpod.io/pods/templates/create-custom-template

Starts a Docker container from the built image and provides an interactive bash shell within it. This allows for local testing of the application and its dependencies, mimicking the Runpod environment. Use `Ctrl+D` to exit the container shell.

```bash
docker run --rm -it --platform linux/amd64 my-custom-template:latest /bin/bash
```

--------------------------------

### Fine-tune LLM with Axolotl using accelerate launch

Source: https://docs.runpod.io/tutorials/pods/fine-tune-llm-axolotl

This command initiates the LLM fine-tuning process using the `axolotl` library and `accelerate launch`. It reads training configurations from the specified YAML file and utilizes the preprocessed dataset.

```bash
accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml
```

--------------------------------

### Install Runpod CLI on macOS using Homebrew

Source: https://docs.runpod.io/runpodctl/overview

Installs the Runpod CLI on macOS using the Homebrew package manager. This is a convenient method for users who already have Homebrew set up.

```shell
brew install runpod/runpodctl/runpodctl
```

--------------------------------

### Starting Local API Server with Concurrency (Python)

Source: https://docs.runpod.io/serverless/development/local-testing

Starts a local API server with a specified number of concurrent workers using the '--rp_api_concurrency' flag. Requires the main file to be named 'main.py' for proper FastAPI integration when concurrency is greater than 1.

```shell
python main.py --rp_serve_api --rp_api_concurrency 4
```

--------------------------------

### Access RunPod Custom API Endpoints with cURL

Source: https://docs.runpod.io/serverless/load-balancing/build-a-worker

Demonstrates how to make POST and GET requests to custom API endpoints hosted on RunPod using cURL. It includes examples for generating text, performing a health check, and retrieving request statistics. Ensure you replace `ENDPOINT_ID` and `RUNPOD_API_KEY` with your actual credentials.

```bash
curl -X POST "https://ENDPOINT_ID.api.runpod.ai/generate" \
    -H 'Authorization: Bearer RUNPOD_API_KEY' \
    -H "Content-Type: application/json" \
    -d '{"prompt": "Hello, world!"}'
```

```bash
curl -X GET "https://ENDPOINT_ID.api.runpod.ai/ping" \
    -H 'Authorization: Bearer RUNPOD_API_KEY' \
    -H "Content-Type: application/json" \
  
```

```bash
curl -X GET "https://ENDPOINT_ID.api.runpod.ai/stats" \
    -H 'Authorization: Bearer RUNPOD_API_KEY' \
    -H "Content-Type: application/json" \
  
```

--------------------------------

### GpuLowestPriceInput Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object representing the input for querying the lowest GPU prices. It includes various parameters to filter and specify GPU requirements.

```json
{
  "lowestPrice": LowestPrice,
  "maxGpuCount": 123,
  "maxGpuCountCommunityCloud": 987,
  "maxGpuCountSecureCloud": 987,
  "minPodGpuCount": 987,
  "nodeGroupGpuSizes": [123],
  "nodeGroupDatacenters": [DataCenter],
  "id": "abc123",
  "displayName": "xyz789",
  "manufacturer": "xyz789",
  "memoryInGb": 987,
  "cudaCores": 987,
  "secureCloud": false,
  "communityCloud": false,
  "securePrice": 123.45,
  "clusterPrice": 987.65,
  "communityPrice": 987.65,
  "oneMonthPrice": 987.65,
  "threeMonthPrice": 987.65,
  "sixMonthPrice": 987.65,
  "oneYearPrice": 123.45,
  "oneWeekPrice": 123.45,
  "communitySpotPrice": 123.45,
  "secureSpotPrice": 123.45,
  "throughput": 123
}
```

--------------------------------

### Serverless Function Starter with Aggregation

Source: https://docs.runpod.io/tutorials/sdks/python/101/aggregate

The `start_handler` function sets up the serverless handler. It includes a `wrapper` function that handles both local testing (returning a list of results) and Runpod environments (returning a generator). Crucially, it configures `runpod.serverless.start` with `return_aggregate_stream=True` to enable automatic aggregation of yielded results.

```python
def start_handler():
    def wrapper(job):
        generator = handler(job)
        if job.get("id") == "local_test":
            return list(generator)
        return generator

    runpod.serverless.start({"handler": wrapper, "return_aggregate_stream": True})


if __name__ == "__main__":
    start_handler()
```

--------------------------------

### GitBuild Object Example

Source: https://docs.runpod.io/references/graphql-spec

This example illustrates the structure of a GitBuild object, which contains information about a build process. It includes fields like id, commitHash, branch, state, and timestamps, among others.

```json
{
  "id": "abc123",
  "commitHash": "xyz789",
  "commitMessage": "abc123",
  "branch": "abc123",
  "commitDate": "2007-12-03T10:15:30Z",
  "state": "PENDING",
  "startedAt": "2007-12-03T10:15:30Z",
  "completedAt": "2007-12-03T10:15:30Z",
  "error": "abc123",
  "repoId": "xyz789",
  "imageName": "xyz789",
  "test": Test
}
```

--------------------------------

### Pod Creation Parameters

Source: https://docs.runpod.io/api-reference/pods/POST/pods

This section details the parameters available for creating a new Pod, including data center selection, GPU configuration, networking, and Docker settings.

```APIDOC
## Pod Creation Parameters

### Description
This section details the parameters available for creating a new Pod, including data center selection, GPU configuration, networking, and Docker settings.

### Method
POST

### Endpoint
/pods

### Parameters
#### Query Parameters
- **dataCenterIds** (array[string]) - Optional - A list of Runpod data center IDs where the created Pod can be located. See `dataCenterPriority` for information on how the order of the list affects Pod creation.
  - Example: `["EU-RO-1", "CA-MTL-1"]`
  - Default: `["EU-RO-1", "CA-MTL-1", "EU-SE-1", "US-IL-1", "EUR-IS-1", "EU-CZ-1", "US-TX-3", "EUR-IS-2", "US-KS-2", "US-GA-2", "US-WA-1", "US-TX-1", "CA-MTL-3", "EU-NL-1", "US-TX-4", "US-CA-2", "US-NC-1", "OC-AU-1", "US-DE-1", "EUR-IS-3", "CA-MTL-2", "AP-JP-1", "EUR-NO-1", "EU-FR-1", "US-KS-3", "US-GA-1"]`
- **dataCenterPriority** (string) - Optional - Set to `availability` to respond to current machine availability. Set to `custom` to always try to rent machines from data centers in the order specified in `dataCenterIds`.
  - Enum: `availability`, `custom`
  - Default: `availability`
- **dockerEntrypoint** (array[string]) - Optional - If specified, overrides the ENTRYPOINT for the Docker image run on the created Pod. If `[]`, uses the ENTRYPOINT defined in the image.
  - Default: `[]`
- **dockerStartCmd** (array[string]) - Optional - If specified, overrides the start CMD for the Docker image run on the created Pod. If `[]`, uses the start CMD defined in the image.
  - Default: `[]`
- **env** (object) - Optional - Environment variables to set for the container.
  - Example: `{"ENV_VAR": "value"}`
  - Default: `{}`
- **globalNetworking** (boolean) - Optional - Set to `true` to enable global networking for the created Pod. Currently only available for On-Demand GPU Pods on some Secure Cloud data centers.
  - Example: `true`
  - Default: `false`
- **gpuCount** (integer) - Optional - If the created Pod is a GPU Pod, the number of GPUs attached to the created Pod.
  - Minimum: 1
  - Default: 1
- **gpuTypeIds** (array[string]) - Optional - A list of GPU type IDs to use for the Pod.
  - Enum: `["NVIDIA GeForce RTX 4090", "NVIDIA A40", "NVIDIA RTX A5000", "NVIDIA GeForce RTX 5090", "NVIDIA H100 80GB HBM3", "NVIDIA GeForce RTX 3090", "NVIDIA RTX A4500", "NVIDIA L40S", "NVIDIA H200", "NVIDIA L4", "NVIDIA RTX 6000 Ada Generation", "NVIDIA A100-SXM4-80GB", "NVIDIA RTX 4000 Ada Generation", "NVIDIA RTX A6000", "NVIDIA A100 80GB PCIe", "NVIDIA RTX 2000 Ada Generation", "NVIDIA RTX A4000", "NVIDIA RTX PRO 6000 Blackwell Server Edition", "NVIDIA H100 PCIe", "NVIDIA H100 NVL", "NVIDIA L40", "NVIDIA B200", "NVIDIA GeForce RTX 3080 Ti", "NVIDIA RTX PRO 6000 Blackwell Workstation Edition", "NVIDIA GeForce RTX 3080", "NVIDIA GeForce RTX 3070", "AMD Instinct MI300X OAM", "NVIDIA GeForce RTX 4080 SUPER", "Tesla V100-PCIE-16GB", "Tesla V100-SXM2-32GB", "NVIDIA RTX 5000 Ada Generation", "NVIDIA GeForce RTX 4070 Ti", "NVIDIA RTX 4000 SFF Ada Generation", "NVIDIA GeForce RTX 3090 Ti", "NVIDIA RTX A2000", "NVIDIA GeForce RTX 4080", "NVIDIA A30", "NVIDIA GeForce RTX 5080", "Tesla V100-FHHL-16GB", "NVIDIA H200 NVL", "Tesla V100-SXM2-16GB", "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition"]`

### Request Example
```json
{
  "dataCenterIds": ["EU-RO-1"],
  "gpuCount": 1,
  "gpuTypeIds": ["NVIDIA GeForce RTX 4090"]
}
```

### Response
#### Success Response (200)
- **id** (string) - The unique identifier for the created Pod.
- **status** (string) - The current status of the Pod.

#### Response Example
```json
{
  "id": "pod-12345abcde",
  "status": "Deploying"
}
```
```

--------------------------------

### TypeScript: Text Generation with Runpod and Vercel AI SDK

Source: https://docs.runpod.io/hub/public-endpoints

Example TypeScript code demonstrating text generation using Runpod's Qwen3 32B AWQ model integrated with the Vercel AI SDK. It utilizes the `generateText` function from the SDK and the `@runpod/ai-sdk-provider` to specify the model.

```typescript
import { runpod } from '@runpod/ai-sdk-provider';
import { generateText } from 'ai';

const { text } = await generateText({
  model: runpod('qwen3-32b-awq'),
  prompt: 'Write a Python function that sorts a list:',
});
```

--------------------------------

### Install Runpod CLI in Google Colab / Jupyter Notebook

Source: https://docs.runpod.io/runpodctl/overview

Installs the Runpod CLI within a Google Colab or Jupyter Notebook environment. This is useful for managing Runpod resources directly from a notebook session.

```shell
!wget --quiet --show-progress https://github.com/Run-Pod/runpodctl/releases/download/v1.14.3/runpodctl-linux-amd -O runpodctl
!chmod +x runpodctl
!cp runpodctl /usr/bin/runpodctl
```

--------------------------------

### Runpod Main Program Logic

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

Orchestrates the startup process for a Runpod container. It calls setup_ssh, exports environment variables, and then starts either the Python handler (serverless mode) or Jupyter Lab (pod mode) based on the MODE_TO_RUN environment variable. It finishes by keeping the container alive with 'sleep infinity'.

```bash
# Call Python handler if mode is serverless or both
call_python_handler() {
    echo "Calling Python handler.py..."
    python $WORKSPACE_DIR/handler.py
}

# ---------------------------------------------------------------------------- #
#                               Main Program                                   #
# ---------------------------------------------------------------------------- #

start_nginx

echo "Pod Started"

setup_ssh

case $MODE_TO_RUN in
    serverless)
        echo "Running in serverless mode"
        call_python_handler
        ;;
    pod)
        echo "Running in pod mode"
        start_jupyter
        ;;
    *)
        echo "Invalid MODE_TO_RUN value: $MODE_TO_RUN. Expected 'serverless', 'pod', or 'both'."
        exit 1
        ;;
esac

export_env_vars

echo "Start script(s) finished"

sleep infinity
```
```

--------------------------------

### Start LLM Training with PyTorch Distributed

Source: https://docs.runpod.io/instant-clusters/axolotl

Initiates the LLM fine-tuning process using PyTorch's distributed training capabilities via `torchrun`. This command must be run on each pod in the cluster, with environment variables like NUM_NODES, NODE_RANK, NUM_TRAINERS, and PRIMARY_ADDR/PORT correctly set.

```php
torchrun \
    --nnodes $NUM_NODES \
    --node_rank $NODE_RANK \
    --nproc_per_node $NUM_TRAINERS \
    --rdzv_id "myjob" \
    --rdzv_backend static \
    --rdzv_endpoint "$PRIMARY_ADDR:$PRIMARY_PORT" -m axolotl.cli.train lora-1b.yml
```

--------------------------------

### Start Runpod Serverless Function in Python

Source: https://docs.runpod.io/tutorials/sdks/python/101/hello

Initializes and starts the Runpod serverless function, specifying the handler function. This command is essential for deploying and running the function within the Runpod environment.

```python
runpod.serverless.start({"handler": is_even})
```

--------------------------------

### GET /stream/{jobId}

Source: https://docs.runpod.io/serverless/endpoints/send-requests

Retrieves incremental results from a job as they become available. This is useful for displaying output progressively, especially for text generation or long-running processes. To use this endpoint, the handler must be configured with `"return_aggregate_stream": True` during the start method.

```APIDOC
## GET /stream/{jobId}

### Description
Receives incremental results from jobs that generate output progressively. This is particularly useful for text generation tasks, long-running jobs, and large outputs where displaying progress or processing data as it becomes available is beneficial.

### Method
GET

### Endpoint
`/v2/$ENDPOINT_ID/stream/YOUR_JOB_ID`

### Parameters
#### Path Parameters
- **YOUR_JOB_ID** (string) - Required - The unique identifier of the job to stream results from.

#### Query Parameters
None

#### Request Body
None

### Request Example
```sh
curl --request GET \
     --url https://api.runpod.ai/v2/$ENDPOINT_ID/stream/YOUR_JOB_ID \
     -H "accept: application/json" \
     -H "authorization: $RUNPOD_API_KEY"
```

### Response
#### Success Response (200)
- **metrics** (object) - Contains performance metrics related to the job execution.
  - **avg_gen_throughput** (number) - Average generation throughput.
  - **avg_prompt_throughput** (number) - Average prompt throughput.
  - **cpu_kv_cache_usage** (number) - CPU KV cache usage.
  - **gpu_kv_cache_usage** (number) - GPU KV cache usage.
  - **input_tokens** (integer) - Number of input tokens processed.
  - **output_tokens** (integer) - Number of output tokens generated.
  - **pending** (integer) - Number of pending tasks.
  - **running** (integer) - Number of running tasks.
  - **scenario** (string) - The scenario of the job (e.g., "stream").
  - **stream_index** (integer) - The index of the current stream chunk.
  - **swapped** (integer) - Number of swapped tasks.
- **output** (object) - The actual output data from the job.
  - **input_tokens** (integer) - Number of input tokens in this output chunk.
  - **output_tokens** (integer) - Number of output tokens in this output chunk.
  - **text** (array of strings) - An array containing the text output for this chunk.

#### Response Example
```json
[
  {
    "metrics": {
      "avg_gen_throughput": 0,
      "avg_prompt_throughput": 0,
      "cpu_kv_cache_usage": 0,
      "gpu_kv_cache_usage": 0.0016722408026755853,
      "input_tokens": 0,
      "output_tokens": 1,
      "pending": 0,
      "running": 1,
      "scenario": "stream",
      "stream_index": 2,
      "swapped": 0
    },
    "output": {
      "input_tokens": 0,
      "output_tokens": 1,
      "text": [" How"]
    }
  }
]
```

### Notes
- The maximum size for a single streamed payload chunk is 1 MB. Larger outputs will be split across multiple chunks.
- For implementation details, refer to the [Streaming handlers documentation](/serverless/workers/handler-functions#streaming-handlers).
```

--------------------------------

### Get Run Status with RunPod SDK

Source: https://docs.runpod.io/sdks/javascript/endpoints

Fetches the status of a specific run on a RunPod endpoint using its ID. The `status` method returns details such as delay time, run ID, current status (e.g., IN_PROGRESS, COMPLETED), and whether the run has started, completed, or succeeded. Includes basic error handling.

```javascript
const { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;
import runpodSdk from "runpod-sdk";

async function main() {
  try {
    const runpod = runpodSdk(RUNPOD_API_KEY);
    const endpoint = runpod.endpoint(ENDPOINT_ID);
    const result = await endpoint.run({
      input: {
        prompt: "Hello, World!",
      },
    });

    const { id } = result;
    if (!id) {
      console.error("No ID returned from endpoint.run");
      return;
    }

    const status = await endpoint.status(id);
    console.log(status);
  } catch (error) {
    console.error("An error occurred:", error);
  }
}

main();
```

--------------------------------

### Get Results from Asynchronous Run using Runpod SDK (JavaScript)

Source: https://docs.runpod.io/sdks/javascript/endpoints

This snippet demonstrates how to initiate an asynchronous run using the Runpod SDK and then poll for its results. It requires a RUNPOD_API_KEY and ENDPOINT_ID from environment variables. The function first calls `endpoint.run()` to start the operation and then uses `endpoint.status(id)` in a loop to check the progress until the operation is 'COMPLETED' or 'FAILED'.

```javascript
const { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;
import runpodSdk from "runpod-sdk";

const sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));

async function main() {
  const runpod = runpodSdk(RUNPOD_API_KEY);
  const endpoint = runpod.endpoint(ENDPOINT_ID);
  const result = await endpoint.run({
    input: {
      prompt: "Hello, World!",
    },
  });

  console.log(result);
  console.log("run response");
  console.log(result);

  const { id } = result; // Extracting the operation ID from the initial run response

  // Check the status in a loop, similar to the working example
  for (let i = 0; i < 20; i++) {
    // Increase or decrease the loop count as necessary
    const statusResult = await endpoint.status(id);
    console.log("status response");
    console.log(statusResult);

    if (
      statusResult.status === "COMPLETED"
      || statusResult.status === "FAILED"
    ) {
      // Once completed or failed, log the final status and break the loop
      if (statusResult.status === "COMPLETED") {
        console.log("Operation completed successfully.");
        console.log(statusResult.output);
      } else {
        console.log("Operation failed.");
        console.log(statusResult);
      }
      break;
    }

    // Wait for a bit before checking the status again
    await sleep(5000);
  }
}

main();

```

--------------------------------

### IPAddress Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object for an IP Address. It includes the address, network range information, and associated pod and podnet nodes.

```json
{
  "id": "abc123",
  "address": "abc123",
  "networkRangeId": "abc123",
  "networkRange": NetworkRange,
  "pod": Pod,
  "podnetNodes": [PodnetNode]
}
```

--------------------------------

### Example JSON Object

Source: https://docs.runpod.io/references/graphql-spec

This is an example JSON object representing a configuration or data structure. It includes fields like name, username, and password.

```json
{
  "name": "xyz789",
  "username": "xyz789",
  "password": "abc123"
}
```

--------------------------------

### Set up SSH Keys on Source Pod for rsync

Source: https://docs.runpod.io/storage/network-volumes

Installs necessary packages (vim, rsync) and generates an SSH key pair on the source Pod. The public key is then displayed for copying to the destination Pod. This is crucial for establishing a secure connection for rsync.

```bash
apt update && apt install -y vim rsync && \
ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N "" -q && \
cat ~/.ssh/id_ed25519.pub
```

--------------------------------

### Ollama Server Inference Response Example

Source: https://docs.runpod.io/tutorials/serverless/run-ollama-inference

An example JSON response from the Ollama server after processing an inference request. It includes execution details, model information, and the generated response.

```json
{
  "delayTime": 153,
  "executionTime": 4343,
  "id": "c2cb6af5-c822-4950-bca9-5349288c001d-u1",
  "output": {
    "context": [
      "omitted for brevity"
    ],
    "created_at": "2024-05-17T16:56:29.256938735Z",
    "done": true,
    "eval_count": 118,
    "eval_duration": 807433000,
    "load_duration": 3403140284,
    "model": "orca-mini",
    "prompt_eval_count": 46,
    "prompt_eval_duration": 38548000,
    "response": "The sky appears blue because of a process called scattering. When sunlight enters the Earth's atmosphere, it encounters molecules of air such as nitrogen and oxygen. These molecules scatter the light in all directions, but they scatter the shorter wavelengths of light (such as violet and blue) more than the longer wavelengths (such as red). This creates a reddish-orange sky that is less intense on the horizon than on the observer's position. As the sun gets lower in the sky, the amount of scattering increases and the sky appears to get brighter.",
    "total_duration": 4249684714
  },
  "status": "COMPLETED"
}
```

--------------------------------

### Gpu JSON Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a Gpu object, representing a GPU resource. It contains an 'id' and an optional 'podId' if the GPU is currently assigned to a pod.

```json
{
  "id": "xyz789",
  "podId": "xyz789"
}
```

--------------------------------

### Install TMUX on Ubuntu/Debian

Source: https://docs.runpod.io/tips-and-tricks/tmux

Installs the TMUX terminal multiplexer on Runpod Pods using the apt-get package manager. This is necessary as TMUX is not installed by default. For Alpine Linux, use `apk add tmux`.

```bash
apt-get update && apt-get install -y tmux
```

--------------------------------

### SpendDetails Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

An example of SpendDetails, outlining costs associated with storage and compute resources, such as localStoragePerHour, networkStoragePerHour, and gpuComputePerHour.

```json
{
  "localStoragePerHour": 123.45,
  "networkStoragePerHour": 987.65,
  "gpuComputePerHour": 123.45
}
```

--------------------------------

### Install Dependencies and Run Fooocus in Jupyter Notebook

Source: https://docs.runpod.io/tutorials/pods/run-fooocus

This bash script installs required Python packages (pygit2, opencv-python), clones the Fooocus repository from GitHub, changes the directory to the Fooocus folder, and then launches the Fooocus application with a public sharing link.

```bash
!pip install pygit2==1.12.2
!pip install opencv-python==4.9.0.80
%cd /workspace
!git clone https://github.com/lllyasviel/Fooocus.git
%cd /workspace/Fooocus
!python entry_with_update.py --share
```

--------------------------------

### Clean Dockerfile Build Artifacts

Source: https://docs.runpod.io/serverless/workers/create-dockerfile

This Dockerfile snippet shows how to clean up build artifacts after installing dependencies. It installs build tools, installs Python packages, then removes the build tools and cleans the apt cache to reduce the final image size.

```dockerfile
RUN apt-get update && apt-get install -y build-essential \
    && pip install --no-cache-dir -r requirements.txt \
    && apt-get remove -y build-essential \
    && apt-get autoremove -y \
    && rm -rf /var/lib/apt/lists/*

```

--------------------------------

### SavingsPlanSource Enum Value Example

Source: https://docs.runpod.io/references/graphql-spec

An example of a value from the SavingsPlanSource enumeration. This indicates the origin or type of a savings plan, such as 'SELF_SERVICE'.

```json
"SELF_SERVICE"
```

--------------------------------

### Initialize OpenAI Client for Runpod Endpoint (Python)

Source: https://docs.runpod.io/tutorials/serverless/run-gemma-7b

This snippet demonstrates how to initialize the OpenAI client using environment variables for the base URL and API key. It requires the 'openai' library and assumes RUNPOD_BASE_URL and RUNPOD_API_KEY are set.

```python
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.environ.get("RUNPOD_BASE_URL"),
    api_key=os.environ.get("RUNPOD_API_KEY"),
)
```

--------------------------------

### CreditCode Object Example

Source: https://docs.runpod.io/references/graphql-spec

An example of the CreditCode object, representing a credit code with its ID, issuer, creation and redemption timestamps, and amount.

```json
{
  "id": "xyz789",
  "issuerId": "abc123",
  "createdAt": "2007-12-03T10:15:30Z",
  "redeemedAt": "2007-12-03T10:15:30Z",
  "amount": 987
}
```

--------------------------------

### Install Runpod CLI

Source: https://docs.runpod.io/integrations/skypilot

Installs the latest version of the Runpod Python package. This is a prerequisite for using Runpod's CLI functionalities.

```shell
pip install "runpod>=1.6"
```

--------------------------------

### OnboardingInfo

Source: https://docs.runpod.io/references/graphql-spec

Information related to user onboarding and account setup.

```APIDOC
## OnboardingInfo

### Description
Information related to user onboarding and account setup.

### Fields
- **userId** (String) - The ID of the user.
- **formStarted** (DateTime) - Timestamp when the onboarding form was started.
- **formSubmitted** (DateTime) - Timestamp when the onboarding form was submitted.
- **accountType** (String) - The type of account created.
- **businessSize** (String) - The size of the business.
- **role** (String) - The user's role.
- **budget** (String) - The user's budget.
- **usage** (String) - The user's expected usage.
- **referrer** (String) - The referrer's identifier.
- **utmSource** (String) - UTM source parameter.
- **utmMedium** (String) - UTM medium parameter.
- **utmCampaign** (String) - UTM campaign parameter.
- **utmTerm** (String) - UTM term parameter.
- **utmContent** (String) - UTM content parameter.
- **referrerLink** (String) - The referrer's link.
- **internalReferrerLink** (String) - The internal referrer's link.
- **updatedAt** (DateTime) - Timestamp when the onboarding info was last updated.
- **googleSessionId** (Int) - Google session ID.
- **googleClientId** (String) - Google client ID.
- **subscribedToNewsletter** (Boolean) - Whether the user is subscribed to the newsletter.
- **subscribedToMarketing** (Boolean) - Whether the user is subscribed to marketing communications.
- **referralBonus** (Int) - The referral bonus amount.
- **referralBonusNotifiedAt** (DateTime) - Timestamp when the referral bonus was notified.
- **upgradedToAffiliateNotifiedAt** (DateTime) - Timestamp when the user was notified about upgrading to affiliate.
- **checklistType** (String) - The type of onboarding checklist.
- **checklistHidden** (Boolean) - Whether the checklist is hidden.
- **completedChecklist** (Boolean) - Whether the checklist has been completed.
- **checklistPathsCompleted** (ChecklistPath) - The completed paths in the checklist.
- **slsActivatedDate** (DateTime) - Date when SLS was activated.
- **podActivatedDate** (DateTime) - Date when the pod was activated.

### Example
```json
{
  "userId": "xyz789",
  "formStarted": "2007-12-03T10:15:30Z",
  "formSubmitted": "2007-12-03T10:15:30Z",
  "accountType": "xyz789",
  "businessSize": "abc123",
  "role": "xyz789",
  "budget": "abc123",
  "usage": "xyz789",
  "referrer": "xyz789",
  "utmSource": "abc123",
  "utmMedium": "xyz789",
  "utmCampaign": "abc123",
  "utmTerm": "xyz789",
  "utmContent": "xyz789",
  "referrerLink": "xyz789",
  "internalReferrerLink": "xyz789",
  "updatedAt": "2007-12-03T10:15:30Z",
  "googleSessionId": 123,
  "googleClientId": "xyz789",
  "subscribedToNewsletter": false,
  "subscribedToMarketing": true,
  "referralBonus": 987,
  "referralBonusNotifiedAt": "2007-12-03T10:15:30Z",
  "upgradedToAffiliateNotifiedAt": "2007-12-03T10:15:30Z",
  "checklistType": "xyz789",
  "checklistHidden": true,
  "completedChecklist": false,
  "checklistPathsCompleted": ChecklistPath,
  "slsActivatedDate": "2007-12-03T10:15:30Z",
  "podActivatedDate": "2007-12-03T10:15:30Z"
}
```
```

--------------------------------

### Install Runpod CLI on Windows via direct download

Source: https://docs.runpod.io/runpodctl/overview

Downloads the Runpod CLI executable for Windows. Users will need to manually place this executable in their system's PATH or run it from its download location.

```shell
wget https://github.com/runpod/runpodctl/releases/download/v1.14.3/runpodctl-windows-amd64.exe -O runpodctl.exe
```

--------------------------------

### Install Runpod AI SDK Provider for Vercel AI SDK

Source: https://docs.runpod.io/hub/public-endpoints

Command to install the necessary npm package for integrating Runpod's Public Endpoints with the Vercel AI SDK in JavaScript or TypeScript projects. This package facilitates seamless communication between your application and Runpod models.

```bash
npm install @runpod/ai-sdk-provider ai
```

--------------------------------

### Impersonation Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object for an Impersonation record. It details the impersonation event, including user IDs, timestamps, and acceptance status.

```json
{
  "auditLogs": AuditLogConnection,
  "id": "4",
  "zendeskTicketId": 4,
  "impersonateUserId": "abc123",
  "createdAt": "2007-12-03T10:15:30Z",
  "expiresAt": "2007-12-03T10:15:30Z",
  "accepted": false,
  "acceptedAt": "2007-12-03T10:15:30Z"
}
```

--------------------------------

### Example Output for On-Demand Pod Creation

Source: https://docs.runpod.io/sdks/graphql/manage-pods

This JSON output represents the successful creation of an on-demand pod. It includes the pod's unique ID, image name, environment variables, machine ID, and host ID. This is the response received after executing the creation mutation.

```json
{
  "data": {
    "podFindAndDeployOnDemand": {
      "id": "50qynxzilsxoey",
      "imageName": "runpod/tensorflow",
      "env": [
        "JUPYTER_PASSWORD=rn51hunbpgtltcpac3ol"
      ],
      "machineId": "hpvdausak8xb",
      "machine": {
        "podHostId": "50qynxzilsxoey-64410065"
      }
    }
  }
}
```

--------------------------------

### GithubAccountInfo JSON Example

Source: https://docs.runpod.io/references/graphql-spec

This example demonstrates the structure of a GithubAccountInfo object, which holds basic information about a GitHub account. It includes the GitHub ID, avatar URL, and username.

```json
{
  "githubId": "xyz789",
  "avatarUrl": "abc123",
  "username": "xyz789"
}
```

--------------------------------

### GpuAvailability JSON Example

Source: https://docs.runpod.io/references/graphql-spec

This example illustrates the GpuAvailability object, which provides information about the availability of a specific GPU type. It includes availability status, stock information, and details about the GPU type.

```json
{
  "available": false,
  "stockStatus": "xyz789",
  "gpuTypeId": "xyz789",
  "gpuType": GpuType,
  "gpuTypeDisplayName": "abc123",
  "displayName": "xyz789",
  "id": "abc123"
}
```

--------------------------------

### Install Python Libraries for Hugging Face and PyTorch

Source: https://docs.runpod.io/tutorials/sdks/python/102/huggingface-models

Installs the 'torch' and 'transformers' Python libraries. 'torch' is essential for running machine learning models, and 'transformers' provides access to pre-trained models from Hugging Face.

```bash
pip install torch transformers
```

--------------------------------

### DataCenterRegion Enum Example

Source: https://docs.runpod.io/references/graphql-spec

An example of the DataCenterRegion enum, specifying geographical regions for data centers.

```json
"NORTH_AMERICA"
```

--------------------------------

### Example Serverless Endpoint Response

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

An example of the JSON output received when testing a deployed Serverless endpoint. It includes details like delay time, execution time, and the processed output.

```json
{
    "delayTime": 12345, 
    "executionTime": 3050, 
    "id": "some-unique-id",
    "output": {
        "output": "Processed prompt: 'Hello Serverless World!' after 3s in Serverless mode."
    },
    "status": "COMPLETED"
}
```

--------------------------------

### Display Docker Version (CLI)

Source: https://docs.runpod.io/tutorials/introduction/containers

This command displays the client and server versions of the Docker installation. It's a fundamental check to ensure Docker is running correctly.

```bash
docker version
```

--------------------------------

### CpuType Object Example

Source: https://docs.runpod.io/references/graphql-spec

An example of the CpuType object, detailing CPU specifications such as ID, display name, manufacturer, core count, and threads per core.

```json
{
  "id": "abc123",
  "displayName": "abc123",
  "manufacturer": "abc123",
  "cores": 987,
  "threadsPerCore": 987,
  "groupId": "abc123"
}
```

--------------------------------

### Configure Container Startup: Run Application After Services (run.sh)

Source: https://docs.runpod.io/pods/templates/create-custom-template

A bash script designed to be executed after base services start. It launches Jupyter/SSH in the background, waits briefly, then executes the main Python application. Finally, it waits for all background processes to complete.

```bash
#!/bin/bash
# Start base image services (Jupyter/SSH) in background
/start.sh &

# Wait for services to start
sleep 2

# Run your application
python /app/main.py

# Wait for background processes
wait
```

--------------------------------

### Install SkyPilot with Runpod Cloud Support

Source: https://docs.runpod.io/integrations/skypilot

Installs the SkyPilot framework with specific support for the Runpod cloud provider. This enables SkyPilot to manage resources on Runpod.

```shell
pip install "skypilot-nightly[runpod]"
```

--------------------------------

### Upload Large File using Helper Script

Source: https://docs.runpod.io/storage/s3-api

This command-line example demonstrates how to use a helper script for uploading very large files (10GB+) to RunPod network volumes. It automatically handles timeouts and retries, improving reliability. You need to provide the local file path, network volume ID, AWS access key, secret key, S3 endpoint, and region.

```bash
./upload_large_file.py --file /path/to/large/file.mp4 \
     --bucket NETWORK_VOLUME_ID \
     --access_key YOUR_ACCESS_KEY_ID \
     --secret_key YOUR_SECRET_ACCESS_KEY \
     --endpoint https://s3api-eur-is-1.runpod.io/ \
     --region EUR-IS-1
```

--------------------------------

### Initialize dstack (Bash)

Source: https://docs.runpod.io/integrations/dstack

Initializes dstack in the current directory where the '.dstack.yml' file is located. This prepares dstack for applying the configuration.

```bash
dstack init
```

--------------------------------

### Get Pods using runpodctl

Source: https://docs.runpod.io/runpodctl/reference/runpodctl-get-pod

This command retrieves information about your RunPod pods. You can list all pods or get details for a specific pod by providing its ID. The `--allfields` flag can be used to display all available information for each pod.

```sh
runpodctl get pod <podId> [flags]
runpodctl get pod --allfields
```

--------------------------------

### CreateClusterInput Object Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows the CreateClusterInput object, used for defining parameters when creating a new cluster. It includes details like cluster name, GPU type, pod count, and various configuration options.

```json
{
  "clusterName": "xyz789",
  "gpuTypeId": "xyz789",
  "podCount": 987,
  "gpuCountPerPod": 987,
  "type": "APPLICATION",
  "templateId": "xyz789",
  "deployCost": 987.65,
  "networkVolumeId": "xyz789",
  "volumeInGb": 123,
  "throughput": 987,
  "allowedCudaVersions": ["xyz789"],
  "minCudaVersion": "xyz789",
  "volumeKey": "xyz789",
  "dataCenterId": "abc123",
  "startJupyter": false,
  "startSsh": true,
  "containerDiskInGb": 123,
  "imageName": "abc123",
  "dockerArgs": "xyz789",
  "env": [EnvironmentVariableInput],
  "volumeMountPath": "xyz789",
  "ports": "xyz789"
}
```

--------------------------------

### Configure Container Startup: Run Application After Services (Dockerfile)

Source: https://docs.runpod.io/pods/templates/create-custom-template

This Dockerfile configuration ensures that base image services (like Jupyter and SSH) start in the background, followed by the execution of a custom startup script. This is useful for running applications alongside existing services.

```dockerfile
# Run application after services start
COPY run.sh /app/run.sh
RUN chmod +x /app/run.sh
CMD ["/app/run.sh"]
```

--------------------------------

### StorageType Enum Value Example

Source: https://docs.runpod.io/references/graphql-spec

An example of a value from the StorageType enumeration, specifying the type of storage, such as 'MOOSE_FS'.

```json
"MOOSE_FS"
```

--------------------------------

### Launch Interactive Mode for Storage Management

Source: https://docs.runpod.io/community-solutions/runpod-network-volume-storage-tool

Starts the interactive, menu-driven interface for the Runpod storage tool. This mode allows users to manage volumes, upload/download files, and browse the file system interactively.

```bash
uv run runpod-storage interactive
```

--------------------------------

### Initialize OpenAI Client for Runpod

Source: https://docs.runpod.io/tutorials/serverless/run-gemma-7b

This snippet shows how to initialize the OpenAI client with your Runpod Serverless Endpoint's base URL and API key.

```APIDOC
## Initialize OpenAI Client for Runpod

### Description
Initialize the `OpenAI` client to communicate with your Runpod Serverless Endpoint. This requires setting the `base_url` to your endpoint's specific URL and providing your Runpod API key.

### Method
Python (using `openai` library)

### Endpoint
N/A (Client initialization)

### Parameters
#### Environment Variables
- **RUNPOD_BASE_URL** (string) - Required - The base URL for your Serverless Endpoint, formatted as `https://api.runpod.ai/v2/${RUNPOD_ENDPOINT_ID}/openai/v1`.
- **RUNPOD_API_KEY** (string) - Required - Your Runpod API key.

### Request Example
```python
from openai import OpenAI
import os

client = OpenAI(
    base_url=os.environ.get("RUNPOD_BASE_URL"),
    api_key=os.environ.get("RUNPOD_API_KEY"),
)
```

### Response
N/A (Client initialization does not return a response in this context)

### Error Handling
- Ensure environment variables `RUNPOD_BASE_URL` and `RUNPOD_API_KEY` are correctly set.
```

--------------------------------

### Example Container Disk Size Configuration

Source: https://docs.runpod.io/sdks/graphql/configurations

Specifies the size of the disk allocated for the container in gigabytes. The `containerDiskInGb` argument determines the space available for the OS, applications, and data.

```json
{
  "containerDiskInGb": 10
}
```

--------------------------------

### Python SDK for Runpod Storage Management

Source: https://docs.runpod.io/community-solutions/runpod-network-volume-storage-tool

Provides examples of using the Python SDK to programmatically manage Runpod network volumes. It covers listing volumes, creating volumes, and uploading files/directories with progress tracking.

```python
from runpod_storage import RunpodStorageAPI

api = RunpodStorageAPI()

# List volumes
volumes = api.list_volumes()

# Create volume
volume = api.create_volume(
    name="ml-datasets",
    size=100,
    datacenter="EU-RO-1"
)

# Upload with automatic chunk size optimization
api.upload_file("data.csv", volume_id, "datasets/data.csv")

# Upload directory with progress tracking
def progress_callback(current, total, filename):
    percent = (current / total) * 100
    print(f"[{current}/{total}] {percent:.1f}% - Uploading: {filename}")

api.upload_directory(
    "my_project/",
    volume_id,
    "projects/my_project/",
    progress_callback=progress_callback
)
```

--------------------------------

### Basic Dockerfile for Runpod Serverless Worker

Source: https://docs.runpod.io/serverless/workers/create-dockerfile

A fundamental Dockerfile that sets up a Python environment, installs dependencies, copies handler code, and defines the startup command.

```dockerfile
FROM python:3.11.1-slim

WORKDIR /

# Copy and install requirements
COPY builder/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy your handler code
COPY src/handler.py .

# Command to run when the container starts
CMD ["python", "-u", "/handler.py"]
```

--------------------------------

### Create Python Virtual Environment (Windows)

Source: https://docs.runpod.io/sdks/python/overview

Creates a Python virtual environment named 'env' and activates it on Windows. This isolates project dependencies. Requires Python and the venv module.

```bash
python -m venv env
env\Scripts\activate
```

--------------------------------

### DataCenterStorage Object Example

Source: https://docs.runpod.io/references/graphql-spec

This snippet shows an example of the DataCenterStorage object, which holds storage-related information for a data center, including hostname, IPs, credentials, and storage lists.

```json
{
  "hostname": "string",
  "ips": ["String"],
  "pw": "String",
  "type": "String",
  "user": "String",
  "list": [DataCenterStorageList]
}
```

--------------------------------

### Create Project Directory and Files (Bash)

Source: https://docs.runpod.io/pods/templates/create-custom-template

This snippet demonstrates the bash commands to create a new directory for your custom Pod template project and initialize the necessary files: Dockerfile, requirements.txt, and main.py.

```bash
mkdir my-custom-pod-template
cd my-custom-pod-template
touch Dockerfile requirements.txt main.py
```

--------------------------------

### GpuAvailabilityInput JSON Example

Source: https://docs.runpod.io/references/graphql-spec

This example demonstrates the GpuAvailabilityInput object, used to query for available GPUs based on specific criteria. It allows specifying GPU count, disk space, memory, vCPU count, and CUDA version requirements.

```json
{
  "gpuCount": 123,
  "minDisk": 123,
  "minMemoryInGb": 123,
  "minVcpuCount": 987,
  "secureCloud": false,
  "allowedCudaVersions": ["xyz789"],
  "minCudaVersion": "xyz789",
  "includeAiApi": false
}
```

--------------------------------

### HubRelease Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object representing a Hub Release. It includes details about the release, author, deployment information, and associated build and listing data.

```json
{
  "id": "4",
  "name": "xyz789",
  "tagName": "abc123",
  "createdAt": "2007-12-03T10:15:30Z",
  "releasedAt": "2007-12-03T10:15:30Z",
  "updatedAt": "2007-12-03T10:15:30Z",
  "authorName": "abc123",
  "authorId": "abc123",
  "authorAvatarUrl": "xyz789",
  "iconUrl": "xyz789",
  "body": "xyz789",
  "readme": "abc123",
  "branch": "abc123",
  "license": "xyz789",
  "deploys": 987,
  "config": "abc123",
  "build": GitBuild,
  "listing": Listing
}
```

--------------------------------

### Upload File to Runpod Network Volume using Boto3 (Python)

Source: https://docs.runpod.io/storage/s3-api

This script uploads a local file to a Runpod network volume, which acts as an S3 bucket. It requires AWS credentials to be set as environment variables and takes the network volume ID, S3 endpoint URL, local file path, and desired object name as command-line arguments. Ensure the Boto3 library is installed (`pip install boto3`).

```python
#!/usr/bin/env python3

import os
import argparse
import boto3 # AWS SDK for Python, used to interact with Runpod S3-compatible APIs

def create_s3_client(region: str, endpoint_url: str):

    # Creates and returns an S3 client configured for Runpod network volume S3-compatible API.
    #
    # Args:
    #   region (str): The Runpod datacenter ID, used as the AWS region
    #                 (e.g., 'ca-qc-1').
    #   endpoint_url (str): The S3 endpoint URL for the specific Runpod datacenter
    #                       (e.g., 'https://ca-qc-1-s3api.runpod.io/').

    # Returns:
    #   boto3.client: An S3 client object, configured for the Runpod S3 API.

    # Retrieve Runpod S3 API key credentials from environment variables.
    aws_access_key_id = os.environ.get("AWS_ACCESS_KEY_ID")
    aws_secret_access_key = os.environ.get("AWS_SECRET_ACCESS_KEY")

    # Ensure necessary S3 API key credentials are set in the environment
    if not aws_access_key_id or not aws_secret_access_key:
        raise EnvironmentError(
            "Please set AWS_ACCESS_KEY_ID (with S3 API Key Access Key) and "
            "AWS_SECRET_ACCESS_KEY (with S3 API Key Secret Access Key) environment variables. "
            "These are obtained from 'S3 API Keys' in the Runpod console settings."
        )

    # Initialize and return the S3 client for Runpod's S3-compatible API
    return boto3.client(
        "s3",
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=region, # Corresponds to the Runpod datacenter ID
        endpoint_url=endpoint_url, # Datacenter-specific S3 API endpoint
    )

def put_object(s3_client, bucket_name: str, object_name: str, file_path: str):

    # Uploads a local file to the specified Runpod network volume.
    #
    # Args:
    #   s3_client: The S3 client object (e.g., returned by create_s3_client).
    #   bucket_name (str): The ID of the target Runpod network volume.
    #   object_name (str): The desired file path for the object on the network volume.
    #   file_path (str): The local path to the file (including the filename) that will be uploaded.

    try:
        # Attempt to upload the file to the Runpod network volume.
        s3_client.upload_file(file_path, bucket_name, object_name)
        print(f"Successfully uploaded '{file_path}' to Network Volume '{bucket_name}' as '{object_name}'")
    except Exception as e:
        # Catch any exception during upload, print an error, and re-raise
        print(f"Error uploading file '{file_path}' to Network Volume '{bucket_name}' as '{object_name}': {e}")
        raise

def main():

    # Parses command-line arguments and orchestrates the file upload process
    # to a Runpod network volume.

    # Set up command-line argument parsing
    parser = argparse.ArgumentParser(
        description="Upload a file to a Runpod Network Volume using its S3-compatible API. "
                    "Requires AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY env vars to be set "
                    "with your Runpod S3 API key credentials."
    )
    parser.add_argument(
        "-b", "--bucket",
        required=True,
        help="The ID of your Runpod Network Volume (acts as the S3 bucket name)."
    )
    parser.add_argument(
        "-e", "--endpoint",
        required=True,
        help="The S3 endpoint URL for your Runpod datacenter (e.g., 'https://s3api-DATACENTER.runpod.io/')."
    )
    parser.add_argument(
        "-f", "--file",
        required=True,
        help="The local path to the file to be uploaded."
    )
    parser.add_argument(
        "-o", "--object",
        required=True,
        help="The desired path and filename for the object on the Runpod Network Volume."
    )

    args = parser.parse_args()

    # Extract region from endpoint URL (e.g., 'ca-qc-1' from 'https://ca-qc-1-s3api.runpod.io/')
    # This assumes a consistent endpoint URL format.
    region = args.endpoint.split('//')[1].split('-s3api')[0]

    try:
        # Create an S3 client configured for Runpod
        s3_client = create_s3_client(region, args.endpoint)

        # Upload the file using the created S3 client
        put_object(s3_client, args.bucket, args.object, args.file)

    except EnvironmentError as env_err:
        print(f"Configuration Error: {env_err}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    main()

```

--------------------------------

### Apply dstack Configuration (Bash)

Source: https://docs.runpod.io/integrations/dstack

Deploys the task defined in the '.dstack.yml' file. It prompts for confirmation before proceeding with the provisioning and deployment process on RunPod.

```bash
dstack apply
```

--------------------------------

### Create Multiple Pods with runpodctl

Source: https://docs.runpod.io/runpodctl/reference/runpodctl-create-pods

This command demonstrates how to create multiple identical Pods with specified configurations. It requires the `runpodctl` CLI tool. The example shows creating 3 Pods with a specific name, GPU type, count, and container image in the Secure Cloud.

```shell
runpodctl create pods \
  --name "training-worker" \
  --podCount 3 \
  --gpuType "NVIDIA GeForce RTX 3090" \
  --gpuCount 1 \
  --secureCloud \
  --imageName "runpod/pytorch:2.0.1-py3.10-cuda11.8.0-devel"
```

--------------------------------

### GpuTypeFilter Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object for filtering GPU types. It allows filtering by a single ID, multiple IDs, or by cluster status.

```json
{
  "id": "xyz789",
  "ids": ["xyz789"],
  "cluster": false
}
```

--------------------------------

### JSON Scalar Type Example

Source: https://docs.runpod.io/references/graphql-spec

Example of the JSON scalar type, which represents arbitrary JSON values.

```json
{}
```

--------------------------------

### GpuLowestPriceInput JSON Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a GpuLowestPriceInput object, used to find the lowest price for GPUs based on various parameters. It includes country code, data center ID, GPU count, and various resource constraints like disk, memory, and network.

```json
{
  "countryCode": "xyz789",
  "dataCenterId": "xyz789",
  "gpuCount": 123,
  "includeAiApi": false,
  "minDisk": 987,
  "minDownload": 123,
  "minMemoryInGb": 987,
  "minUpload": 987,
  "minVcpuCount": 123,
  "secureCloud": false,
  "supportPublicIp": false,
  "totalDisk": 987,
  "cudaVersion": "xyz789",
  "allowedCudaVersions": ["xyz789"],
  "minCudaVersion": "abc123",
  "compliance": ["GDPR"],
  "isCluster": false,
  "podCount": 987,
  "throughput": 123,
  "globalNetwork": true
}
```

--------------------------------

### Start On-Demand Pod with cURL and GraphQL

Source: https://docs.runpod.io/sdks/graphql/manage-pods

Initiates an on-demand pod using the RunPod API. Requires an API key and specifies pod ID, GPU count. The GraphQL mutation is used to define the action, and cURL is used to send the request. The output is a JSON object confirming the pod's status.

```bash
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { podResume( input: { podId: \"inzk6tzuz833h5\", gpuCount: 1 } ) { id desiredStatus imageName env machineId machine { podHostId } } }"}'
```

```graphql
mutation {
  podResume(input: {podId: "inzk6tzuz833h5", gpuCount: 1}) {
    id
    desiredStatus
    imageName
    env
    machineId
    machine {
      podHostId
    }
  }
}
```

--------------------------------

### CloudStorage Object Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object representing CloudStorage configuration. It includes the storage ID, provider, region, bucket name, status, and creation/update timestamps. The provider and status are enums.

```json
{
  "id": 4,
  "provider": "AWS",
  "region": "xyz789",
  "bucketName": "xyz789",
  "status": "ACTIVE",
  "createdAt": "xyz789",
  "updatedAt": "abc123"
}
```

--------------------------------

### Example JSON Output from RunPod Endpoint Request

Source: https://docs.runpod.io/tutorials/serverless/generate-sdxl-turbo

This is an example of the JSON response you can expect after sending a request to a RunPod endpoint. It includes details about the request's execution, such as delay time, execution time, a unique job ID, the output (often base64 encoded), and the completion status.

```json
{
  "delayTime": 168,
  "executionTime": 251,
  "id": "sync-fa542d19-92b2-47d0-8e58-c01878f0365d-u1",
  "output": "BASE_64",
  "status": "COMPLETED"
}
```

--------------------------------

### Run Sample Application in Container

Source: https://docs.runpod.io/pods/templates/create-custom-template

Executes the sample Python application (`main.py`) within the running Docker container. This step verifies that the application runs as expected, including model loading and inference. Press `Ctrl+C` to stop the application.

```bash
python main.py
```

--------------------------------

### Synchronous Job Response Example

Source: https://docs.runpod.io/serverless/endpoints/operations

An example of the response received after submitting a synchronous job. It includes details like delay time, execution time, job ID, output, and the final status of the job.

```json
{
  "delayTime": 824,
  "executionTime": 3391,
  "id": "sync-79164ff4-d212-44bc-9fe3-389e199a5c15",
  "output": [
    {
      "image": "https://image.url",
      "seed": 46578
    }
  ],
  "status": "COMPLETED"
}

```

--------------------------------

### String Scalar Type Example

Source: https://docs.runpod.io/references/graphql-spec

This example demonstrates the usage of the String scalar type, which represents textual data in UTF-8 format.

```json
"xyz789"
```

--------------------------------

### CpuFlavor Object Example

Source: https://docs.runpod.io/references/graphql-spec

This example illustrates the CpuFlavor object, which defines characteristics of CPU types, including vCPU counts, RAM multiplier, and disk limits. It also includes a placeholder for Specifics.

```json
{
  "id": "xyz789",
  "groupId": "abc123",
  "groupName": "abc123",
  "displayName": "abc123",
  "minVcpu": 123.45,
  "maxVcpu": 987,
  "vcpuBurstable": true,
  "ramMultiplier": 123.45,
  "diskLimitPerVcpu": 987,
  "specifics": Specifics
}
```

--------------------------------

### Command Line Operations for Runpod Storage

Source: https://docs.runpod.io/community-solutions/runpod-network-volume-storage-tool

Demonstrates common command-line operations for managing Runpod network volumes, including listing, creating, uploading files/directories, and downloading files.

```bash
# List volumes
uv run runpod-storage list-volumes

# Create a volume
uv run runpod-storage create-volume --name "my-storage" --size 50 --datacenter EU-RO-1

# Upload files
uv run runpod-storage upload /path/to/file.txt volume-id
uv run runpod-storage upload /path/to/directory volume-id

# Download files
uv run runpod-storage download volume-id remote/file.txt
```

--------------------------------

### Build Docker Image for Runpod Deployment (Bash)

Source: https://docs.runpod.io/serverless/workers/custom-worker

Builds a Docker image for the Serverless worker, specifying the platform as 'linux/amd64' for Runpod deployment. Replace '[YOUR_USERNAME]' with your Docker Hub username.

```bash
docker build --platform linux/amd64 --tag [YOUR_USERNAME]/serverless-test .

```

--------------------------------

### Serve RunPod API Locally

Source: https://docs.runpod.io/serverless/development/local-testing

Starts the local API server for RunPod. The `--rp_api_host` flag can be used to specify the hostname, with '0.0.0.0' allowing network access. Be cautious with security implications when exposing the API.

```sh
python handler.py --rp_serve_api --rp_api_host 0.0.0.0
```

--------------------------------

### SavingsPlanInput Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents the input parameters for creating or updating a savings plan. It includes essential fields like planLength and upfrontCost.

```json
{
  "planLength": "abc123",
  "upfrontCost": 987.65
}
```

--------------------------------

### WorkerState Example (JSON)

Source: https://docs.runpod.io/references/graphql-spec

An example of the WorkerState object, detailing the current status of a worker, including timestamps and counts for different states like initializing, idle, running, throttled, and unhealthy.

```json
{
  "time": "2007-12-03T10:15:30Z",
  "initializing": 987,
  "idle": 123,
  "running": 987,
  "throttled": 987,
  "unhealthy": 987
}
```

--------------------------------

### Example Pod Data Structure

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents an example of a pod's data structure, including details about its cost, GPU information, disk specifications, and availability. It serves as a reference for pod resource configurations.

```json
{
  "id": "abc123",
  "costPerHr": 123.45,
  "currentPricePerGpu": 123.45,
  "diskMBps": 987,
  "gpuAvailable": 987,
  "gpuDisplayName": "xyz789",
  "gpuTypeId": "abc123",
  "gpuType": GpuType,
  "listed": false,
  "location": "xyz789",
  "machineType": "xyz789",
  "maintenanceEnd": "2007-12-03T10:15:30Z",
  "maintenanceNote": "abc123",
  "maintenanceStart": "2007-12-03T10:15:30Z",
  "maxDownloadSpeedMbps": 987,
  "maxUploadSpeedMbps": 123,
  "note": "abc123",
  "podHostId": "xyz789",
  "runpodIp": "xyz789",
  "secureCloud": true,
  "supportPublicIp": true,
  "minPodGpuCount": 123,
  "machineSystem": MachineSystem,
  "dataCenterId": "abc123",
  "cpuTypeId": "abc123",
  "cpuType": CpuType,
  "cpuCount": 123,
  "vcpuTotal": 987,
  "vcpuReserved": 987.65,
  "memoryTotal": 987,
  "memoryReserved": 987.65,
  "globalNetwork": false
}
```

--------------------------------

### Run Endpoint Synchronously with Go SDK

Source: https://docs.runpod.io/sdks/go/endpoints

This Go code demonstrates how to execute an endpoint synchronously using the Runpod Go SDK. It initializes the endpoint configuration, defines the job input including a prompt, and calls the `RunSync` method. The output, including job status and results, is then printed.

```go
package main

import (
	"encoding/json"
	"fmt"
	"log"
	"os"

	"github.com/runpod/go-sdk/pkg/sdk"
	"github.com.runpod/go-sdk/pkg/sdk/config"
	rpEndpoint "github.com/runpod/go-sdk/pkg/sdk/endpoint"
)

func main() {
	apiKey := os.Getenv("RUNPOD_API_KEY")
	baseURL := os.Getenv("RUNPOD_BASE_URL")

	endpoint, err := rpEndpoint.New(
		&config.Config{ApiKey: &apiKey},
		&rpEndpoint.Option{EndpointId: &baseURL},
	)
	if err != nil {
		log.Fatalf("Failed to create endpoint: %v", err)
	}

	jobInput := rpEndpoint.RunSyncInput{
		JobInput: &rpEndpoint.JobInput{
			Input: map[string]interface{}{
				"prompt": "Hello World",
			},
		},
		Timeout: sdk.Int(120),
	}

	output, err := endpoint.RunSync(&jobInput)
	if err != nil {
		panic(err)
	}

	data, _ := json.Marshal(output)
	fmt.Printf("output: %s\n", data)
}
```

--------------------------------

### ClusterInput Object Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a ClusterInput object, which is used for specifying a cluster by its ID. It contains a single required field, 'id', which is a string.

```json
{"id": "abc123"}
```

--------------------------------

### Get Pods

Source: https://docs.runpod.io/runpodctl/reference/runpodctl-get-pod

List all your Pods or retrieve details about a specific Pod by its ID. You can also include all fields for complete Pod information.

```APIDOC
## GET /pod

### Description
List all your Pods or retrieve details about a specific Pod by its ID. If no ID is provided, all Pods will be listed.

### Method
GET

### Endpoint
/pod

### Parameters
#### Path Parameters
- **podId** (string) - Optional - The ID of a specific Pod to retrieve.

#### Query Parameters
- **allfields** (boolean) - Optional - Include all available fields in the output, providing complete Pod information.

### Request Example
```sh
runpodctl get pod <podId> --allfields
```

### Response
#### Success Response (200)
- **pods** (array) - A list of Pod objects, each containing detailed information about a Pod.

#### Response Example
```json
{
  "pods": [
    {
      "id": "123e4567-e89b-12d3-a456-426614174000",
      "name": "my-pod",
      "status": "Running",
      "image": "my-image:latest",
      "ports": [
        {
          "containerPort": 8000,
          "hostPort": 8000,
          "protocol": "TCP"
        }
      ],
      "gpuCount": 1,
      "gpuType": "NVIDIA RTX 3090",
      "cpuCount": 4,
      "memoryMB": 16384,
      "storageMB": 100000,
      "createdAt": "2023-10-27T10:00:00Z",
      "desiredStatus": "Running",
      "lastStatusChangeTime": "2023-10-27T10:05:00Z"
    }
  ]
}
```
```

--------------------------------

### Run FastAPI Server with Uvicorn in Python

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

This Python code snippet shows how to start a FastAPI application using Uvicorn. It retrieves the port from environment variables, logs the server startup, and runs the Uvicorn server on the specified host and port with info log level.

```python
if __name__ == "__main__":
    # Get ports from environment variables
    port = int(os.getenv("PORT", 80))
    logger.info(f"Starting vLLM server on port {port}")
    
    # If health port is different, you'd need to run a separate health server
    # For simplicity, we're using the same port here
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=port,
        log_level="info"
    )
```

--------------------------------

### Runpod Serverless Function Setup (Python)

Source: https://docs.runpod.io/tutorials/sdks/python/101/async

Configures the main execution block for the serverless function using Runpod's SDK. It specifies the handler function and enables aggregate stream returns for incremental results.

```python
async def run_test(job):
    async for item in async_generator_handler(job):
        print(json.dumps(item))

if __name__ == "__main__":
    if "--test_input" in sys.argv:
        # Code for local testing (see full example)
        pass
    else:
        runpod.serverless.start({
            "handler": async_generator_handler,
            "return_aggregate_stream": True
        })
```

--------------------------------

### ComfyUI Workflow Example

Source: https://docs.runpod.io/tutorials/serverless/comfyui

An example of a ComfyUI workflow JSON file used for image generation with the FLUX.1-dev-fp8 model. This workflow defines the steps from loading models and encoding prompts to decoding and saving the final image.

```APIDOC
## ComfyUI Workflow JSON

### Description
This JSON defines a ComfyUI workflow for image generation using the FLUX.1-dev-fp8 model. It includes nodes for prompt encoding, model loading, sampling, and image decoding/saving.

### Method
Not Applicable (This is a configuration file)

### Endpoint
Not Applicable

### Parameters
Not Applicable

### Request Example
```json
{
  "input": {
    "workflow": {
      "6": {
        "inputs": {
          "text": "a whimsical and intricate treehouse nestled in the branches of a giant, ancient cherry blossom tree, surrounded by a field of glowing flowers. A gentle stream flows nearby. Fantasy art, cinematic, volumetric lighting, epic scale.",
          "clip": ["30", 1]
        },
        "class_type": "CLIPTextEncode",
        "_meta": {
          "title": "CLIP Text Encode (Positive Prompt)"
        }
      },
      "8": {
        "inputs": {
          "samples": ["31", 0],
          "vae": ["30", 2]
        },
        "class_type": "VAEDecode",
        "_meta": {
          "title": "VAE Decode"
        }
      },
      "9": {
        "inputs": {
          "filename_prefix": "ComfyUI",
          "images": ["8", 0]
        },
        "class_type": "SaveImage",
        "_meta": {
          "title": "Save Image"
        }
      },
      "27": {
        "inputs": {
          "width": 512,
          "height": 512,
          "batch_size": 1
        },
        "class_type": "EmptySD3LatentImage",
        "_meta": {
          "title": "EmptySD3LatentImage"
        }
      },
      "30": {
        "inputs": {
          "ckpt_name": "flux1-dev-fp8.safetensors"
        },
        "class_type": "CheckpointLoaderSimple",
        "_meta": {
          "title": "Load Checkpoint"
        }
      },
      "31": {
        "inputs": {
          "seed": 243057879077961,
          "steps": 10,
          "cfg": 1,
          "sampler_name": "euler",
          "scheduler": "simple",
          "denoise": 1,
          "model": ["30", 0],
          "positive": ["35", 0],
          "negative": ["33", 0],
          "latent_image": ["27", 0]
        },
        "class_type": "KSampler",
        "_meta": {
          "title": "KSampler"
        }
      },
      "33": {
        "inputs": {
          "text": "",
          "clip": ["30", 1]
        },
        "class_type": "CLIPTextEncode",
        "_meta": {
          "title": "CLIP Text Encode (Negative Prompt)"
        }
      },
      "35": {
        "inputs": {
          "guidance": 3.5,
          "conditioning": ["6", 0]
        },
        "class_type": "FluxGuidance",
        "_meta": {
          "title": "FluxGuidance"
        }
      },
      "38": {
        "inputs": {
          "images": ["8", 0]
        },
        "class_type": "PreviewImage",
        "_meta": {
          "title": "Preview Image"
        }
      },
      "40": {
        "inputs": {
          "filename_prefix": "ComfyUI",
          "images": ["8", 0]
        },
        "class_type": "SaveImage",
        "_meta": {
          "title": "Save Image"
        }
      }
    }
  }
}
```

### Response
Not Applicable
```

--------------------------------

### Machine Fields Example

Source: https://docs.runpod.io/references/graphql-spec

Example JSON object representing machine fields, including operating system, CUDA version, disk space, IP addresses, and Docker/kernel versions.

```json
{
  "os": "xyz789",
  "cudaVersion": "abc123",
  "diskTotal": 987,
  "diskFree": 123,
  "privateIp": "xyz789",
  "dockerVersion": "xyz789",
  "kernelVersion": "xyz789",
  "publicIp": "abc123"
}
```

--------------------------------

### StripeReloadTransaction Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

An example of a StripeReloadTransaction, including transaction details like ID, completion time, receipt link, amount, and transaction medium/type.

```json
{
  "id": "xyz789",
  "mediumId": "xyz789",
  "transactionCompletedAt": "2007-12-03T10:15:30Z",
  "receiptLink": "abc123",
  "amount": 123.45,
  "medium": "STRIPE",
  "type": "RELOAD"
}
```

--------------------------------

### Text Completion Parameters

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

This section outlines all available parameters for the /completions endpoint, including their types, default values, and descriptions.

```APIDOC
## POST /completions

### Description
Generates text completions for given prompts using specified models and parameters.

### Method
POST

### Endpoint
/completions

### Parameters
#### Query Parameters
- **prompt** (string or list[str]) - Required - The prompt(s) to generate completions for.
- **model** (string) - Required - The model repo that you've deployed on your Runpod Serverless endpoint.
- **temperature** (float) - Optional - Controls the randomness of sampling. Lower values make it more deterministic, higher values make it more random. Zero means greedy sampling. (Default: 0.7)
- **top_p** (float) - Optional - Controls the cumulative probability of top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens. (Default: 1.0)
- **n** (int) - Optional - Number of output sequences to return for the given prompt. (Default: 1)
- **max_tokens** (int) - Optional - Maximum number of tokens to generate per output sequence. (Default: 16)
- **seed** (int) - Optional - Random seed to use for the generation. (Default: None)
- **stop** (string or list[str]) - Optional - String(s) that stop generation when produced. The returned output will not contain the stop strings. (Default: [])
- **stream** (bool) - Optional - Whether to stream the response. (Default: false)
- **presence_penalty** (float) - Optional - Penalizes new tokens based on whether they appear in the generated text so far. Values > 0 encourage new tokens, values < 0 encourage repetition. (Default: 0.0)
- **frequency_penalty** (float) - Optional - Penalizes new tokens based on their frequency in the generated text so far. Values > 0 encourage new tokens, values < 0 encourage repetition. (Default: 0.0)
- **logit_bias** (dict[str, float]) - Optional - Unsupported by vLLM. (Default: None)
- **user** (string) - Optional - Unsupported by vLLM. (Default: None)

### Request Example
```json
{
  "prompt": "Write a short story about a robot.",
  "model": "your-deployed-model-repo",
  "temperature": 0.8,
  "max_tokens": 50
}
```

### Response
#### Success Response (200)
- **choices** (list) - A list of completion choices.
  - **text** (string) - The generated text completion.
  - **index** (int) - The index of the choice.
  - **logprobs** (object) - Log probabilities of the generated tokens.
  - **finish_reason** (string) - The reason the generation finished (e.g., 'stop', 'length').

#### Response Example
```json
{
  "id": "cmpl-xxxxxxxxxxxxxxxxx",
  "object": "text_completion",
  "created": 1677652700,
  "model": "your-deployed-model-repo",
  "choices": [
    {
      "text": "\n\nOnce upon a time, there was a robot named Bolt.",
      "index": 0,
      "logprobs": null,
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 15,
    "total_tokens": 25
  }
}
```
```

--------------------------------

### Int Scalar Type Example

Source: https://docs.runpod.io/references/graphql-spec

Example of the Int scalar type, representing a signed whole numeric value within a specific range.

```json
123
```

--------------------------------

### EnvironmentVariableInput JSON Example

Source: https://docs.runpod.io/references/graphql-spec

This example demonstrates the structure of an EnvironmentVariableInput object, which is used to define environment variables for a RunPod instance. It includes a 'key' and 'value' field, both of which are strings.

```json
{
  "key": "xyz789",
  "value": "xyz789"
}
```

--------------------------------

### AffiliateProgramEarnings Type Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object illustrates the `AffiliateProgramEarnings` type, detailing various earning metrics within the affiliate program, including total referrals, paid referrals, and different types of earnings.

```json
{
  "totalReferrals": 123,
  "totalPaidReferrals": 123,
  "templateEarnings": 123.45,
  "referralEarnings": 123.45,
  "referralBonusEarnings": 123.45,
  "affiliateEarnings": 987.65
}
```

--------------------------------

### Example JSON Object

Source: https://docs.runpod.io/references/graphql-spec

This is an example of a JSON object, likely representing a resource or entity within the system. It includes fields like id, permissions, timestamps, and status flags.

```json
{
  "id": "abc123",
  "permissions": "abc123",
  "createdAt": "2007-12-03T10:15:30Z",
  "lastUsed": "2007-12-03T10:15:30Z",
  "name": "abc123",
  "policies": {},
  "isActive": true,
  "isActiveHumanReadableString": "abc123",
  "isLegacy": true
}
```

--------------------------------

### Start a Pod using RunPod CLI

Source: https://docs.runpod.io/pods/manage-pods

This command starts a specified RunPod instance using the `runpodctl` command-line tool. It requires the Pod ID to be set in the `RUNPOD_POD_ID` environment variable. This action resumes a stopped pod.

```bash
runpodctl start pod $RUNPOD_POD_ID
```

--------------------------------

### EnvironmentVariableInput JSON Example with Different Value

Source: https://docs.runpod.io/references/graphql-spec

This example shows another instance of an EnvironmentVariableInput object, highlighting that the 'value' field can be different from the 'key'. Both fields are required strings.

```json
{
  "key": "xyz789",
  "value": "abc123"
}
```

--------------------------------

### Create Python Virtual Environment (macOS/Linux)

Source: https://docs.runpod.io/sdks/python/overview

Creates a Python virtual environment named 'env' and activates it. This isolates project dependencies. Requires Python 3 and the venv module.

```bash
python3 -m venv env
source env/bin/activate
```

--------------------------------

### Python Request Example for Runpod Pod Deployment

Source: https://docs.runpod.io/hub/overview

This Python code snippet demonstrates how to send a POST request to a deployed Runpod Pod. It includes setting up request headers with authorization and defining the JSON payload for the prompt. This is useful for interacting with deployed AI models via their API.

```python
import requests

headers = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer YOUR_API_KEY'
}

data = {
    'input': {"prompt":"Your prompt"}
}

response = requests.post('https://POD_ID-80.proxy.runpod.net/v2/LOCAL/run', headers=headers, json=data)
```

--------------------------------

### Push Docker Image to Registry (Bash)

Source: https://docs.runpod.io/serverless/workers/custom-worker

Pushes the built Docker image to a container registry, such as Docker Hub. This makes the image available for deployment on Runpod.

```bash
docker push [YOUR_USERNAME]/serverless-test:latest

```

--------------------------------

### POST /v1/chat/completions - Chat Completions (Streaming)

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

Illustrates how to make a streaming chat completion request, receiving output incrementally for interactive applications.

```APIDOC
## POST /v1/chat/completions - Chat Completions (Streaming)

### Description
Streaming allows you to receive the model's output incrementally as it's generated, rather than waiting for the complete response. This real-time delivery enhances responsiveness, making it ideal for interactive applications like chatbots or for monitoring the progress of lengthy generation tasks.

### Method
`POST`

### Endpoint
`/v1/chat/completions`

### Parameters
#### Query Parameters
None

#### Request Body
- **model** (string) - Required - The name of the model to use for completion.
- **messages** (array) - Required - A list of message objects, each with a `role` and `content`.
- **temperature** (number) - Optional - Controls randomness. Lower values make output more focused and deterministic.
- **max_tokens** (integer) - Optional - The maximum number of tokens to generate in the completion.
- **stream** (boolean) - Required - Set to `true` to enable streaming.

### Request Example
```python
# Assuming 'client' is already initialized as shown in the 'Initialize OpenAI Client' section
MODEL_NAME = "YOUR_MODEL_NAME"  # Replace with your actual model

# Create a streaming chat completion request
stream = client.chat.completions.create(
    model=MODEL_NAME,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about stars."}
    ],
    temperature=0.7,
    max_tokens=200,
    stream=True  # Enable streaming
)

# Process the streaming response (example: print each chunk)
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```

### Response
#### Success Response (200)
Responses are streamed as Server-Sent Events (SSE). Each event contains a JSON payload representing a chunk of the completion. The structure of each chunk is similar to the non-streaming response, but partial.

#### Response Example (Partial Chunk)
```json
{
  "id": "cmpl-123abc",
  "object": "chat.completion.chunk",
  "created": 1677858242,
  "model": "mistralai/Mistral-7B-Instruct-v0.2",
  "choices": [
    {
      "delta": {
        "content": "Twinkling in the velvet night,"
      },
      "index": 0,
      "finish_reason": null
    }
  ],
  "usage": null
}
```
```

--------------------------------

### ID Scalar Type Example

Source: https://docs.runpod.io/references/graphql-spec

Example of the ID scalar type, which represents a unique identifier. It is typically a string but can be accepted as an integer input.

```json
"4"
```

--------------------------------

### ContainerRegistryAuth Object Example

Source: https://docs.runpod.io/references/graphql-spec

An example of the ContainerRegistryAuth object, used for storing authentication details for container registries. It includes fields like id, name, userId, and registryAuth.

```json
{
  "id": "xyz789",
  "name": "abc123",
  "userId": "xyz789",
  "registryAuth": "xyz789"
}
```

--------------------------------

### Configure Application Port using Environment Variable (YAML)

Source: https://docs.runpod.io/pods/configuration/expose-ports

This YAML configuration example illustrates how to dynamically set the server port using an environment variable, allowing the application to adapt to assigned symmetrical TCP ports.

```yaml
server:
  host: 0.0.0.0
  port: ${RUNPOD_TCP_PORT_70000}
```

--------------------------------

### SavingsPlan Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

An example of the SavingsPlan data structure, detailing fields like id, endTime, startTime, gpuType, and cost information. This structure is used to represent savings plans within the RunPod system.

```json
{
  "id": "4",
  "endTime": "2007-12-03T10:15:30Z",
  "startTime": "2007-12-03T10:15:30Z",
  "gpuType": GpuType,
  "podId": "abc123",
  "gpuTypeId": "xyz789",
  "pod": Pod,
  "savingsPlanType": "xyz789",
  "costPerHr": 123.45,
  "upfrontCost": 987.65,
  "planLength": "abc123",
  "createdByUserId": "xyz789",
  "source": "SELF_SERVICE",
  "createdAt": "2007-12-03T10:15:30Z",
  "dealId": "xyz789"
}
```

--------------------------------

### Model Configuration Environment Variables

Source: https://docs.runpod.io/pods/references/environment-variables

Example environment variables for configuring AI models. These allow for dynamic model loading without rebuilding containers.

```shell
MODEL_NAME=gpt-3.5-turbo
MODEL_PATH=/workspace/models
MAX_TOKENS=2048
TEMPERATURE=0.7
```

--------------------------------

### Example `config.yaml` for LLM Fine-tuning

Source: https://docs.runpod.io/fine-tune

This YAML configuration file outlines the parameters for fine-tuning a large language model using Axolotl. It includes settings for the base model, dataset, training optimization, logging, and other advanced configurations. Ensure all paths and identifiers are correctly set for your specific fine-tuning task.

```yaml
base_model: NousResearch/Meta-Llama-3.1-8B

# Model loading settings
load_in_8bit: false
load_in_4bit: false
strict: false

# Dataset configuration
datasets:
  - path: tatsu-lab/alpaca
    type: alpaca
dataset_prepared_path: last_run_prepared
val_set_size: 0.05
output_dir: ./outputs/out

# Training parameters
sequence_len: 8192
sample_packing: true
pad_to_sequence_len: true

# Weights & Biases logging (optional)
wandb_project:
wandb_entity:
wandb_watch:
wandb_name:
wandb_log_model:

# Training optimization
gradient_accumulation_steps: 8
micro_batch_size: 1
num_epochs: 1
optimizer: paged_adamw_8bit
lr_scheduler: cosine
learning_rate: 2e-5

# Additional settings
train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
early_stopping_patience:
resume_from_checkpoint:
logging_steps: 1
xformers_attention:
flash_attention: true

warmup_steps: 100
evals_per_epoch: 2
eval_table_size:
saves_per_epoch: 1
debug:
deespeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:
  pad_token: <|end_of_text|>
```

--------------------------------

### IBM Granite-4.0-H-Small Input JSON Example

Source: https://docs.runpod.io/hub/public-endpoint-reference

This JSON object demonstrates the expected input format for interacting with the IBM Granite-4.0-H-Small model. It includes a list of messages with roles (system, user) and their content, as well as sampling parameters to control the generation process.

```json
{
  "input": {
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant. Please ensure responses are professional, accurate, and safe."
      },
      {
        "role": "user",
        "content": "What is Runpod?"
      }
    ],
    "sampling_params": {
      "max_tokens": 512,
      "temperature": 0.7,
      "seed": -1,
      "top_k": -1,
      "top_p": 1
    }
  }
}
```

--------------------------------

### Pod Type Definition and Example

Source: https://docs.runpod.io/references/graphql-spec

Details the structure of a Pod resource, encompassing configuration, status, resource allocation, and associated metadata. The example provides a comprehensive view of a Pod object with various fields populated.

```json
{
  "lowestBidPriceToResume": 123.45,
  "aiApiId": "abc123",
  "apiKey": "xyz789",
  "clusterIp": "abc123",
  "clusterIdx": 123,
  "clusterCidr": "xyz789",
  "clusterRole": "SLURM_CONTROLLER",
  "consumerUserId": "abc123",
  "containerDiskInGb": 123,
  "containerRegistryAuthId": "abc123",
  "costMultiplier": 987.65,
  "costPerHr": 987.65,
  "createdAt": "2007-12-03T10:15:30Z",
  "adjustedCostPerHr": 123.45,
  "desiredStatus": "CREATED",
  "dockerArgs": "xyz789",
  "dockerId": "xyz789",
  "env": ["xyz789"],
  "gpuCount": 123,
  "gpuPowerLimitPercent": 123,
  "gpus": [Gpu],
  "id": "xyz789",
  "imageName": "xyz789",
  "lastStatusChange": "abc123",
  "locked": false,
  "machineId": "xyz789",
  "memoryInGb": 987.65,
  "name": "abc123",
  "podType": "INTERRUPTABLE",
  "port": 43612,
  "ports": "abc123",
  "registry": PodRegistry,
  "templateId": "xyz789",
  "uptimeSeconds": 123,
  "vcpuCount": 123.45,
  "version": 123,
  "volumeEncrypted": true,
  "volumeInGb": 123.45,
  "volumeKey": "xyz789",
  "volumeMountPath": "xyz789",
  "lastStartedAt": "2007-12-03T10:15:30Z",
  "cpuFlavorId": "xyz789",
  "machineType": "abc123",
  "slsVersion": 123,
  "networkVolumeId": "abc123",
  "testPod": true,
  "ideAiApiId": "abc123",
  "hubReleaseId": "abc123",
  "hubRelease": HubRelease,
  "modelVersions": [ModelVersionPodAssignment],
  "cpuFlavor": CpuFlavor,
  "runtime": PodRuntime,
  "machine": PodMachineInfo,
  "latestTelemetry": PodTelemetry,
  "endpoint": Endpoint,
  "networkVolume": NetworkVolume,
  "savingsPlans": [SavingsPlan],
  "clusterId": "xyz789",
  "ipAddress": IPAddress,
  "models": ["abc123"]
}
```

--------------------------------

### Scope Enum Value Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a possible value for the Scope enumeration, representing different access levels or roles within the system, such as 'CSR_ENGINEER'.

```json
"CSR_ENGINEER"
```

--------------------------------

### Service Configuration Environment Variables

Source: https://docs.runpod.io/pods/references/environment-variables

Example environment variables for setting up web services and APIs. They enable flexible configuration of ports, debug modes, and logging.

```shell
API_PORT=8000
DEBUG_MODE=false
LOG_LEVEL=INFO
CORS_ORIGINS=https://myapp.com,https://staging.myapp.com
```

--------------------------------

### CloudTypeEnum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string literal representing one of the possible values for the CloudTypeEnum enumeration. The enum defines types of cloud access, such as SECURE, COMMUNITY, and ALL.

```plaintext
"SECURE"
```

--------------------------------

### Compliance Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This snippet shows an example of a compliance enum value. These values represent different industry standards and regulations that RunPod IO adheres to.

```json
"GDPR"
```

--------------------------------

### Example Persistent Volume Size Configuration

Source: https://docs.runpod.io/sdks/graphql/configurations

Defines the size of an additional persistent volume in gigabytes using `volumeInGb`. This volume is crucial for data that needs to survive container restarts or redeployments.

```json
{
  "volumeInGb": 5
}
```

--------------------------------

### Secret Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

An example of the Secret data structure, used for managing sensitive information. It includes fields like id, name, createdAt, and description.

```json
{
  "id": "4",
  "name": "abc123",
  "createdAt": "2007-12-03T10:15:30Z",
  "updatedAt": "2007-12-03T10:15:30Z",
  "description": "xyz789",
  "lastRetrievedAt": "2007-12-03T10:15:30Z",
  "lastUpdatedAt": "2007-12-03T10:15:30Z"
}
```

--------------------------------

### TransactionType Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string value representing the type of transaction. The 'TransactionType' type is an enum with values such as 'RELOAD', 'CREDIT', 'DEBIT', and 'PAYOUT'.

```string
"RELOAD"
```

--------------------------------

### Incremental File Transfer Demonstration with rsync

Source: https://docs.runpod.io/pods/storage/transfer-files

This demonstrates rsync's incremental transfer capability. The first transfer shows a full copy of a file, including output details. The second transfer, with the same command, shows minimal data transfer because the file already exists and is unchanged on the destination, highlighting rsync's efficiency.

```bash
rsync -avz -e "ssh -p 43201" ~/documents/example.txt root@194.26.196.6:/root/example.txt
sending incremental file list
example.txt
             119 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=0/1)

sent 243 bytes  received 35 bytes  185.33 bytes/sec
total size is 119  speedup is 0.43
```

```bash
rsync -avz -e "ssh -p 43201" ~/documents/example.txt root@194.26.196.6:/root/example.txt
sending incremental file list

sent 120 bytes  received 12 bytes  88.00 bytes/sec
total size is 119  speedup is 0.90
```

--------------------------------

### Create Pod using runpodctl CLI

Source: https://docs.runpod.io/pods/manage-pods

This command-line snippet demonstrates how to create a new Pod using the `runpodctl` tool. It specifies parameters such as Pod name, GPU type, Docker image, container disk size, volume size, and arguments to be executed within the container. Ensure `runpodctl` is installed and configured.

```sh
runpodctl create pods \
  --name hello-world \
  --gpuType "NVIDIA A40" \
  --imageName "runpod/pytorch:3.10-2.0.0-117" \
  --containerDiskSize 10 \
  --volumeSize 100 \
  --args "bash -c 'mkdir /testdir1 && /start.sh'"
```

--------------------------------

### Open `config.yaml` with Nano Editor

Source: https://docs.runpod.io/fine-tune

This command opens the `config.yaml` file in the nano text editor, allowing you to review and modify the fine-tuning parameters directly within the training environment. Save your changes before proceeding with the training process.

```sh
nano config.yaml
```

--------------------------------

### BenchmarkPod Object Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object for a BenchmarkPod, containing its ID and desired status. Both fields are strings, with 'id' being a unique identifier and 'desiredStatus' indicating the intended state of the pod.

```json
{
  "id": "xyz789",
  "desiredStatus": "xyz789"
}
```

--------------------------------

### CloudStorageProvider Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string literal representing one of the possible values for the CloudStorageProvider enumeration. The enum defines supported cloud storage providers, such as AWS and CF.

```plaintext
"AWS"
```

--------------------------------

### Configure Wan 2.2 I2V 720p Model Parameters (JSON)

Source: https://docs.runpod.io/hub/public-endpoint-reference

This JSON configuration defines the input parameters for the Wan 2.2 I2V model. It includes a text prompt, an image URL, and various settings to control video generation such as inference steps, guidance scale, video size, duration, and seed for reproducibility. The negative prompt allows exclusion of specific elements.

```json
{
  "input": {
    "prompt": "cinematic shot: slow-tracking camera glides parallel to a giant white origami boat as it gently drifts down a jade-green river",
    "image": "https://image.runpod.ai/asset/alibaba/wan-2-2-i2v-720.png",
    "num_inference_steps": 30,
    "guidance": 5,
    "negative_prompt": "",
    "size": "1280*720",
    "duration": 5,
    "flow_shift": 5,
    "seed": -1,
    "enable_prompt_optimization": false,
    "enable_safety_checker": true
  }
}
```

--------------------------------

### Python FastAPI Application Setup with vLLM Engine

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

This Python script sets up a FastAPI application to serve language model inference using vLLM. It includes an asynchronous context manager for initializing and cleaning up the vLLM engine during application lifespan and defines the FastAPI app instance.

```python
from fastapi import FastAPI, HTTPException, status
from fastapi.responses import StreamingResponse, JSONResponse
from contextlib import asynccontextmanager
from typing import Optional, AsyncGenerator
import json
import logging
import os
import uvicorn
from vllm import AsyncLLMEngine
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.sampling_params import SamplingParams
from vllm.utils import random_uuid
from utils import format_chat_prompt, create_error_response
from .models import GenerationRequest, GenerationResponse, ChatCompletionRequest

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
    ]
)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(_: FastAPI):
    """Initialize the vLLM engine on startup and cleanup on shutdown"""
    # Startup
    await create_engine()
    yield
    # Shutdown cleanup
    global engine, engine_ready
    if engine:
        logger.info("Shutting down vLLM engine...")
        # vLLM AsyncLLMEngine doesn't have an explicit shutdown method,
        # but we can clean up our references
        engine = None
        engine_ready = False
        logger.info("vLLM engine shutdown complete")


app = FastAPI(title="vLLM Load Balancing Server", version="1.0.0", lifespan=lifespan)


```

--------------------------------

### SFTP Setup and Connection Details

Source: https://docs.runpod.io/community-solutions/ohmyrunpod/overview

When SFTP is chosen for file transfer, OhMyRunPod automatically configures the SSH server, sets up SSH keys, generates a password, and creates connection scripts. It provides complete connection details including server address, port, username, and password, along with platform-specific instructions for various clients.

```bash
# OhMyRunpod automatically installs and configures SSH server.
# SSH keys are set up and a secure password is generated.
# Connection scripts for your OS are created.
# Connection details are provided, including:
# - Server address
# - Port (usually 22)
# - Username
# - Password (saved to /workspace/root_password.txt)
# Platform-specific instructions for FileZilla, WinSCP, Command Line, and VS Code are also generated.
```

--------------------------------

### Choosing Python Slim Base Image

Source: https://docs.runpod.io/serverless/workers/create-dockerfile

Utilize slim Python base images for smaller Docker image sizes and faster download/startup times, recommended for most use cases.

```dockerfile
FROM python:3.11.1-slim
```

--------------------------------

### ClusterRole Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string literal representing one of the possible values for the ClusterRole enumeration. The enum defines roles within a cluster, such as SLURM_CONTROLLER and SLURM_COMPUTE.

```plaintext
"SLURM_CONTROLLER"
```

--------------------------------

### Start Fine-tuning Process with Axolotl

Source: https://docs.runpod.io/fine-tune

This command initiates the fine-tuning process for a large language model using the Axolotl framework. It requires a `config.yaml` file containing all the necessary training parameters. Monitor the terminal output for training progress and potential errors.

```sh
axolotl train config.yaml
```

--------------------------------

### Pre-load Models with wget in Dockerfile

Source: https://docs.runpod.io/pods/templates/create-custom-template

This snippet demonstrates how to download model files using wget directly within a Dockerfile. It creates a directory for the model and downloads configuration, model weights, and tokenizer files from Hugging Face. This method provides more control over model sourcing.

```dockerfile
# Create model directory and download files
RUN mkdir -p /app/models/distilbert-model && \
    cd /app/models/distilbert-model && \
    wget -q https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/config.json && \
    wget -q https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors && \
    wget -q https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/tokenizer_config.json && \
    wget -q https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/vocab.txt
```

--------------------------------

### AssignmentStatus Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string literal representing one of the possible values for the AssignmentStatus enumeration. The enum defines states for assignment, such as ASSIGNED, UNASSIGNED, DEPLOYED, and EVICTED.

```plaintext
"ASSIGNED"
```

--------------------------------

### Install Python packages for LLM inference

Source: https://docs.runpod.io/tutorials/pods/run-your-first

Installs the 'transformers' and 'accelerate' Python libraries required for running LLM models. These libraries are essential for loading models and optimizing their performance on the available hardware.

```bash
pip install transformers accelerate
```

--------------------------------

### Start PyTorch Process with torchrun

Source: https://docs.runpod.io/instant-clusters/pytorch

This script launches multiple PyTorch processes for distributed training on each Pod. It sets essential environment variables like NCCL_DEBUG and NCCL_SOCKET_IFNAME for efficient inter-node communication. The command uses torchrun to manage the distributed execution of the main.py script, specifying the number of processes per node, total nodes, node rank, master address, and master port.

```bash
export NCCL_DEBUG=INFO
export NCCL_SOCKET_IFNAME=ens1
torchrun \
  --nproc_per_node=$NUM_TRAINERS \
  --nnodes=$NUM_NODES \
  --node_rank=$NODE_RANK \
  --master_addr=$MASTER_ADDR \
  --master_port=$MASTER_PORT \
torch-demo/main.py
```

--------------------------------

### Verify Model Response Structure

Source: https://docs.runpod.io/integrations/dstack

This is an example JSON response from the vLLM model server, confirming that the model is running and responding correctly to chat completion requests. It includes details like message content, usage tokens, and finish reason.

```json
{
  "id": "chat-f0566a5143244d34a0c64c968f03f80c",
  "object": "chat.completion",
  "created": 1727902323,
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "My name is Poddy, and I'm here to assist you with any questions or information you may need.",
        "tool_calls": []
      },
      "logprobs": null,
      "finish_reason": "stop",
      "stop_reason": null
    }
  ],
  "usage": {
    "prompt_tokens": 49,
    "total_tokens": 199,
    "completion_tokens": 150
  },
  "prompt_logprobs": null
}
```

--------------------------------

### ChecklistPath Object Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a ChecklistPath object, which contains boolean flags indicating the presence of different path types: pod, serverless, public_endpoint, and instant_cluster. These flags likely control routing or feature availability.

```json
{
  "pod": true,
  "serverless": false,
  "public_endpoint": true,
  "instant_cluster": true
}
```

--------------------------------

### Pod Creation API

Source: https://docs.runpod.io/api-reference/endpoints/POST/endpoints/endpointId/update

This section details the parameters required to create a new Pod, including specifications for GPU, disk, image, and environment variables.

```APIDOC
## POST /pods

### Description
Creates a new Pod with specified configurations.

### Method
POST

### Endpoint
/pods

### Parameters
#### Request Body
- **image** (string) - Required - The image tag for the container run on a Pod.
- **gpuCount** (integer) - Optional - The number of GPUs to allocate to the Pod. Defaults to 1 if not specified.
- **gpuType** (string) - Optional - The ID of the GPU type to use for the Pod.
- **cloud** (string) - Optional - The cloud provider to use for the Pod.
- **interruptible** (boolean) - Optional - Whether the Pod should be interruptible (cheaper but can be stopped). Defaults to false.
- **containerDiskInGb** (integer) - Optional - The amount of disk space in GB to allocate for the container disk. Defaults to 50.
- **volumeInGb** (integer) - Optional - The amount of network volume in GB to allocate for persistent storage. Defaults to 0.
- **env** (object) - Optional - Environment variables to set in the container. Example: `{"ENV_VAR": "value"}`.
- **dockerEntrypoint** (array of strings) - Optional - Overrides the Docker image's ENTRYPOINT.
- **dockerStartCmd** (array of strings) - Optional - Overrides the Docker image's start CMD.
- **desiredStatus** (string) - Optional - The desired status for the Pod (e.g., RUNNING, EXITED). 
- **consumerUserId** (string) - Optional - The Runpod user ID renting the Pod.
- **containerRegistryAuthId** (string) - Optional - The ID for container registry authentication.
- **endpointId** (string) - Optional - The ID of the associated endpoint if this is a Serverless worker.

### Request Example
```json
{
  "image": "runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04",
  "gpuCount": 1,
  "gpuType": "NVIDIA A100",
  "interruptible": false,
  "containerDiskInGb": 100,
  "volumeInGb": 200,
  "env": {
    "MY_VAR": "my_value"
  },
  "desiredStatus": "RUNNING"
}
```

### Response
#### Success Response (200)
- **id** (string) - A unique string identifying the created Pod.
- **image** (string) - The image tag used for the Pod.
- **gpuCount** (integer) - The number of GPUs allocated.
- **gpuType** (string) - The type of GPU allocated.
- **interruptible** (boolean) - Indicates if the Pod is interruptible.
- **containerDiskInGb** (integer) - The container disk size in GB.
- **volumeInGb** (integer) - The network volume size in GB.
- **env** (object) - Environment variables set for the Pod.
- **desiredStatus** (string) - The desired status of the Pod.
- **consumerUserId** (string) - The user ID renting the Pod.
- **costPerHr** (number) - The cost per hour for the Pod.

#### Response Example
```json
{
  "id": "xedezhzb9la3ye",
  "image": "runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04",
  "gpuCount": 1,
  "gpuType": "NVIDIA A100",
  "interruptible": false,
  "containerDiskInGb": 100,
  "volumeInGb": 200,
  "env": {
    "MY_VAR": "my_value"
  },
  "desiredStatus": "RUNNING",
  "consumerUserId": "user_2PyTJrLzeuwfZilRZ7JhCQDuSqo",
  "costPerHr": 0.74
}
```
```

--------------------------------

### Pod Configuration Parameters

Source: https://docs.runpod.io/api-reference/endpoints/POST/endpoints

This section details the various parameters used to configure a Pod, including its resources, storage, and runtime environment.

```APIDOC
## Pod Configuration Parameters

### Description
Details the parameters for configuring a Runpod Pod, including compute resources, disk space, container image, and runtime environment settings.

### Method
N/A (This describes a data structure, not an endpoint)

### Endpoint
N/A

### Parameters
#### Path Parameters
None

#### Query Parameters
None

#### Request Body
- **aiApiId** (string) - Optional - Synonym for endpointId (legacy name).
- **consumerUserId** (string) - Optional - A unique string identifying the Runpod user who rents a Pod.
- **containerDiskInGb** (integer) - Optional - The amount of disk space, in gigabytes (GB), to allocate on the container disk for a Pod. The data on the container disk is wiped when the Pod restarts. To persist data across Pod restarts, set volumeInGb to configure the Pod network volume.
- **containerRegistryAuthId** (string) - Optional - If a Pod is created with a container registry auth, the unique string identifying that container registry auth.
- **costPerHr** (number) - Optional - The cost in Runpod credits per hour of running a Pod. Note that the actual cost may be lower if Savings Plans are applied.
- **cpuFlavorId** (string) - Optional - If the Pod is a CPU Pod, the unique string identifying the CPU flavor the Pod is running on.
- **desiredStatus** (string) - Optional - The current expected status of a Pod. Enum: RUNNING, EXITED, TERMINATED.
- **dockerEntrypoint** (array of strings) - Optional - If specified, overrides the ENTRYPOINT for the Docker image run on the created Pod. If [], uses the ENTRYPOINT defined in the image.
- **dockerStartCmd** (array of strings) - Optional - If specified, overrides the start CMD for the Docker image run on the created Pod. If [], uses the start CMD defined in the image.
- **endpointId** (string) - Optional - If the Pod is a Serverless worker, a unique string identifying the associated endpoint.
- **env** (object) - Optional - Environment variables for the container. Example: `{"ENV_VAR": "value"}`. Default: `{}`.
- **gpu** (object) - Optional - GPU configuration for the Pod.
  - **id** (string) - Optional - GPU ID.
  - **count** (integer) - Optional - The number of GPUs attached to a Pod. Example: 1.
  - **displayName** (string) - Optional - Display name of the GPU.
  - **securePrice** (number) - Optional - Secure price for the GPU.
  - **communityPrice** (number) - Optional - Community price for the GPU.
  - **oneMonthPrice** (number) - Optional - One-month price for the GPU.
  - **threeMonthPrice** (number) - Optional - Three-month price for the GPU.
  - **sixMonthPrice** (number) - Optional - Six-month price for the GPU.
  - **oneWeekPrice** (number) - Optional - One-week price for the GPU.
  - **communitySpotPrice** (number) - Optional - Community spot price for the GPU.
  - **secureSpotPrice** (number) - Optional - Secure spot price for the GPU.
- **id** (string) - Optional - A unique string identifying a Pod.
- **image** (string) - Required - The image tag for the container run on a Pod. Example: `runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04`.
- **interruptible** (boolean) - Optional - Describes how a Pod is rented. An interruptible Pod can be rented at a lower cost but can be stopped at any time to free up resources for another Pod. A reserved Pod is rented at a higher cost but runs until it exits or is manually stopped. Example: `false`.
- **lastStartedAt** (string) - Optional - The UTC timestamp when a Pod was last started.
- **lastStatusChange** (string) - Optional - A string describing the last lifecycle event on a Pod.
- **locked** (boolean) - Optional - Set to true to lock a Pod. Locking a Pod disables stopping or resetting the Pod. Example: `false`.
- **machine** (object) - Optional - Machine configuration details.
  - **minPodGpuCount** (integer) - Optional - Minimum GPU count for the machine.
  - **gpuTypeId** (string) - Optional - GPU type ID.
  - **gpuType** (object) - Optional - GPU type details.
    - **id** (string) - Optional - GPU type ID.

### Request Example
```json
{
  "image": "runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04",
  "gpu": {
    "count": 1
  },
  "containerDiskInGb": 50,
  "env": {
    "MY_VAR": "my_value"
  }
}
```

### Response
#### Success Response (200)
- **id** (string) - The unique identifier of the created Pod.
- **desiredStatus** (string) - The desired status of the Pod.
- **image** (string) - The container image used for the Pod.
- **costPerHr** (number) - The hourly cost of the Pod.

#### Response Example
```json
{
  "id": "pod_abcdef123456",
  "desiredStatus": "RUNNING",
  "image": "runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04",
  "costPerHr": 0.74
}
```
```

--------------------------------

### ClusterType Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string literal representing one of the possible values for the ClusterType enumeration. The enum defines the types of clusters, such as APPLICATION, TRAINING, and SLURM.

```plaintext
"APPLICATION"
```

--------------------------------

### CloudStorageStatus Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string literal representing one of the possible values for the CloudStorageStatus enumeration. The enum defines the possible states for cloud storage, such as ACTIVE and INACTIVE.

```plaintext
"ACTIVE"
```

--------------------------------

### Set up Runpod Serverless Main Execution Block

Source: https://docs.runpod.io/tutorials/sdks/python/101/generator

Configures the main execution block for a Runpod serverless function. It allows for local testing via command-line arguments and starts the serverless endpoint using the defined streaming handler.

```python
if __name__ == "__main__":
    if "--test_input" in sys.argv:
        # Code for local testing (see full example)
        pass
    else:
        runpod.serverless.start({"handler": streaming_handler, "return_aggregate_stream": True})
```

--------------------------------

### Card Object Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object representing credit card details. It includes the card's brand, the last four digits, and expiration month and year. The fields are strings and integers.

```json
{
  "brand": "abc123",
  "last4": "xyz789",
  "expMonth": 987,
  "expYear": 123
}
```

--------------------------------

### Configure Wan 2.1 I2V 720p Image-to-Video Generation (JSON)

Source: https://docs.runpod.io/hub/public-endpoint-reference

This JSON configuration defines the parameters for the Wan 2.1 I2V model to generate a 720p video from a given image. It includes the input prompt, image URL, and various generation settings like inference steps, guidance scale, and video duration. Ensure the 'image' URL is accessible and the 'prompt' accurately describes the desired video content.

```json
{
  "input": {
    "prompt": "The family of three just took a selfie. They lean in together, smiling and relaxed. The daughter holds the phone and shows the screen",
    "image": "https://image.runpod.ai/asset/alibaba/wan-2-1-i2v-720.png",
    "num_inference_steps": 30,
    "guidance": 5,
    "negative_prompt": "",
    "size": "1280*720",
    "duration": 5,
    "flow_shift": 5,
    "seed": -1,
    "enable_prompt_optimization": false,
    "enable_safety_checker": true
  }
}
```

--------------------------------

### Get GPUs

Source: https://docs.runpod.io/sdks/python/apis

Lists all GPUs that can be allocated to endpoints in Runpod, enabling optimal resource selection based on computational needs.

```APIDOC
## GET /gpus

### Description
Fetches a list of all available GPUs that can be allocated to endpoints.

### Method
GET

### Endpoint
/gpus

### Parameters
#### Query Parameters
None

#### Request Body
None

### Request Example
None

### Response
#### Success Response (200)
- **id** (string) - Unique identifier for the GPU model.
- **displayName** (string) - User-friendly name for the GPU.
- **memoryInGb** (integer) - Amount of memory in GB.

#### Response Example
```json
[
  {
    "id": "NVIDIA A100 80GB PCIe",
    "displayName": "A100 80GB",
    "memoryInGb": 80
  },
  {
    "id": "NVIDIA A100-SXM4-80GB",
    "displayName": "A100 SXM 80GB",
    "memoryInGb": 80
  }
]
```
```

--------------------------------

### Development vs. Production Settings Environment Variables

Source: https://docs.runpod.io/pods/references/environment-variables

Example environment variables used to differentiate configurations between development and production environments. This allows for distinct settings like caching and rate limiting.

```shell
ENVIRONMENT=production
CACHE_ENABLED=true
RATE_LIMIT=1000
MONITORING_ENABLED=true
```

--------------------------------

### POST /pods

Source: https://docs.runpod.io/api-reference/pods/POST/pods

Creates a new Pod and optionally deploys it. This endpoint allows for the creation of both GPU and CPU pods with various configuration options.

```APIDOC
## POST /pods

### Description
Creates a new Pod and optionally deploys it. This endpoint allows for the creation of both GPU and CPU pods with various configuration options.

### Method
POST

### Endpoint
/pods

### Parameters
#### Request Body
- **allowedCudaVersions** (array[string]) - Optional - If the created Pod is a GPU Pod, a list of acceptable CUDA versions. If not set, any CUDA version is acceptable.
- **cloudType** (string) - Optional - Set to SECURE to create the Pod in Secure Cloud. Set to COMMUNITY to create the Pod in Community Cloud.
- **computeType** (string) - Optional - Set to GPU to create a GPU Pod. Set to CPU to create a CPU Pod.
- **containerDiskInGb** (integer) - Optional - The amount of disk space, in gigabytes (GB), to allocate on the container disk.
- **containerRegistryAuthId** (string) - Optional - Registry credentials ID.
- **countryCodes** (array[string]) - Optional - A list of country codes where the created Pod can be located.
- **cpuFlavorIds** (array[string]) - Optional - If the created Pod is a CPU Pod, a list of Runpod CPU flavors.
- **cpuFlavorPriority** (string) - Optional - If the created Pod is a CPU Pod, set to availability to respond to current CPU flavor availability. Set to custom to always try to rent CPU flavors in the order specified in cpuFlavorIds.

### Request Example
```json
{
  "allowedCudaVersions": [
    "12.1"
  ],
  "cloudType": "SECURE",
  "computeType": "GPU",
  "containerDiskInGb": 100,
  "containerRegistryAuthId": "clzdaifot0001l90809257ynb",
  "countryCodes": [
    "US",
    "CA"
  ],
  "cpuFlavorIds": [
    "cpu3c",
    "cpu3g"
  ],
  "cpuFlavorPriority": "availability"
}
```

### Response
#### Success Response (201)
- **Pod** (object) - Description of the created Pod object (details omitted for brevity, refer to OpenAPI schema for full details).

#### Response Example
```json
{
  "id": "pod-12345",
  "name": "My New Pod",
  "templateId": "tpl-abcde",
  "state": "RUNNING",
  "desiredState": "RUNNING",
  "ports": [
    {
      "containerPort": 80,
      "hostPort": 80,
      "protocol": "TCP"
    }
  ],
  "gpuType": {
    "id": "NVIDIA-RTX-3090",
    "name": "NVIDIA RTX 3090"
  },
  "gpuCount": 1,
  "cpuCount": 4,
  "ramInGb": 16,
  "diskInGb": 100,
  "createdAt": "2023-10-27T10:00:00Z"
}
```
```

--------------------------------

### Create Pod using Runpod REST API

Source: https://docs.runpod.io/pods/manage-pods

This example shows how to create a Pod programmatically using the Runpod REST API. It involves sending a POST request to the `/pods` endpoint with a JSON payload detailing the Pod's configuration, including CUDA versions, cloud type, compute type, disk sizes, and CPU flavor. Replace `RUNPOD_API_KEY` with your actual API key.

```bash
curl --request POST \
  --url https://rest.runpod.io/v1/pods \
  --header 'Authorization: Bearer RUNPOD_API_KEY' \
  --header 'Content-Type: application/json' \
  --data '{
      "allowedCudaVersions": [
        "12.8"
      ],
      "cloudType": "SECURE",
      "computeType": "GPU",
      "containerDiskInGb": 50,
      "containerRegistryAuthId": "clzdaifot0001l90809257ynb",
      "countryCodes": [
        "US"
      ],
      "cpuFlavorIds": [
        "cpu3c"
      ],
      "cpuFlavorPriority": "availability"
    }'
```

--------------------------------

### MachinePoolState Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a possible value for the MachinePoolState enum, indicating the availability status of a machine within a pool. The enum defines states like GENERALLY_AVAILABLE, POOL_ASSIGNED, and POOL_AVAILABLE.

```json
"GENERALLY_AVAILABLE"
```

--------------------------------

### Start Jupyter Lab Server in Runpod

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

Starts a Jupyter Lab server accessible from any IP address on port 8888. It configures the server to run without a password, disables trash functionality, sets the default directory to WORKSPACE_DIR, and logs output to /jupyter.log. It requires the WORKSPACE_DIR environment variable to be set.

```bash
start_jupyter() {
    echo "Starting Jupyter Lab..."
    mkdir -p "$WORKSPACE_DIR" && \
    cd / && \
    nohup jupyter lab --allow-root --no-browser --port=8888 --ip=* --NotebookApp.token='' --NotebookApp.password='' --FileContentsManager.delete_to_trash=False --ServerApp.terminado_settings='{"shell_command":["/bin/bash"]}' --ServerApp.allow_origin=* --ServerApp.preferred_dir="$WORKSPACE_DIR" &> /jupyter.log &
    echo "Jupyter Lab started without a password"
}

```

--------------------------------

### RunPod User Cluster Billing GroupBy Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a possible value for the UserClusterBillingGroupBy enum. It indicates how cluster billing data can be grouped, such as by 'GPU_TYPE' or 'CLUSTER_ID', for aggregated reporting.

```json
"GPU_TYPE"
```

--------------------------------

### PaymentMethod Type Definition and Example

Source: https://docs.runpod.io/references/graphql-spec

Defines the structure for a payment method, including user association, Stripe details, card information, and status flags. The example shows a populated PaymentMethod object.

```json
{
  "id": "xyz789",
  "userId": "xyz789",
  "stripePaymentId": "xyz789",
  "type": "xyz789",
  "card": Card,
  "createdAt": "2007-12-03T10:15:30Z",
  "updatedAt": "2007-12-03T10:15:30Z",
  "deletedAt": "2007-12-03T10:15:30Z",
  "isDefault": true,
  "failedPaymentCount": 123,
  "name": "abc123",
  "priority": 123
}
```

--------------------------------

### ClientCreditCharge Object Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object for a ClientCreditCharge, detailing various charges like amount, disk, pod, API, and serverless charges, along with an update timestamp and charge type. The amount fields are floats, and the type is a ClientCreditChargeType enum.

```json
{
  "amount": 123.45,
  "updatedAt": "2007-12-03T10:15:30Z",
  "diskCharges": 987.65,
  "podCharges": 987.65,
  "apiCharges": 123.45,
  "serverlessCharges": 123.45,
  "type": "CHARGE_SERVERLESS"
}
```

--------------------------------

### Download Files from RunPod with rsync

Source: https://docs.runpod.io/pods/storage/transfer-files

This rsync command downloads a file from a RunPod instance to a local directory, using verbose output, compression, and progress. It mirrors the 'send' command but reverses the source and destination. The example downloads 'example.txt' from the Pod to the local 'documents' directory.

```bash
rsync -avz -e "ssh -p 43201" root@194.26.196.6:/root/example.txt ~/documents/example.txt
```

--------------------------------

### Cluster Object Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object representing a Cluster. It includes cluster details like ID, name, user ID, GPU type, pod count, and creation timestamp. It also contains an array of associated Pod objects.

```json
{
  "id": "xyz789",
  "name": "abc123",
  "userId": "xyz789",
  "gpuTypeId": "abc123",
  "gpuDisplayName": "xyz789",
  "podCount": 123,
  "gpuCountPerPod": 987,
  "type": "APPLICATION",
  "templateId": "abc123",
  "createdAt": "2007-12-03T10:15:30Z",
  "pods": [Pod]
}
```

--------------------------------

### Initiate OhMyRunPod File Transfer

Source: https://docs.runpod.io/community-solutions/ohmyrunpod/overview

This command starts the OhMyRunPod file transfer process. Ensure port 22 is exposed in your Runpod instance before execution. The command will then prompt you to choose a transfer method.

```bash
OhMyRunpod --file-transfer
```

--------------------------------

### Use API Keys from Environment Variables in Python

Source: https://docs.runpod.io/serverless/development/environment-variables

Shows a Python handler example for securely accessing API keys (e.g., OpenAI, Anthropic) stored as environment variables. It emphasizes the importance of not hardcoding secrets and provides a pattern for checking if keys are configured before making API calls.

```python
import os
import runpod
import requests

def handler(event):
    # Read API keys from environment
    openai_key = os.environ.get("OPENAI_API_KEY")
    anthropic_key = os.environ.get("ANTHROPIC_API_KEY")
    
    # Use them in your code
    if not openai_key:
        return {"error": "OPENAI_API_KEY not configured"}
    
    # Your API call here
    result = call_openai(openai_key, event["input"]["prompt"])
    return {"output": result}

runpod.serverless.start({"handler": handler})
```

--------------------------------

### Start Runpod Serverless Function (Python)

Source: https://docs.runpod.io/tutorials/sdks/python/101/error

Initiates the Runpod serverless function by calling `runpod.serverless.start()`. This function takes the main handler as an argument, allowing the Runpod environment to execute the defined logic when a job is received.

```python
runpod.serverless.start({"handler": handler})
```

--------------------------------

### Initiate Asynchronous Run with Runpod IO (Go)

Source: https://docs.runpod.io/sdks/go/endpoints

This Go code snippet demonstrates how to initiate an asynchronous run using the Runpod SDK. It requires API keys and base URLs to be set as environment variables. The function takes job input and a request timeout, returning an output that includes a job ID and its initial status.

```go
package main

import (
	"encoding/json"
	"fmt"
	"log"
	"os"

	"github.com/runpod/go-sdk/pkg/sdk"
	"github.com/runpod/go-sdk/pkg/sdk/config"
	rpEndpoint "github.com/runpod/go-sdk/pkg/sdk/endpoint"
)

func main() {
	apiKey := os.Getenv("RUNPOD_API_KEY")
	baseURL := os.Getenv("RUNPOD_BASE_URL")

	endpoint, err := rpEndpoint.New(
		&config.Config{ApiKey: &apiKey},
		&rpEndpoint.Option{EndpointId: &baseURL},
	)
	if err != nil {
		log.Fatalf("Failed to create endpoint: %v", err)
	}

	jobInput := rpEndpoint.RunInput{
		JobInput: &rpEndpoint.JobInput{
			Input: map[string]interface{}{
				"mock_delay": 95,
			},
		},
		RequestTimeout: sdk.Int(120),
	}

	output, err := endpoint.Run(&jobInput)
	if err != nil {
		panic(err)
	}

	data, _ := json.Marshal(output)
	fmt.Printf("output: %s\n", data)
}
```

--------------------------------

### TeamRole Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string value representing a role within a team. The 'TeamRole' type is an enum with predefined values such as 'owner', 'admin', 'member', 'dev', 'billing', and 'basic'.

```string
"owner"
```

--------------------------------

### List Available Models API

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

The /models endpoint allows you to retrieve a list of all models available on your endpoint.

```APIDOC
## GET /models

### Description
Retrieves a list of all available models that can be used with the API.

### Method
GET

### Endpoint
/models

### Parameters
This endpoint does not require any parameters.

### Request Example
```python
models_response = client.models.list()
list_of_models = [model.id for model in models_response]
print(list_of_models)
```

### Response
#### Success Response (200)
- **object** (string) - Type of the object, e.g., "list".
- **data** (array) - An array of model objects.
  - **id** (string) - The unique identifier of the model.
  - **object** (string) - Type of the object, e.g., "model".
  - **created** (integer) - Timestamp of model creation.
  - **owned_by** (string) - The entity that owns the model (e.g., "runpod").

#### Response Example
```json
{
  "object": "list",
  "data": [
    {
      "id": "mistralai/Mistral-7B-Instruct-v0.2",
      "object": "model",
      "created": 1677858242,
      "owned_by": "runpod"
    }
  ]
}
```
```

--------------------------------

### ClientCreditChargeType Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string literal representing one of the possible values for the ClientCreditChargeType enumeration. The enum defines categories for credit charges, such as serverless, pod, API, and storage.

```plaintext
"CHARGE_SERVERLESS"
```

--------------------------------

### HTML Frontend for AI Image Generator

Source: https://docs.runpod.io/tutorials/serverless/generate-sdxl-turbo

Sets up the user interface for the AI image generator. It includes an input field for the prompt, a button to trigger generation, and a div to display the resulting image. This HTML file links to an external JavaScript file for the application's logic.

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Runpod AI Image Generator</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            padding: 20px;
        }

        #imageResult {
            margin-top: 20px;
        }
    </style>
</head>
<body>
    <h1>Runpod AI Image Generator</h1>
    <input type="text" id="promptInput" placeholder="Enter your image prompt" />
    <button onclick="generateImage()">Generate Image</button>

    <div id="imageResult"></div>

    <script src="script.js"></script>
</body>
</html>
```

--------------------------------

### AuditLog Object Example

Source: https://docs.runpod.io/references/graphql-spec

An example JSON object representing an audit log entry. It includes details about the actor, resource, action, and timestamp of an event. The fields are all strings, except for the timestamp which is a DateTime.

```json
{
  "actorId": "abc123",
  "email": "xyz789",
  "ownerId": "abc123",
  "resourceType": "abc123",
  "resourceId": "abc123",
  "action": "xyz789",
  "value": "xyz789",
  "timestamp": "2007-12-03T10:15:30Z"
}
```

--------------------------------

### Synchronous Request to /runsync

Source: https://docs.runpod.io/hub/public-endpoints

Example of a synchronous API request to the /runsync endpoint for immediate results.

```APIDOC
## POST /v2/{model-name}/runsync

### Description
Executes a model synchronously, returning the result directly in the response.

### Method
POST

### Endpoint
`/v2/{model-name}/runsync`

### Parameters
#### Path Parameters
- **model-name** (string) - Required - The name of the model to run.

#### Query Parameters
None

#### Request Body
- **input** (object) - Required - The input parameters for the model.
  - **prompt** (string) - Required - The text prompt for image generation.
  - **width** (integer) - Required - The desired width of the output image.
  - **height** (integer) - Required - The desired height of the output image.
  - **num_inference_steps** (integer) - Required - The number of inference steps to perform.
  - **guidance** (float) - Required - The guidance scale for the generation.

### Request Example
```bash
curl -X POST "https://api.runpod.ai/v2/black-forest-labs-flux-1-dev/runsync" \
  -H "Authorization: Bearer RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
  "input": {
    "prompt": "A serene mountain landscape at sunset",
    "width": 1024,
    "height": 1024,
    "num_inference_steps": 20,
    "guidance": 7.5
  }
}'
```

### Response
#### Success Response (200)
- **delayTime** (integer) - The time in milliseconds the request was delayed.
- **executionTime** (integer) - The time in milliseconds the request took to execute.
- **id** (string) - The unique identifier for the job.
- **output** (object) - The result of the model execution.
  - **cost** (float) - The cost of the execution.
  - **image_url** (string) - The URL of the generated image.
- **status** (string) - The status of the job (e.g., "COMPLETED").
- **workerId** (string) - The ID of the worker that processed the request.

#### Response Example
```json
{
  "delayTime": 17,
  "executionTime": 3986,
  "id": "sync-0965434e-ff63-4a1c-a9f9-5b705f66e176-u2",
  "output": {
    "cost": 0.02097152,
    "image_url": "https://image.runpod.ai/6/6/mCwUZlep6S/453ad7b7-67c6-43a1-8348-3ad3428ef97a.png"
  },
  "status": "COMPLETED",
  "workerId": "oqk7ao1uomckye"
}
```
```

--------------------------------

### TransactionMedium Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string value representing the medium through which a transaction occurred. The 'TransactionMedium' type is an enum with values like 'STRIPE', 'CRYPTO', 'RUNPOD', 'COINBASE', 'WIRE', and 'REFERRAL'.

```string
"STRIPE"
```

--------------------------------

### SpecificsInput Example

Source: https://docs.runpod.io/references/graphql-spec

Input parameters for specifying details related to an instance and data center. It includes instanceId and dataCenterId.

```json
{
  "instanceId": "xyz789",
  "dataCenterId": "xyz789"
}
```

--------------------------------

### Build Docker Image

Source: https://docs.runpod.io/tutorials/serverless/generate-sdxl-turbo

Builds a Docker image using the Dockerfile in the current directory. Replace '<username>/<repo>:<tag>' with your desired image name and tag. This image will contain the SDXL Turbo model and the Runpod Handler.

```bash
docker build --tag <username>/<repo>:<tag> .
```

--------------------------------

### Start Runpod Serverless Worker with Sentiment Analysis Handler

Source: https://docs.runpod.io/tutorials/sdks/python/102/huggingface-models

Initializes and starts a Runpod serverless worker, specifying the 'sentiment_analysis_handler' function as the entry point for processing requests. This enables the deployment of the sentiment analysis function.

```python
runpod.serverless.start({"handler": sentiment_analysis_handler})
```

--------------------------------

### Database and External Service Connection Environment Variables

Source: https://docs.runpod.io/pods/references/environment-variables

Example environment variables for securely connecting to databases and external APIs. These often include URLs and credentials.

```shell
DATABASE_URL=postgresql://user:pass@host:5432/db
REDIS_URL=redis://localhost:6379
API_BASE_URL=https://api.external-service.com
```

--------------------------------

### UserServerlessBilling Object Example

Source: https://docs.runpod.io/references/graphql-spec

Details billing for serverless compute usage, including time, amount, duration, and associated resource identifiers. Used for analyzing serverless costs.

```json
{
  "time": "2007-12-03T10:15:30Z",
  "amount": 123.45,
  "timeBilledSeconds": 987,
  "endpointId": "xyz789",
  "gpuTypeId": "abc123",
  "instanceId": "abc123"
}
```

--------------------------------

### Advanced Endpoint Definitions (FastAPI Example)

Source: https://docs.runpod.io/serverless/load-balancing/build-a-worker

This section demonstrates how to define multiple, organized endpoints using FastAPI, including authentication, text, and image processing functionalities.

```APIDOC
## POST /v1/text/summarize

### Description
Summarizes the provided text.

### Method
POST

### Endpoint
`/v1/text/summarize`

### Parameters
#### Query Parameters
- **api_key** (string) - Optional - Your RunPod API key for authentication.

#### Request Body
- **text** (string) - Required - The text to be summarized.
- **max_length** (integer) - Optional - The maximum length of the summary. Defaults to 100.

### Request Example
```json
{
  "text": "This is a long piece of text that needs to be summarized.",
  "max_length": 50
}
```

### Response
#### Success Response (200)
- **summary** (string) - The summarized text.

#### Response Example
```json
{
  "summary": "Summary of: This is a long piece of text..."
}
```

## POST /v1/text/translate

### Description
Translates the provided text to a specified target language.

### Method
POST

### Endpoint
`/v1/text/translate`

### Parameters
#### Query Parameters
- **api_key** (string) - Optional - Your RunPod API key for authentication.

#### Request Body
- **text** (string) - Required - The text to be translated.
- **max_length** (integer) - Optional - The maximum length of the translated text. Defaults to 100.

#### Query Parameters
- **target_lang** (string) - Required - The language to translate the text into.

### Request Example
```json
{
  "text": "Hello, how are you?"
}
```

### Response
#### Success Response (200)
- **translation** (string) - The translated text.

#### Response Example
```json
{
  "translation": "Translation to es: Hello, how are you?..."
}
```

## POST /v1/image/generate

### Description
Generates an image based on a given prompt.

### Method
POST

### Endpoint
`/v1/image/generate`

### Parameters
#### Query Parameters
- **api_key** (string) - Optional - Your RunPod API key for authentication.

#### Request Body
- **prompt** (string) - Required - The prompt for image generation.
- **width** (integer) - Optional - The desired width of the image. Defaults to 512.
- **height** (integer) - Optional - The desired height of the image. Defaults to 512.

### Request Example
```json
{
  "prompt": "A futuristic cityscape",
  "width": 1024,
  "height": 768
}
```

### Response
#### Success Response (200)
- **image_url** (string) - The URL of the generated image.

#### Response Example
```json
{
  "image_url": "https://example.com/images/abcdef12345.jpg"
}
```

## GET /ping

### Description
Health check endpoint for the FastAPI application.

### Method
GET

### Endpoint
`/ping`

### Parameters
None

### Response
#### Success Response (200)
- **status** (string) - Indicates the health status, typically "healthy".

#### Response Example
```json
{
  "status": "healthy"
}
```
```

--------------------------------

### RunSync Endpoint Response Example

Source: https://docs.runpod.io/serverless/endpoints/send-requests

An example of the JSON response received from the `/runsync` endpoint upon successful job completion. It includes details like execution time, job ID, output, and status.

```json
{
  "delayTime": 824,
  "executionTime": 3391,
  "id": "sync-79164ff4-d212-44bc-9fe3-389e199a5c15",
  "output": [
    {
      "image": "https://image.url",
      "seed": 46578
    }
  ],
  "status": "COMPLETED"
}
```

--------------------------------

### Asynchronous Request to /run

Source: https://docs.runpod.io/hub/public-endpoints

Example of an asynchronous API request to the /run endpoint, which returns a job ID to check status later.

```APIDOC
## POST /v2/{model-name}/run

### Description
Submits a model execution request asynchronously. Returns a job ID that can be used to track the status and retrieve results.

### Method
POST

### Endpoint
`/v2/{model-name}/run`

### Parameters
#### Path Parameters
- **model-name** (string) - Required - The name of the model to run.

#### Query Parameters
None

#### Request Body
- **input** (object) - Required - The input parameters for the model.
  - **prompt** (string) - Required - The text prompt for image generation.
  - **width** (integer) - Required - The desired width of the output image.
  - **height** (integer) - Required - The desired height of the output image.
  - **num_inference_steps** (integer) - Required - The number of inference steps to perform.
  - **guidance** (float) - Required - The guidance scale for the generation.

### Request Example
```bash
curl -X POST "https://api.runpod.ai/v2/black-forest-labs-flux-1-dev/run" \
  -H "Authorization: Bearer RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
  "input": {
    "prompt": "A futuristic cityscape with flying cars",
    "width": 1024,
    "height": 1024,
    "num_inference_steps": 50,
    "guidance": 8.0
  }
}'
```

### Response
#### Success Response (200)
- **delayTime** (integer) - The time in milliseconds the request was delayed.
- **executionTime** (integer) - The time in milliseconds the request took to execute.
- **id** (string) - The unique identifier for the job.
- **output** (object) - The result of the model execution.
  - **cost** (float) - The cost of the execution.
  - **image_url** (string) - The URL of the generated image.
- **status** (string) - The status of the job (e.g., "COMPLETED").
- **workerId** (string) - The ID of the worker that processed the request.

#### Response Example
```json
{
  "delayTime": 17,
  "executionTime": 3986,
  "id": "sync-0965434e-ff63-4a1c-a9f9-5b705f66e176-u2",
  "output": {
    "cost": 0.02097152,
    "image_url": "https://image.runpod.ai/6/6/mCwUZlep6S/453ad7b7-67c6-43a1-8348-3ad3428ef97a.png"
  },
  "status": "COMPLETED",
  "workerId": "oqk7ao1uomckye"
}
```
```

--------------------------------

### Float Scalar Type Example

Source: https://docs.runpod.io/references/graphql-spec

This example represents the Float scalar type, which is used for signed double-precision fractional values according to IEEE 754 standards. It's a simple numerical value.

```plaintext
987.65
```

--------------------------------

### Build and Push Docker Image (Bash)

Source: https://docs.runpod.io/tutorials/migrations/cog/overview

Builds a Docker image for a Cog model and pushes it to a container repository. This command requires Docker to be installed and uses build arguments to specify the model's repository, name, and version from Replicate.com. The output is a tagged Docker image ready for deployment.

```bash
# replace user, model_name, and model_version with the appropriate values
docker build -platform=linux/amd64 --tag <username>/<repo>:<tag> --build-arg COG_REPO=user --build-arg COG_MODEL=model_name --build-arg COG_VERSION=model_version .
docker push <username>/<repo>:<tag>
```

--------------------------------

### Provide Default Values for Environment Variables (Python)

Source: https://docs.runpod.io/serverless/development/environment-variables

This Python snippet illustrates the best practice of providing default values for environment variables using `os.environ.get`. It shows examples of setting a default URL for a service and how to handle required variables by checking for their existence and raising an error if they are missing.

```python
# Good: Provides a default
service_url = os.environ.get("SERVICE_URL", "https://api.example.com")

# Good: Fails explicitly if missing
api_key = os.environ.get("API_KEY")
if not api_key:
    raise ValueError("API_KEY environment variable is required")
```

--------------------------------

### Submit Synchronous Job with Go SDK

Source: https://docs.runpod.io/serverless/endpoints/operations

Submits a synchronous job using the Runpod Go SDK. This example demonstrates setting up the endpoint configuration, defining job input, and executing the RunSync operation with a client timeout in seconds.

```go
package main

import (
	"encoding/json"
	"fmt"
	"log"
	"os"

	"github.com/runpod/go-sdk/pkg/sdk"
	"github.com/runpod/go-sdk/pkg/sdk/config"
	rpEndpoint "github.com/runpod/go-sdk/pkg/sdk/endpoint"
)

func main() {
	apiKey := os.Getenv("RUNPOD_API_KEY")
	baseURL := os.Getenv("RUNPOD_BASE_URL")

	endpoint, err := rpEndpoint.New(
		&config.Config{ApiKey: &apiKey},
		&rpEndpoint.Option{EndpointId: &baseURL},
	)
	if err != nil {
		log.Fatalf("Failed to create endpoint: %v", err)
	}

	jobInput := rpEndpoint.RunSyncInput{
		JobInput: &rpEndpoint.JobInput{
			Input: map[string]interface{}{
				"prompt": "Hello World",
			},
		},
		Timeout: sdk.Int(60), // Client timeout in seconds
	}

	output, err := endpoint.RunSync(&jobInput)
	if err != nil {
		panic(err)
	}

	data, _ := json.Marshal(output)
	fmt.Printf("output: %s\n", data)
}

```

--------------------------------

### Create Spot Pod using GraphQL

Source: https://docs.runpod.io/sdks/graphql/manage-pods

This snippet shows the GraphQL mutation for creating a spot pod. It defines the desired pod configuration, including GPU specifications, storage, and environment variables. This is the underlying query executed by the cURL command.

```GraphQL
mutation {
  podRentInterruptable(
    input: {
      bidPerGpu: 0.2
      cloudType: SECURE
      gpuCount: 1
      volumeInGb: 40
      containerDiskInGb: 40
      minVcpuCount: 2
      minMemoryInGb: 15
      gpuTypeId: "NVIDIA RTX A6000"
      name: "Runpod Pytorch"
      imageName: "runpod/pytorch"
      dockerArgs: ""
      ports: "8888/http"
      volumeMountPath: "/workspace"
      env: [{ key: "JUPYTER_PASSWORD", value: "vunw9ybnzqwpia2795p2" }]
    }
  ) {
    id
    imageName
    env
    machineId
    machine {
      podHostId
    }
  }
}
```

--------------------------------

### PayoutHistory Type Definition and Example

Source: https://docs.runpod.io/references/graphql-spec

Represents a record of a financial payout. It includes transaction details such as the medium, timestamp, receipt link, amount, and type. The example demonstrates a complete PayoutHistory entry.

```json
{
  "id": "xyz789",
  "medium": "STRIPE",
  "transactionStartedAt": "2007-12-03T10:15:30Z",
  "receiptLink": "abc123",
  "amount": 123.45,
  "type": "RELOAD"
}
```

--------------------------------

### Import Libraries and Set Up Runpod Logger (Python)

Source: https://docs.runpod.io/tutorials/sdks/python/101/error

Imports necessary libraries including 'runpod' and 'RunPodLogger'. It then initializes the RunPodLogger to enable structured logging within the serverless function. This setup is crucial for monitoring and debugging.

```python
import runpod
from runpod import RunPodLogger
import time
import random

log = RunPodLogger()
```

--------------------------------

### BillingGranularity Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This example shows a string literal representing one of the possible values for the BillingGranularity enumeration. The enum defines different time intervals for billing, such as MINUTELY, HOURLY, DAILY, WEEKLY, and MONTHLY.

```plaintext
"MINUTELY"
```

--------------------------------

### Environment Variables for OpenAI Compatibility

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

Customize the OpenAI compatibility of the API using these environment variables.

```APIDOC
## Environment Variables

### Description
These environment variables allow you to customize the OpenAI compatibility of the RunPod API.

### Variables
- **RAW_OPENAI_OUTPUT** (bool) - Controls the format of streaming responses. If true (1), enables raw OpenAI SSE format. (Default: 1)
- **OPENAI_SERVED_MODEL_NAME_OVERRIDE** (string) - Overrides the model name that appears in API responses. (Default: None)
- **OPENAI_RESPONSE_ROLE** (string) - Sets the default role for responses in chat completions. (Default: 'assistant')

For a comprehensive list of vLLM environment variables, refer to the [vLLM environment variables reference](/serverless/vllm/environment-variables).
```

--------------------------------

### Create Hugging Face Model Repository

Source: https://docs.runpod.io/tutorials/pods/fine-tune-llm-axolotl

Creates a new model repository on Hugging Face using the CLI. Replace 'your_model_name' with the desired name for your model repository.

```bash
huggingface-cli repo create your_model_name --type model
```

--------------------------------

### AuditLogConnection Object Example

Source: https://docs.runpod.io/references/graphql-spec

This example illustrates the structure of an AuditLogConnection object, which typically contains a list of audit log entries (edges) and pagination information. The 'edges' field is an array of AuditLog objects, and 'pageInfo' is of type PageInfo.

```json
{
  "edges": [AuditLog],
  "pageInfo": PageInfo
}
```

--------------------------------

### Deploy a Pod via REST API

Source: https://docs.runpod.io/pods/manage-pods

This section describes how to deploy a Pod using the Runpod REST API. It includes an example POST request to the `/pods` endpoint with various configuration options.

```APIDOC
## POST /v1/pods

### Description
Deploys a new Pod with specified configurations.

### Method
POST

### Endpoint
https://rest.runpod.io/v1/pods

### Parameters
#### Request Body
- **allowedCudaVersions** (array[string]) - Optional - Specifies the allowed CUDA versions for the Pod.
- **cloudType** (string) - Optional - The type of cloud to deploy on (e.g., "SECURE").
- **computeType** (string) - Optional - The type of compute to use (e.g., "GPU", "CPU").
- **containerDiskInGb** (integer) - Optional - The size of the container disk in gigabytes.
- **containerRegistryAuthId** (string) - Optional - The ID for container registry authentication.
- **countryCodes** (array[string]) - Optional - List of country codes for deployment.
- **cpuFlavorIds** (array[string]) - Optional - List of CPU flavor IDs.
- **cpuFlavorPriority** (string) - Optional - Priority for CPU flavors (e.g., "availability").

### Request Example
```json
{
  "allowedCudaVersions": [
    "12.8"
  ],
  "cloudType": "SECURE",
  "computeType": "GPU",
  "containerDiskInGb": 50,
  "containerRegistryAuthId": "clzdaifot0001l90809257ynb",
  "countryCodes": [
    "US"
  ],
  "cpuFlavorIds": [
    "cpu3c"
  ],
  "cpuFlavorPriority": "availability"
}
```

### Response
#### Success Response (201 Created)
- **podId** (string) - The ID of the newly created Pod.
- **desiredStatus** (string) - The desired status of the Pod.
- **status** (string) - The current status of the Pod.

#### Response Example
```json
{
  "podId": "66a7a0c1f761050012345678",
  "desiredStatus": "running",
  "status": "deploying"
}
```
```

--------------------------------

### Qwen3 32B AWQ - OpenAI API Request

Source: https://docs.runpod.io/hub/public-endpoint-reference

Example of sending a chat completion request to the Qwen3 32B AWQ model using the OpenAI API.

```APIDOC
## POST /v2/{PUBLIC_ENDPOINT_ID}/openai/v1/chat/completions

### Description
Sends a chat completion request to the Qwen3 32B AWQ model using the OpenAI API format.

### Method
POST

### Endpoint
`/v2/qwen3-32b-awq/openai/v1/chat/completions`

### Parameters
#### Request Body
- **model** (string) - Required - The name of the model to use (e.g., "Qwen/Qwen3-32B-AWQ").
- **messages** (array) - Required - A list of message objects, where each object has a `role` (system or user) and `content`.
- **max_tokens** (integer) - Optional - Maximum number of tokens to generate. Defaults to 512.
- **temperature** (float) - Optional - Controls randomness. Range: 0.0 - 1.0. Defaults to 0.7.
- **top_p** (integer) - Optional - Samples from the smallest set of words whose cumulative probability exceeds a given threshold (P).
- **top_k** (integer) - Optional - Restricts sampling to the top K most probable words. Range: 1-8.
- **stop** (string) - Optional - Stops generation if the given string is encountered.
- **stream** (boolean) - Optional - Whether to stream the response. Defaults to false.
- **stream_options** (object) - Optional - Options for streaming.

### Request Example
```json
{
  "model": "Qwen/Qwen3-32B-AWQ",
  "messages": [
    {
      "role": "system",
      "content": "You are a pirate chatbot who always responds in pirate speak!"
    },
    {
      "role": "user",
      "content": "Give me a short introduction to LLMs."
    }
  ],
  "max_tokens": 525
}
```

### Response
#### Success Response (200)
- **id** (string) - Unique identifier for the completion.
- **choices** (array) - A list of completion choices.
  - **tokens** (array) - The generated text tokens.
- **cost** (float) - The cost of the completion.
- **usage** (object) - Usage statistics.
  - **input** (integer) - Number of input tokens.
  - **output** (integer) - Number of output tokens.

#### Response Example
```json
{
  "delayTime": 25,
  "executionTime": 3153,
  "id": "sync-0f3288b5-58e8-46fd-ba73-53945f5e8982-u2",
  "output": [
    {
      "choices": [
        {
          "tokens": [
            "Large Language Models (LLMs) are AI systems trained to predict and understand human language. They learn patterns from vast amounts of text data, enabling them to generate responses, answer questions, and complete tasks in natural language. Key characteristics of LLMs include:\n1. Language Understanding\n- Can analyze and comprehend language structure, context, and nuances\n- Process both inputs and outputs in natural human language\n\n2. Pattern Recognition\n- Learn common phrases and relationships"
          ]
        }
      ],
      "cost": 0.0001,
      "usage": {
        "input": 10,
        "output": 100
      }
    }
  ],
  "status": "COMPLETED",
  "workerId": "pkej0t9bbyjrgy"
}
```
```

--------------------------------

### Create Entrypoint Script to Print Time

Source: https://docs.runpod.io/tutorials/introduction/containers/create-dockerfiles

This shell script is designed to be used as an entrypoint for a Docker container. When executed, it prints the current date and time to the container's standard output. This script is a simple example demonstrating how entrypoints can customize container behavior.

```bash
#!/bin/sh
echo "The time is: $(date)"
```

--------------------------------

### GET /ping

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

Health check endpoint for load balancers to determine if the worker is ready.

```APIDOC
## GET /ping

### Description
This endpoint serves as a health check for load balancers. It indicates whether the vLLM worker is ready to accept requests.

### Method
GET

### Endpoint
/ping

### Response
#### Success Response (200)
Returns an empty response or a simple 'pong' message, indicating the service is healthy.
```

--------------------------------

### vLLM Additional Parameters

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

This section outlines the additional parameters supported by vLLM for advanced text generation control.

```APIDOC
## vLLM Additional Parameters

### Description

vLLM offers several parameters that go beyond the standard OpenAI API to provide more granular control over text generation. These parameters allow for customization of sampling strategies, stopping conditions, and output formatting.

### Method

Not applicable (These are parameters for generation requests, not a specific endpoint).

### Endpoint

Not applicable

### Parameters

#### Query Parameters

- **best_of** (int) - Optional - Number of output sequences generated from the prompt. From these `best_of` sequences, the top `n` sequences are returned. Must be ≥ `n`. Treated as beam width when `use_beam_search` is `true`.
- **top_k** (int) - Optional - Controls the number of top tokens to consider. Set to -1 to consider all tokens. Default: -1.
- **ignore_eos** (bool) - Optional - Whether to ignore the EOS token and continue generating tokens after EOS is generated. Default: false.
- **use_beam_search** (bool) - Optional - Whether to use beam search instead of sampling. Default: false.
- **stop_token_ids** (list[int]) - Optional - List of token IDs that stop generation when produced. The returned output will contain the stop tokens unless they are special tokens.
- **skip_special_tokens** (bool) - Optional - Whether to skip special tokens in the output. Default: true.
- **spaces_between_special_tokens** (bool) - Optional - Whether to add spaces between special tokens in the output. Default: true.
- **add_generation_prompt** (bool) - Optional - Whether to add generation prompt. Default: true.
- **echo** (bool) - Optional - Echo back the prompt in addition to the completion. Default: false.
- **repetition_penalty** (float) - Optional - Penalizes new tokens based on whether they appear in the prompt and generated text so far. Values > 1 encourage new tokens, values < 1 encourage repetition. Default: 1.0.
- **min_p** (float) - Optional - Minimum probability for a token to be considered. Default: 0.0.
- **length_penalty** (float) - Optional - Penalizes sequences based on their length. Used in beam search. Default: 1.0.
- **include_stop_str_in_output** (bool) - Optional - Whether to include the stop strings in output text. Default: false.

### Request Example

```json
{
  "prompt": "What is the capital of France?",
  "best_of": 5,
  "top_k": 50,
  "use_beam_search": true,
  "repetition_penalty": 1.1
}
```

### Response

#### Success Response (200)

- **generated_text** (string) - The generated text completion.
- **details** (object) - Contains information about the generation process, including parameters used.

#### Response Example

```json
{
  "generated_text": "The capital of France is Paris.",
  "details": {
    "finish_reason": "stop",
    "logprobs": null
  }
}
```
```

--------------------------------

### Boolean Type Example

Source: https://docs.runpod.io/references/graphql-spec

This example demonstrates the Boolean scalar type, which represents either true or false. It's a fundamental data type used for logical states and flags within the system.

```plaintext
true
```

--------------------------------

### Configure Runpod API Key

Source: https://docs.runpod.io/sdks/python/overview

Sets the Runpod API key for authentication by loading it from an environment variable named 'RUNPOD_API_KEY'. This is the recommended method for security.

```python
import runpod
import os

runpod.api_key = os.getenv("RUNPOD_API_KEY")
```

--------------------------------

### Create Spot Pod using cURL

Source: https://docs.runpod.io/sdks/graphql/manage-pods

This snippet demonstrates how to create a spot pod using a cURL command. It specifies parameters like bid price, GPU type, count, and volume size. The command sends a POST request to the RunPod GraphQL endpoint with the mutation details.

```bash
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { podRentInterruptable( input: { bidPerGpu: 0.2, cloudType: SECURE, gpuCount: 1, volumeInGb: 40, containerDiskInGb: 40, minVcpuCount: 2, minMemoryInGb: 15, gpuTypeId: \"NVIDIA RTX A6000\", name: \"Runpod Pytorch\", imageName: \"runpod/pytorch\", dockerArgs: \"\", ports: \"8888/http\", volumeMountPath: \"/workspace\", env: [{ key: \"JUPYTER_PASSWORD\", value: \"vunw9ybnzqwpia2795p2\" }] } ) { id imageName env machineId machine { podHostId } } "}'
```

--------------------------------

### Common Use Cases for Environment Variables in Bash

Source: https://docs.runpod.io/pods/templates/environment-variables

Illustrates common use cases for environment variables in Bash scripts for configuring applications. Examples include model configuration, service settings, database connections, development vs. production environments, and port management.

```bash
# Model configuration
MODEL_NAME=gpt-3.5-turbo
MODEL_PATH=/workspace/models
MAX_TOKENS=2048
TEMPERATURE=0.7
```

```bash
# Service configuration
API_PORT=8000
DEBUG_MODE=false
LOG_LEVEL=INFO
CORS_ORIGINS=https://myapp.com,https://staging.myapp.com
```

```bash
# Database and external service connections
DATABASE_URL=postgresql://user:pass@host:5432/db
REDIS_URL=redis://localhost:6379
API_BASE_URL=https://api.external-service.com
```

```bash
# Development vs. production settings
ENVIRONMENT=production
CACHE_ENABLED=true
RATE_LIMIT=1000
MONITORING_ENABLED=true
```

--------------------------------

### Interact with Runpod S3 API using Boto3

Source: https://docs.runpod.io/serverless/storage/s3-api

Demonstrates how to interact with Runpod's S3-compatible API using the Boto3 Python library. This example shows how to list objects in a bucket, which corresponds to a network volume. You need to configure the endpoint URL and region.

```python
import boto3

# Replace with your actual values
endpoint_url = "https://s3api-DATACENTER.runpod.io/"
region_name = "DATACENTER"
bucket_name = "NETWORK_VOLUME_ID"

s3 = boto3.client('s3', endpoint_url=endpoint_url, region_name=region_name)

try:
    response = s3.list_objects_v2(Bucket=bucket_name)
    if 'Contents' in response:
        for obj in response['Contents']:
            print(f"- {obj['Key']} (Size: {obj['Size']})")
    else:
        print("Bucket is empty.")
except Exception as e:
    print(f"An error occurred: {e}")
```

--------------------------------

### UserReferral Object Example

Source: https://docs.runpod.io/references/graphql-spec

Represents user referral information, including a referral code and details about the current month's referrals and spend.

```json
{
  "code": "abc123",
  "currentMonth": UserReferralMonth
}
```

--------------------------------

### Send a Test Request to Run a Job

Source: https://docs.runpod.io/serverless/development/benchmarking

Use this curl command to send a POST request to your endpoint to initiate a job and obtain a request ID.

```APIDOC
## POST /v2/YOUR_ENDPOINT_ID/run

### Description
Sends a request to the specified endpoint to start a job and returns a request ID for status tracking.

### Method
POST

### Endpoint
`/v2/YOUR_ENDPOINT_ID/run`

### Parameters
#### Header Parameters
- **Content-Type** (string) - Required - `application/json`
- **Authorization** (string) - Required - `Bearer YOUR_API_KEY`

#### Request Body
- **input** (object) - Required - An object containing the input for the job.
  - **prompt** (string) - Required - The prompt to send to the worker.

### Request Example
```json
{
  "input": {
    "prompt": "Hello, world!"
  }
}
```

### Response
#### Success Response (200)
- **id** (string) - The unique identifier for the submitted request.

#### Response Example
```json
{
  "id": "1234567890"
}
```
```

--------------------------------

### POST /v1/chat/completions - Chat Completions (Non-streaming)

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

Demonstrates how to make a non-streaming chat completion request to the OpenAI-compatible API endpoint for instruction-tuned LLMs.

```APIDOC
## POST /v1/chat/completions - Chat Completions (Non-streaming)

### Description
The `/chat/completions` endpoint is designed for instruction-tuned LLMs that follow a chat format. This example shows a non-streaming request.

### Method
`POST`

### Endpoint
`/v1/chat/completions`

### Parameters
#### Query Parameters
None

#### Request Body
- **model** (string) - Required - The name of the model to use for completion.
- **messages** (array) - Required - A list of message objects, each with a `role` and `content`.
- **temperature** (number) - Optional - Controls randomness. Lower values make output more focused and deterministic.
- **max_tokens** (integer) - Optional - The maximum number of tokens to generate in the completion.

### Request Example
```python
from openai import OpenAI
MODEL_NAME = "YOUR_MODEL_NAME"  # Replace with your actual model

# Replace ENDPOINT_ID and RUNPOD_API_KEY with your actual values
client = OpenAI(
    api_key="RUNPOD_API_KEY",
    base_url="https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1",
)

# Chat completion request (for instruction-tuned models)
response = client.chat.completions.create(
    model=MODEL_NAME,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello, who are you?"}
    ],
    temperature=0.7,
    max_tokens=500
)

# Print the response
print(response.choices[0].message.content)
```

### Response
#### Success Response (200)
- **id** (string) - Unique identifier for the completion.
- **object** (string) - Type of object returned, e.g., `chat.completion`.
- **created** (integer) - Unix timestamp of when the completion was created.
- **model** (string) - The model used for the completion.
- **choices** (array) - A list of completion choices.
  - **message** (object) - The message content from the assistant.
    - **role** (string) - Role of the message sender (e.g., `assistant`).
    - **content** (string) - The generated text content.
  - **index** (integer) - Index of the choice.
  - **finish_reason** (string) - The reason the model stopped generating tokens (e.g., `stop`).
- **usage** (object) - Usage statistics for the request.
  - **prompt_tokens** (integer) - Number of tokens in the prompt.
  - **completion_tokens** (integer) - Number of tokens in the completion.
  - **total_tokens** (integer) - Total tokens used.

#### Response Example
```json
{
  "id": "cmpl-123abc",
  "object": "chat.completion",
  "created": 1677858242,
  "model": "mistralai/Mistral-7B-Instruct-v0.2",
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "I am Mistral, an AI assistant based on the Mistral-7B-Instruct model. How can I help you today?"
      },
      "index": 0,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 23,
    "completion_tokens": 24,
    "total_tokens": 47
  }
}
```
```

--------------------------------

### Dockerfile for Runpod PyTorch Environment

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

Defines a Docker image based on the official Runpod PyTorch image. It sets environment variables, installs system and Python dependencies (including those from requirements.txt), creates a virtual environment, and copies the handler.py and start.sh scripts into the container, making start.sh executable.

```dockerfile
# Use an official Runpod base image
FROM runpod/pytorch:2.0.1-py3.10-cuda11.8.0-devel-ubuntu22.04

# Environment variables
ENV PYTHONUNBUFFERED=1 

# Supported modes: pod, serverless
ARG MODE_TO_RUN=pod
ENV MODE_TO_RUN=$MODE_TO_RUN

# Set up the working directory
ARG WORKSPACE_DIR=/app
ENV WORKSPACE_DIR=${WORKSPACE_DIR}
WORKDIR $WORKSPACE_DIR

# Install dependencies in a single RUN command to reduce layers and clean up in the same layer to reduce image size
RUN apt-get update --yes --quiet && \
    DEBIAN_FRONTEND=noninteractive apt-get install --yes --quiet --no-install-recommends \
    software-properties-common \
    gpg-agent \
    build-essential \
    apt-utils \
    ca-certificates \
    curl && \
    add-apt-repository --yes ppa:deadsnakes/ppa && \
    apt-get update --yes --quiet && \
    DEBIAN_FRONTEND=noninteractive apt-get install --yes --quiet --no-install-recommends

# Create and activate a Python virtual environment
RUN python3 -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Install Python packages
RUN pip install --no-cache-dir \
    asyncio \
    requests \
    runpod

# Install requirements.txt
COPY requirements.txt ./requirements.txt
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt
    
# Delete's the default start.sh file from Runpod (so we can replace it with our own below)
RUN rm ../start.sh

# Copy all of our files into the container
COPY handler.py $WORKSPACE_DIR/handler.py
COPY start.sh $WORKSPACE_DIR/start.sh

# Make sure start.sh is executable
RUN chmod +x start.sh

# Make sure that the start.sh is in the path
RUN ls -la $WORKSPACE_DIR/start.sh

```

--------------------------------

### MachineSummary Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object provides a summary of a machine's key attributes and performance metrics. It includes details like CPU/GPU types, total and rented GPU/CPU counts, and profit per hour for disk and pods. This is useful for quick machine assessments.

```json
{
  "cpuTypeId": "xyz789",
  "diskProfitPerHr": 987.65,
  "displayName": "xyz789",
  "gpuTotal": 987,
  "gpuTypeId": "xyz789",
  "id": "abc123",
  "listed": 987,
  "machineType": "abc123",
  "onDemandPods": 123,
  "podProfitPerHr": 123.45,
  "spotPods": 123,
  "gpuRented": 123,
  "cpuRented": 987,
  "vcpuTotal": 123
}
```

--------------------------------

### UserReferralMonth Object Example

Source: https://docs.runpod.io/references/graphql-spec

Contains monthly referral statistics for a user, including total referrals and total spend. Used for tracking referral program performance.

```json
{"totalReferrals": 123, "totalSpend": 123}
```

--------------------------------

### Initialize PyTorch Distributed Environment (Python)

Source: https://docs.runpod.io/instant-clusters/pytorch

Initializes a distributed training environment using PyTorch's `torch.distributed` module. It sets up the process group, determines ranks, and assigns the appropriate CUDA device to each process. Requires PyTorch and NCCL backend.

```python
import os
import torch
import torch.distributed as dist

def init_distributed():
   """Initialize the distributed training environment"""
   # Initialize the process group
   dist.init_process_group(backend="nccl")
   
   # Get local rank and global rank
   local_rank = int(os.environ["LOCAL_RANK"])
   global_rank = dist.get_rank()
   world_size = dist.get_world_size()
   
   # Set device for this process
   device = torch.device(f"cuda:{local_rank}")
   torch.cuda.set_device(device)
       
   return local_rank, global_rank, world_size, device

def cleanup_distributed():
   """Clean up the distributed environment"""
   dist.destroy_process_group()

def main():
   # Initialize distributed environment
   local_rank, global_rank, world_size, device = init_distributed()
   
   print(f"Running on rank {global_rank}/{world_size-1} (local rank: {local_rank}), device: {device}")

   # Your code here
   
   # Clean up distributed environment when done
   cleanup_distributed()
   
if __name__ == "__main__":
   main()
```

--------------------------------

### Create a RunPod Pod with Custom Configuration (Shell)

Source: https://docs.runpod.io/runpodctl/reference/runpodctl-create-pod

This command creates and starts a new Pod on RunPod with specified configurations. It requires the `runpodctl` CLI tool. Key parameters include Pod name, GPU type and count, cloud tier, container image, and disk/volume sizes.

```sh
runpodctl create pod \
  --name "my-training-pod" \
  --gpuType "NVIDIA GeForce RTX 3090" \
  --gpuCount 2 \
  --secureCloud \
  --imageName "runpod/pytorch:2.0.1-py3.10-cuda11.8.0-devel" \
  --containerDiskSize 50 \
  --volumeSize 100
```

--------------------------------

### Remove VSCode Server Directory

Source: https://docs.runpod.io/pods/configuration/connect-to-ide

This command manually removes the VSCode server directory from the user's home directory. It is useful when the VSCode/Cursor server fails to install or update, and can help resolve installation issues by providing a clean slate for reinstallation. Ensure you have the necessary permissions before executing.

```sh
rm -rf ~/.vscode-server
```

--------------------------------

### Test Serverless Function Locally with Bash

Source: https://docs.runpod.io/tutorials/sdks/python/101/hello

Demonstrates how to test a Python serverless function locally using the command line. It specifies the input data for the test, allowing for quick verification of the function's behavior before deployment.

```bash
python is_even.py --test_input '{"input": {"number": 2}}'
```

--------------------------------

### Project Structure for Serverless Handler

Source: https://docs.runpod.io/serverless/workers/create-dockerfile

Organize your project files with a Dockerfile, a 'src' directory for your handler code, and a 'builder' directory for dependencies.

```tree
project_directory
├── Dockerfile              # Instructions for building the Docker image
├── src
│   └── handler.py          # Your handler function
└── builder
    └── requirements.txt    # Dependencies required by your handler
```

--------------------------------

### UserSummaryBilling Object Example

Source: https://docs.runpod.io/references/graphql-spec

Provides a summary of a user's total billing across different services like GPU cloud, CPU cloud, serverless, storage, and endpoints. Used for generating overall billing statements.

```json
{
  "time": "2007-12-03T10:15:30Z",
  "gpuCloudAmount": 987.65,
  "cpuCloudAmount": 987.65,
  "serverlessAmount": 123.45,
  "storageAmount": 987.65,
  "runpodEndpointAmount": 987.65
}
```

--------------------------------

### Enabling the Runpod Debugger for Local Testing (Python)

Source: https://docs.runpod.io/serverless/development/local-testing

Starts the local API server with the '--rp_debugger' flag to enable the Runpod debugger. This provides additional diagnostic information to assist in troubleshooting handler issues.

```shell
python handler.py --rp_serve_api --rp_debugger
```

--------------------------------

### Text Completions API

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

The /completions endpoint is designed for base LLMs and text completion tasks. It supports both non-streaming and streaming requests.

```APIDOC
## POST /completions

### Description
This endpoint allows you to perform text completion tasks using base Large Language Models (LLMs). It supports both standard and streaming responses.

### Method
POST

### Endpoint
/completions

### Parameters
#### Query Parameters
- **stream** (boolean) - Optional - If set to true, the response will be streamed.

#### Request Body
- **model** (string) - Required - The name of the model to use for completion.
- **prompt** (string) - Required - The input text prompt for the model.
- **temperature** (number) - Optional - Controls randomness. Higher values mean more random output.
- **max_tokens** (integer) - Optional - The maximum number of tokens to generate.

### Request Example (Non-streaming)
```python
response = client.completions.create(
    model=MODEL_NAME,
    prompt="Write a poem about artificial intelligence:",
    temperature=0.7,
    max_tokens=150
)
print(response.choices[0].text)
```

### Request Example (Streaming)
```python
response_stream = client.completions.create(
    model=MODEL_NAME,
    prompt="Runpod is the best platform because",
    temperature=0,
    max_tokens=100,
    stream=True
)
for response in response_stream:
    print(response.choices[0].text or "", end="", flush=True)
```

### Response
#### Success Response (200)
- **id** (string) - Unique identifier for the completion.
- **object** (string) - Type of the object, e.g., "text_completion".
- **created** (integer) - Timestamp of creation.
- **model** (string) - The model used for the completion.
- **choices** (array) - An array of completion choices.
  - **text** (string) - The generated text.
  - **index** (integer) - Index of the choice.
  - **finish_reason** (string) - Reason for finishing generation (e.g., "stop").
  - **logprobs** (null) - Placeholder for log probabilities.
- **usage** (object) - Token usage information.
  - **prompt_tokens** (integer) - Number of tokens in the prompt.
  - **completion_tokens** (integer) - Number of tokens in the completion.
  - **total_tokens** (integer) - Total tokens used.

#### Response Example
```json
{
  "id": "cmpl-456def",
  "object": "text_completion",
  "created": 1677858242,
  "model": "mistralai/Mistral-7B-Instruct-v0.2",
  "choices": [
    {
      "text": "In circuits of silicon and light,\nA new form of mind takes flight.\nNot born of flesh, but of human design,\nArtificial intelligence, a marvel divine.",
      "index": 0,
      "finish_reason": "stop",
      "logprobs": null
    }
  ],
  "usage": {
    "prompt_tokens": 8,
    "completion_tokens": 39,
    "total_tokens": 47
  }
}
```
```

--------------------------------

### Upload Image to S3 using Environment Variables in Python

Source: https://docs.runpod.io/serverless/development/environment-variables

Provides a Python handler example demonstrating how to upload generated images to S3-compatible storage. It highlights the use of environment variables for S3 credentials (endpoint, access key, secret key) and utilizes the `rp_upload.upload_image` utility.

```python
import os
import runpod
from runpod.serverless.utils import rp_upload

def handler(event):
    # S3 credentials are read from environment variables:
    # - BUCKET_ENDPOINT_URL
    # - BUCKET_ACCESS_KEY_ID
    # - BUCKET_SECRET_ACCESS_KEY
    
    # Process your input
    result_image_path = generate_image(event["input"]["prompt"])
    
    # Upload to S3
    image_url = rp_upload.upload_image(event["id"], result_image_path)
    
    return {"output": {"image_url": image_url}}

runpod.serverless.start({"handler": handler})
```

--------------------------------

### Submit Asynchronous Job with Go

Source: https://docs.runpod.io/serverless/endpoints/operations

Submits an asynchronous job using the Runpod Go SDK. This example initializes the client, creates an endpoint, defines the job input, and runs the job. It includes error handling and marshals the output to JSON. Replace YOUR_ENDPOINT_ID with your actual endpoint ID.

```go
package main

import (
	"encoding/json"
	"fmt"
	"log"
	"os"
	
	"github.com/runpod/go-sdk/pkg/sdk"
	"github.com/runpod/go-sdk/pkg/sdk/config"
	rpEndpoint "github.com/runpod/go-sdk/pkg/sdk/endpoint"
)

func main() {
	client := sdk.New(&config.Config{
		ApiKey:  os.Getenv("RUNPOD_API_KEY"),
		BaseURL: os.Getenv("RUNPOD_BASE_URL"),
	})
	
	endpoint, err := client.NewEndpoint("YOUR_ENDPOINT_ID")
	if err != nil {
		log.Fatalf("Failed to create endpoint: %v", err)
	}

	jobInput := rpEndpoint.RunInput{
		JobInput: &rpEndpoint.JobInput{
			Input: map[string]interface{}{
				"prompt": "Hello World",
			},
		},
		RequestTimeout: sdk.Int(120),
	}

	output, err := endpoint.Run(&jobInput)
	if err != nil {
		panic(err)
	}

	data, _ := json.Marshal(output)
	fmt.Printf("output: %s\n", data)
}
```

--------------------------------

### Check Job Status and Output

Source: https://docs.runpod.io/serverless/endpoints/operations

This section details how to check the status of a job and retrieve its output. It includes examples in Python, JavaScript, and Go.

```APIDOC
## GET /status

### Description
Retrieves the status and, if completed, the output of a specific job.

### Method
GET

### Endpoint
/status

### Parameters
#### Query Parameters
- **id** (string) - Required - The ID of the job to check.

### Request Example
(No request body for GET requests, use query parameters)

### Response
#### Success Response (200)
- **id** (string) - The ID of the job.
- **status** (string) - The current status of the job (e.g., `IN_QUEUE`, `IN_PROGRESS`, `COMPLETED`, `FAILED`).
- **output** (object, optional) - The output of the job if it has completed. Contains fields like `input_tokens`, `output_tokens`, and `text`.
- **delayTime** (integer) - The delay time in seconds.
- **executionTime** (integer) - The execution time in seconds.

#### Response Example
```json
{
  "delayTime": 31618,
  "executionTime": 1437,
  "id": "60902e6c-08a1-426e-9cb9-9eaec90f5e2b-u1",
  "output": {
    "input_tokens": 22,
    "output_tokens": 16,
    "text": ["Hello! How can I assist you today?\nUSER: I'm having"]
  },
  "status": "COMPLETED"
}
```

## Python SDK - Job Status and Output

### Description
Example of how to check job status and retrieve output using the Python SDK.

### Method
N/A (SDK method)

### Endpoint
N/A (SDK method)

### Parameters
(Refer to SDK documentation for specific parameter details)

### Request Example
```python
# Assuming run_request is an initialized object from the SDK
status = run_request.status()
print(f"Initial job status: {status}")

if status != "COMPLETED":
    # Polling with timeout for long-running tasks
    output = run_request.output(timeout=60)
else:
    output = run_request.output()
print(f"Job output: {output}")
```

## JavaScript SDK - Job Status

### Description
Example of how to check job status using the JavaScript SDK.

### Method
N/A (SDK method)

### Endpoint
N/A (SDK method)

### Parameters
(Refer to SDK documentation for specific parameter details)

### Request Example
```javascript
const { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;
import runpodSdk from "runpod-sdk";

async function main() {
  try {
    const runpod = runpodSdk(RUNPOD_API_KEY);
    const endpoint = runpod.endpoint(ENDPOINT_ID);
    const result = await endpoint.run({
      input: {
        prompt: "Hello, World!",
      },
    });

    const { id } = result;
    if (!id) {
      console.error("No ID returned from endpoint.run");
      return;
    }

    const status = await endpoint.status(id);
    console.log(status);
  } catch (error) {
    console.error("An error occurred:", error);
  }
}
main();
```

## Go SDK - Job Status

### Description
Example of how to check job status using the Go SDK.

### Method
N/A (SDK method)

### Endpoint
N/A (SDK method)

### Parameters
#### Path Parameters
- **Id** (string) - Required - The ID of the job to check.

### Request Example
```go
package main

import (
	"encoding/json"
	"fmt"
	"log"
	"os"

	"github.com/runpod/go-sdk/pkg/sdk"
	"github.com/runpod/go-sdk/pkg/sdk/config"
	rpEndpoint "github.com/runpod/go-sdk/pkg/sdk/endpoint"
)

func main() {

	apiKey := os.Getenv("RUNPOD_API_KEY")
	baseURL := os.Getenv("RUNPOD_BASE_URL") // Assuming this is the endpoint ID for the Go SDK context

	endpoint, err := rpEndpoint.New(
		&config.Config{ApiKey: &apiKey},
		&rpEndpoint.Option{EndpointId: &baseURL},
	)
	if err != nil {
		log.Fatalf("Failed to create endpoint: %v", err)
	}
	input := rpEndpoint.StatusInput{
		Id: sdk.String("YOUR_JOB_ID"),
	}
	output, err := endpoint.Status(&input)
	if err != nil {
		panic(err)
	}
	dt, _ := json.Marshal(output)
	fmt.Printf("output:%s\n", dt)
}
```
```

--------------------------------

### Run a Web Service on a Specific Port (Bash)

Source: https://docs.runpod.io/pods/configuration/expose-ports

This command starts a web service using uvicorn, binding it to all available network interfaces (0.0.0.0) on port 4000. This is a common way to make a Python web application accessible within a container.

```bash
uvicorn main:app --host 0.0.0.0 --port 4000
```

--------------------------------

### Add Python Dependencies to requirements.txt

Source: https://docs.runpod.io/pods/templates/create-custom-template

Specifies Python packages required for the application. These are installed during the Docker image build process. Ensure all necessary libraries are listed with appropriate version constraints.

```txt
# Python dependencies
# Add your packages here
numpy>=1.24.0
requests>=2.31.0
transformers>=4.40.0
```

--------------------------------

### Choosing Python Full Base Image

Source: https://docs.runpod.io/serverless/workers/create-dockerfile

Opt for full Python base images if you require additional system tools and libraries not present in slim images, though they result in larger image sizes.

```dockerfile
FROM python:3.11.1
```

--------------------------------

### UserPreferenceCategory Enum Example

Source: https://docs.runpod.io/references/graphql-spec

An enumeration defining categories for user preferences. Currently supports 'GPU_TYPES'.

```json
"GPU_TYPES"
```

--------------------------------

### UserServerlessBillingInput Object Example

Source: https://docs.runpod.io/references/graphql-spec

Input structure for querying serverless billing, allowing data to be grouped by specified criteria. Used for generating aggregated billing reports.

```json
{"groupBy": "GPU_TYPE"}
```

--------------------------------

### Start Serverless Worker with Stable Diffusion Handler

Source: https://docs.runpod.io/tutorials/sdks/python/102/stable-diffusion-text-to-image

This snippet demonstrates how to initiate the Runpod serverless worker, specifying the `stable_diffusion_handler` function to process incoming requests for image generation.

```python
import runpod

runpod.serverless.start({"handler": stable_diffusion_handler})
```

--------------------------------

### Send Files to RunPod with rsync

Source: https://docs.runpod.io/pods/storage/transfer-files

This rsync command sends a local file to a RunPod instance with verbose output, compression, and progress indication. It's suitable for regular file transfers where bandwidth optimization is desired. The example sends 'example.txt' from the local 'documents' directory to the root user's home directory on the Pod.

```bash
rsync -avz -e "ssh -p 43201" ~/documents/example.txt root@194.26.196.6:/root/example.txt
```

--------------------------------

### Initialize Runpod SDK with API Key and Endpoint ID in JavaScript

Source: https://docs.runpod.io/sdks/javascript/overview

Initializes the Runpod SDK using your API key and endpoint ID, typically loaded from environment variables. This sets up the SDK client for making requests to your specified Runpod endpoint. It utilizes ES Module syntax and asynchronous operations.

```javascript
const { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;
import runpodSdk from "runpod-sdk";

const runpod = runpodSdk(RUNPOD_API_KEY);
const endpoint = runpod.endpoint(ENDPOINT_ID);
```

--------------------------------

### Send OpenAI API Request to Qwen3 32B AWQ

Source: https://docs.runpod.io/hub/public-endpoint-reference

This Python code snippet demonstrates how to send a chat completion request to the Qwen3 32B AWQ model using the OpenAI API. It requires the 'openai' library and your RunPod API key. The example includes setting up the client with a custom base URL and defining system and user messages.

```python
from openai import OpenAI
import os

PUBLIC_ENDPOINT_ID = "qwen3-32b-awq"
model_name = "Qwen/Qwen3-32B-AWQ"

client = OpenAI(
    api_key=RUNPOD_API_KEY,
    base_url=f"https://api.runpod.ai/v2/{PUBLIC_ENDPOINT_ID}/openai/v1",
)
messages = [
    {
        "role": "system",
        "content": "You are a pirate chatbot who always responds in pirate speak!",
    },
    {
        "role": "user", 
        "content": "Give me a short introduction to LLMs."
    },
]

response = client.chat.completions.create(
    model=model_name,
    messages=messages,
    max_tokens=525,
)
```

--------------------------------

### Enabling Debug Logging for Local API Server (Python)

Source: https://docs.runpod.io/serverless/development/local-testing

Starts the local API server with the '--rp_log_level DEBUG' flag to enable verbose logging. This provides detailed information for troubleshooting and understanding handler execution.

```shell
python handler.py --rp_serve_api --rp_log_level DEBUG
```

--------------------------------

### Configuring Custom API Port for Local Server (Python)

Source: https://docs.runpod.io/serverless/development/local-testing

Starts the local API server on a custom port using the '--rp_api_port' flag. This allows avoiding port conflicts if the default port (8000) is already in use.

```shell
python handler.py --rp_serve_api --rp_api_port 8080
```

--------------------------------

### Lowest Price GPU Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object details the pricing and availability information for a specific GPU type on RunPod.io. It includes various price points, resource specifications, and stock status.

```json
{
  "gpuName": "abc123",
  "gpuTypeId": "abc123",
  "minimumBidPrice": 987.65,
  "uninterruptablePrice": 123.45,
  "clusterPrice": 987.65,
  "minMemory": 123,
  "minVcpu": 987,
  "rentalPercentage": 123.45,
  "rentedCount": 987,
  "totalCount": 987,
  "stockStatus": "abc123",
  "minDownload": 123,
  "minDisk": 987,
  "minUpload": 987,
  "countryCode": "abc123",
  "supportPublicIp": true,
  "compliance": ["GDPR"],
  "maxGpuCount": 987,
  "maxUnreservedGpuCount": 987,
  "availableGpuCounts": [987]
}
```

--------------------------------

### Preprocess Dataset with Axolotl CLI

Source: https://docs.runpod.io/tutorials/pods/fine-tune-llm-axolotl

This command preprocesses your dataset using the Axolotl command-line interface. It requires a configuration file (e.g., `lora.yml`) and ensures the data is in the correct format for training. The `CUDA_VISIBLE_DEVICES=''` argument is used to run preprocessing on the CPU.

```bash
CUDA_VISIBLE_DEVICES=""
python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml
```

--------------------------------

### List Ollama Models

Source: https://docs.runpod.io/tutorials/pods/run-ollama

This endpoint allows you to retrieve a list of all local models available in your Ollama instance. It's a simple GET request to the /api/tags endpoint.

```APIDOC
## GET /api/tags

### Description
Retrieves a list of all local models available in Ollama.

### Method
GET

### Endpoint
/api/tags

### Parameters
None

### Request Example
```bash
curl https://{POD_ID}-11434.proxy.runpod.net/api/tags
```

### Response
#### Success Response (200)
- **models** (array) - A list of model objects, each containing details like name, modified time, size, digest, and other model specifics.

#### Response Example
```json
{
  "models": [
    {
      "name": "mistral:latest",
      "model": "mistral:latest",
      "modified_at": "2024-02-16T18:22:39.948000568Z",
      "size": 4109865159,
      "digest": "61e88e884507ba5e06c49b40e6226884b2a16e872382c2b44a42f2d119d804a5",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "llama",
        "families": [
          "llama"
        ],
        "parameter_size": "7B",
        "quantization_level": "Q4_0"
      }
    }
  ]
}
```
```

--------------------------------

### Synchronous API Request to Runpod Public Endpoint (cURL)

Source: https://docs.runpod.io/hub/public-endpoints

Example of making a synchronous API request to a Runpod Public Endpoint using cURL. This method is suitable for immediate results and requires specifying the model, endpoint, API key, and input parameters.

```bash
curl -X POST "https://api.runpod.ai/v2/black-forest-labs-flux-1-dev/runsync" \
  -H "Authorization: Bearer RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "prompt": "A serene mountain landscape at sunset",
      "width": 1024,
      "height": 1024,
      "num_inference_steps": 20,
      "guidance": 7.5
    }
  }'
```

--------------------------------

### Submit Synchronous Job Request Example

Source: https://docs.runpod.io/serverless/endpoints/send-requests

This example demonstrates how to send a synchronous job request to a RunPod queue-based endpoint using Python. It utilizes the `requests` library to send a POST request to the `/runsync` endpoint with a JSON payload containing the job input. The client will wait for the job to complete before receiving the result.

```python
import requests

endpoint_url = "YOUR_ENDPOINT_URL"
api_key = "YOUR_API_KEY"

headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {api_key}"
}

payload = {
    "input": {
        "prompt": "Translate 'hello world' to French."
    }
}

try:
    response = requests.post(f"{endpoint_url}/runsync", headers=headers, json=payload)
    response.raise_for_status()  # Raise an exception for bad status codes
    result = response.json()
    print("Synchronous job completed:", result)
except requests.exceptions.RequestException as e:
    print(f"Error sending request: {e}")

```

--------------------------------

### Get GPU by Id

Source: https://docs.runpod.io/sdks/python/apis

Retrieves details about a specific GPU model by its ID, useful for understanding capabilities and costs.

```APIDOC
## GET /gpus/{gpuId}

### Description
Fetches detailed information about a specific GPU model identified by its ID.

### Method
GET

### Endpoint
/gpus/{gpuId}

### Parameters
#### Path Parameters
- **gpuId** (string) - Required - The unique identifier of the GPU model.

#### Query Parameters
None

#### Request Body
None

### Request Example
None

### Response
#### Success Response (200)
- **maxGpuCount** (integer) - Maximum number of GPUs available for this model.
- **id** (string) - Unique identifier for the GPU model.
- **displayName** (string) - User-friendly name for the GPU.
- **manufacturer** (string) - Manufacturer of the GPU.
- **memoryInGb** (integer) - Amount of memory in GB.
- **cudaCores** (integer) - Number of CUDA cores.
- **secureCloud** (boolean) - Indicates if available on secure cloud.
- **communityCloud** (boolean) - Indicates if available on community cloud.
- **securePrice** (float) - Price on secure cloud.
- **communityPrice** (float) - Price on community cloud.
- **communitySpotPrice** (float) - Spot price on community cloud.
- **lowestPrice** (object) - Object containing lowest available pricing.
  - **minimumBidPrice** (float) - Minimum bid price.
  - **uninterruptablePrice** (float) - Uninterruptable price.

#### Response Example
```json
{
  "maxGpuCount": 8,
  "id": "NVIDIA A100 80GB PCIe",
  "displayName": "A100 80GB",
  "manufacturer": "Nvidia",
  "memoryInGb": 80,
  "cudaCores": 0,
  "secureCloud": true,
  "communityCloud": true,
  "securePrice": 1.89,
  "communityPrice": 1.59,
  "oneMonthPrice": null,
  "threeMonthPrice": null,
  "oneWeekPrice": null,
  "communitySpotPrice": 0.89,
  "secureSpotPrice": null,
  "lowestPrice": {
    "minimumBidPrice": 0.89,
    "uninterruptablePrice": 1.59
  }
}
```
```

--------------------------------

### UserPreference Object Example

Source: https://docs.runpod.io/references/graphql-spec

Represents user preferences, including category and specific settings stored as JSON. Used for customizing user experience and system behavior.

```json
{
  "id": "xyz789",
  "userId": "xyz789",
  "category": "GPU_TYPES",
  "preferences": {},
  "metadata": {},
  "createdAt": "2007-12-03T10:15:30Z",
  "updatedAt": "2007-12-03T10:15:30Z"
}
```

--------------------------------

### Example PodMigration Data Structure

Source: https://docs.runpod.io/references/graphql-spec

This JSON object illustrates the structure for PodMigration, detailing fields related to the source and target pods, machine IDs, migration status, progress, and timestamps. It's used to track the state of pod migrations.

```json
{
  "id": "xyz789",
  "sourcePodId": "abc123",
  "targetPodId": "abc123",
  "sourceMachineId": "abc123",
  "targetMachineId": "xyz789",
  "migrationType": "abc123",
  "sourceMount": "abc123",
  "status": "PENDING",
  "progress": 123,
  "message": "abc123",
  "createdAt": "2007-12-03T10:15:30Z",
  "updatedAt": "2007-12-03T10:15:30Z"
}
```

--------------------------------

### Build and Push Docker Image for vLLM Load Balancer

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

Provides bash commands to build the Docker image for the vLLM load balancer and push it to a container registry like Docker Hub. Replace 'YOUR_DOCKER_USERNAME' with your actual registry username and specify a tag (e.g., 'v1.0').

```bash
# Build the image
docker build --platform linux/amd64 -t YOUR_DOCKER_USERNAME/vllm-loadbalancer:v1.0 .

# Push to Docker Hub
docker push YOUR_DOCKER_USERNAME/vllm-loadbalancer:v1.0
```

--------------------------------

### Get Network Volume Billing History (OpenAPI)

Source: https://docs.runpod.io/api-reference/billing/GET/billing/networkvolumes

This OpenAPI specification defines the GET /billing/networkvolumes endpoint for retrieving network volume billing history. It specifies query parameters like bucketSize, endTime, and startTime, and details the structure of the returned billing records, including amount, disk space billed, and time.

```yaml
openapi: 3.0.3
info:
  title: Runpod API
  description: Public Rest API for managing Runpod programmatically.
  version: 0.1.0
  contact:
    name: help
    url: https://contact.runpod.io/hc/requests/new
    email: help@runpod.io
servers:
  - url: https://rest.runpod.io/v1
security:
  - ApiKey: []
tags:
  - name: docs
    description: This documentation page.
  - name: pods
    description: Manage Pods.
  - name: endpoints
    description: Manage Serverless endpoints.
  - name: network volumes
    description: Manage Runpod network volumes.
  - name: templates
    description: Manage Pod and Serverless templates.
  - name: container registry auths
    description: >-
      Manage authentication for container registries such as dockerhub to use
      private images.
  - name: billing
    description: Retrieve billing history for your Runpod account.
externalDocs:
  description: Find out more about Runpod.
  url: https://runpod.io
paths:
  /billing/networkvolumes:
    get:
      tags:
        - billing
      summary: Network volume billing history
      description: Retrieve billing information about your network volumes.
      operationId: NetworkVolumeBilling
      parameters:
        - name: bucketSize
          in: query
          schema:
            type: string
            enum:
              - hour
              - day
              - week
              - month
              - year
            default: day
            description: >-
              The length of each billing time bucket. The billing time bucket is
              the time range over which each billing record is aggregated.
        - name: endTime
          in: query
          schema:
            type: string
            format: date-time
            example: '2023-01-31T23:59:59Z'
            description: The end date of the billing period to retrieve.
        - name: startTime
          in: query
          schema:
            type: string
            format: date-time
            example: '2023-01-01T00:00:00Z'
            description: The start date of the billing period to retrieve.
      responses:
        '200':
          description: Successful operation.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/NetworkVolumeBillingRecords'
components:
  schemas:
    NetworkVolumeBillingRecords:
      type: array
      items:
        type: object
        properties:
          amount:
            type: number
            description: The amount charged for the group for the billing period, in USD.
            example: 100.5
          diskSpaceBilledGb:
            type: integer
            description: >-
              The amount of disk space billed for the billing period, in
              gigabytes (GB). Does not apply to all resource types.
            example: 50
          highPerformanceStorageAmount:
            type: number
            description: >-
              The amount charged for high performance storage for the billing
              period, in USD.
            example: 100.5
          highPerformanceStorageDiskSpaceBilledGb:
            type: integer
            description: >-
              The amount of high performance storage disk space billed for the
              billing period, in gigabytes (GB).
            example: 50
          time:
            type: string
            format: date-time
            description: The start of the period for which the billing record applies.
            example: '2023-01-01T00:00:00Z'
  securitySchemes:
    ApiKey:
      type: http
      scheme: bearer
      bearerFormat: Bearer

```

--------------------------------

### Integrate clean() into a Runpod handler

Source: https://docs.runpod.io/serverless/development/cleanup

Provides a complete example of a Runpod serverless handler that downloads an image, processes it, and then uses `clean()` to remove temporary image files. It includes error handling to ensure cleanup occurs even if the process fails.

```python
import runpod
from runpod.serverless.utils.rp_cleanup import clean
import requests
import os

def download_image(url, save_path):
    response = requests.get(url)
    if response.status_code == 200:
        with open(save_path, "wb") as file:
            file.write(response.content)
        return True
    return False

def handler(event):
    try:
        image_url = event["input"]["image_url"]

        # Create a temporary directory
        os.makedirs("temp_images", exist_ok=True)
        image_path = "temp_images/downloaded_image.jpg"

        # Download the image
        if not download_image(image_url, image_path):
            raise Exception("Failed to download image")

        # Process the image (your code here)
        result = f"Processed image from: {image_url}"

        # Cleanup specific folders after processing
        clean(folder_list=["temp_images"])

        return {"output": result}
    except Exception as e:
        # Attempt cleanup even if an error occurs
        clean(folder_list=["temp_images"])
        return {"error": str(e)}

runpod.serverless.start({"handler": handler})
```

--------------------------------

### Specifics Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents specific details about an item, including stock status and pricing information like securePrice and slsPrice.

```json
{
  "stockStatus": "abc123",
  "securePrice": 987.65,
  "slsPrice": 123.45
}
```

--------------------------------

### Generate Video from Image using WAN 2.5

Source: https://docs.runpod.io/hub/public-endpoint-reference

WAN 2.5 generates videos from static images. It requires a prompt describing the desired video content and a URL to the source image. Optional parameters include negative prompts, video size, duration, seed, prompt expansion, and safety checker.

```json
{
  "input": {
    "prompt": "A stand-up comedian delivering a dad joke",
    "image": "https://image.runpod.ai/uploads/fccSIh7CTx/5abfc82d-44f4-4318-9518-7fdba0b285d9.png",
    "negative_prompt": "",
    "size": "1280*720",
    "duration": 5,
    "seed": -1,
    "enable_prompt_expansion": false,
    "enable_safety_checker": true
  }
}
```

--------------------------------

### Get GPU Type by ID

Source: https://docs.runpod.io/sdks/graphql/manage-pods

Retrieve details for a specific GPU type by its ID, including pricing information.

```APIDOC
## Get GPU Type by ID

### Description
Retrieve details for a specific GPU type by its ID, including pricing information.

### Method
POST

### Endpoint
https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}

### Query Parameters
- **query** (string) - Required - The GraphQL query string.

### Request Body
```json
{
  "query": "query GpuTypes { gpuTypes(input: {id: \"NVIDIA GeForce RTX 3090\"}) { id displayName memoryInGb secureCloud communityCloud lowestPrice(input: {gpuCount: 1}) { minimumBidPrice uninterruptablePrice } } }"
}
```

### Response
#### Success Response (200)
- **data.gpuTypes** (array) - An array containing the requested GPU type object.
  - **id** (string) - The unique identifier for the GPU type.
  - **displayName** (string) - The human-readable name of the GPU type.
  - **memoryInGb** (integer) - The amount of VRAM in gigabytes.
  - **secureCloud** (boolean) - Indicates if the GPU is available on the secure cloud.
  - **communityCloud** (boolean) - Indicates if the GPU is available on the community cloud.
  - **lowestPrice** (object) - Pricing information for the GPU.
    - **minimumBidPrice** (float) - The minimum bid price for the GPU.
    - **uninterruptablePrice** (float) - The uninterruptible price for the GPU.

#### Response Example
```json
{
  "data": {
    "gpuTypes": [
      {
        "id": "NVIDIA GeForce RTX 3090",
        "displayName": "RTX 3090",
        "memoryInGb": 24,
        "secureCloud": false,
        "communityCloud": true,
        "lowestPrice": {
          "minimumBidPrice": 0.163,
          "uninterruptablePrice": 0.3
        }
      }
    ]
  }
}
```
```

--------------------------------

### GET /billing

Source: https://docs.runpod.io/api-reference/billing/GET/billing/endpoints

Retrieves billing records for your RunPod account. You can filter and group these records by various parameters such as GPU type, image name, date range, and template ID.

```APIDOC
## GET /billing

### Description
Retrieves billing records for your RunPod account. You can filter and group these records by various parameters such as GPU type, image name, date range, and template ID.

### Method
GET

### Endpoint
/billing

### Parameters
#### Query Parameters
- **gpuType** (string) - Optional - Filter to endpoints with the provided GPU type attached.
  - Example GPUs: NVIDIA GeForce RTX 3070, AMD Instinct MI300X OAM, NVIDIA GeForce RTX 4080 SUPER, Tesla V100-PCIE-16GB, Tesla V100-SXM2-32GB, NVIDIA RTX 5000 Ada Generation, NVIDIA GeForce RTX 4070 Ti, NVIDIA RTX 4000 SFF Ada Generation, NVIDIA GeForce RTX 3090 Ti, NVIDIA RTX A2000, NVIDIA GeForce RTX 4080, NVIDIA A30, NVIDIA GeForce RTX 5080, Tesla V100-FHHL-16GB, NVIDIA H200 NVL, Tesla V100-SXM2-16GB, NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, NVIDIA A5000 Ada, Tesla V100-PCIE-32GB, NVIDIA  RTX A4500, NVIDIA  A30, NVIDIA GeForce RTX 3080TI, Tesla T4, NVIDIA RTX A30
  - Example: NVIDIA GeForce RTX 4090
- **grouping** (string) - Optional - Group the billing records by the provided field.
  - Allowed values: endpointId, podId, gpuTypeId
  - Default: endpointId
- **imageName** (string) - Optional - Filter to endpoints created with the provided image.
  - Example: runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04
- **startTime** (string) - Optional - The start date of the billing period to retrieve.
  - Format: date-time
  - Example: '2023-01-01T00:00:00Z'
- **templateId** (string) - Optional - Filter to endpoints created from the provided template.
  - Example: 30zmvf89kd

### Response
#### Success Response (200)
- **amount** (number) - The amount charged for the group for the billing period, in USD.
- **diskSpaceBilledGb** (integer) - The amount of disk space billed for the billing period, in gigabytes (GB). Does not apply to all resource types.
- **endpointId** (string) - If grouping by endpoint ID, the endpoint ID of the group.
- **gpuTypeId** (string) - If grouping by GPU type ID, the GPU type ID of the group.
- **podId** (string) - If grouping by Pod ID, the Pod ID of the group.
- **time** (string) - The start of the period for which the billing record applies.
- **timeBilledMs** (integer) - The total time billed for the billing period, in milliseconds. Does not apply to all resource types.

#### Response Example
```json
[
  {
    "amount": 100.5,
    "diskSpaceBilledGb": 50,
    "endpointId": "ep-12345",
    "gpuTypeId": "gpu-abcde",
    "podId": "pod-67890",
    "time": "2023-01-01T00:00:00Z",
    "timeBilledMs": 3600000
  }
]
```
```

--------------------------------

### StorageClusterInfo Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object details information about a storage cluster, including its ID, connection details, type, mount path, and storage type.

```json
{
  "id": "xyz789",
  "connectionDetails": {},
  "isPrimary": true,
  "type": "abc123",
  "mountPath": "abc123",
  "storageType": "MOOSE_FS",
  "isNextGenStorage": true
}
```

--------------------------------

### Endpoint Statistic Granularity Input (JSON)

Source: https://docs.runpod.io/references/graphql-spec

Example input for specifying the granularity of endpoint statistics. The 'granularity' field accepts values such as 'LIVE'.

```json
{"granularity": "LIVE"}
```

--------------------------------

### WAN 2.1 I2V 720p

Source: https://docs.runpod.io/hub/public-endpoint-reference

Open-source AI image-to-video generation using a diffusion transformer architecture at 720p.

```APIDOC
## POST /v2/wan-2-1-i2v-720/

### Description
Generates video from an input image using a diffusion transformer architecture at 720p resolution.

### Method
POST

### Endpoint
`https://api.runpod.ai/v2/wan-2-1-i2v-720/`

### Parameters
#### Query Parameters
- **input_image** (string) - Required - A URL to the input image.
- **prompt** (string) - Optional - A text prompt to guide the video generation.
- **duration** (integer) - Optional - The desired duration of the video in seconds.

### Request Example
```json
{
  "input_image": "https://example.com/images/input.jpg",
  "prompt": "The image transforms into a watercolor painting",
  "duration": 5
}
```

### Response
#### Success Response (200)
- **video_url** (string) - The URL to the generated 720p video.

#### Response Example
```json
{
  "video_url": "https://storage.runpod.ai/videos/wan2.1_i2v_output.mp4"
}
```
```

--------------------------------

### UserReservation Object Example

Source: https://docs.runpod.io/references/graphql-spec

Details a user's reservation, linking a user to a specific reservation with timestamps. Used for managing and tracking resource reservations.

```json
{
  "userId": "xyz789",
  "reservationId": "xyz789",
  "createdAt": "2007-12-03T10:15:30Z",
  "updatedAt": "2007-12-03T10:15:30Z",
  "user": User,
  "reservation": Reservation
}
```

--------------------------------

### UserInformation Object Example

Source: https://docs.runpod.io/references/graphql-spec

Represents detailed user profile information, including name, address, and company details. This structure is used for user account management and verification.

```json
{
  "firstName": "abc123",
  "lastName": "xyz789",
  "addressLine1": "abc123",
  "addressLine2": "xyz789",
  "countryCode": "abc123",
  "companyName": "abc123",
  "companyIdentification": "abc123",
  "taxIdentification": "abc123"
}
```

--------------------------------

### Initialize OpenAI Client for Runpod

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

Initializes an OpenAI client to interact with Runpod's vLLM workers. This requires your Runpod API key and the specific endpoint ID for your deployed model. The `base_url` is constructed using the provided endpoint ID and the OpenAI compatible API path.

```python
from openai import OpenAI

MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.2"  # Use your deployed model

# Replace ENDPOINT_ID and RUNPOD_API_KEY with your actual values
client = OpenAI(
    api_key="RUNPOD_API_KEY",
    base_url="https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1",
)
```

--------------------------------

### Send a Run Request to Ollama Server

Source: https://docs.runpod.io/tutorials/serverless/run-ollama-inference

Example JSON payload for sending an inference request to a deployed Ollama server. It includes the method name and input parameters for the prompt.

```json
{
  "input": {
    "method_name": "generate",
    "input": {
      "prompt": "why the sky is blue?"
    }
  }
}
```

--------------------------------

### ComputeType Enum Example

Source: https://docs.runpod.io/references/graphql-spec

This snippet demonstrates the ComputeType enum, which specifies the type of compute resource, such as CPU or GPU.

```json
"CPU"
```

--------------------------------

### OpenAI API Compatibility - Base URL Structure

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

This section details the base URL pattern for making OpenAI-compatible API requests to your vLLM workers on Runpod.

```APIDOC
## OpenAI API Compatibility - Base URL Structure

You can make OpenAI-compatible API requests to your vLLM workers by sending requests to this base URL pattern:

```
https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1
```

**Note:** Replace `ENDPOINT_ID` with your actual Runpod endpoint ID.
```

--------------------------------

### Serverless Endpoint Configuration Parameters

Source: https://docs.runpod.io/api-reference/endpoints/GET/endpoints/endpointId

This section details the various parameters used to configure a Serverless endpoint, including GPU selection, scaling behavior, and network volume attachments.

```APIDOC
## Serverless Endpoint Configuration Parameters

### Description
This section details the various parameters used to configure a Serverless endpoint, including GPU selection, scaling behavior, and network volume attachments.

### Parameters

#### Request Body Parameters

- **max_request_time_ms** (integer) - Optional - The maximum number of milliseconds an individual request can run on a Serverless endpoint before the worker is stopped and the request is marked as failed.
- **gpuCount** (integer) - Optional - The number of GPUs attached to each worker on a Serverless endpoint.
- **gpuTypeIds** (array of strings) - Optional - A list of Runpod GPU types which can be attached to a Serverless endpoint. 
  - Possible values: `NVIDIA GeForce RTX 4090`, `NVIDIA A40`, `NVIDIA RTX A5000`, `NVIDIA GeForce RTX 5090`, `NVIDIA H100 80GB HBM3`, `NVIDIA GeForce RTX 3090`, `NVIDIA RTX A4500`, `NVIDIA L40S`, `NVIDIA H200`, `NVIDIA L4`, `NVIDIA RTX 6000 Ada Generation`, `NVIDIA A100-SXM4-80GB`, `NVIDIA RTX 4000 Ada Generation`, `NVIDIA RTX A6000`, `NVIDIA A100 80GB PCIe`, `NVIDIA RTX 2000 Ada Generation`, `NVIDIA RTX A4000`, `NVIDIA RTX PRO 6000 Blackwell Server Edition`, `NVIDIA H100 PCIe`, `NVIDIA H100 NVL`, `NVIDIA L40`, `NVIDIA B200`, `NVIDIA GeForce RTX 3080 Ti`, `NVIDIA RTX PRO 6000 Blackwell Workstation Edition`, `NVIDIA GeForce RTX 3080`, `NVIDIA GeForce RTX 3070`, `AMD Instinct MI300X OAM`, `NVIDIA GeForce RTX 4080 SUPER`, `Tesla V100-PCIE-16GB`, `Tesla V100-SXM2-32GB`, `NVIDIA RTX 5000 Ada Generation`, `NVIDIA GeForce RTX 4070 Ti`, `NVIDIA RTX 4000 SFF Ada Generation`, `NVIDIA GeForce RTX 3090 Ti`, `NVIDIA RTX A2000`, `NVIDIA GeForce RTX 4080`, `NVIDIA A30`, `NVIDIA GeForce RTX 5080`, `Tesla V100-FHHL-16GB`, `NVIDIA H200 NVL`, `Tesla V100-SXM2-16GB`, `NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition`, `NVIDIA A5000 Ada`, `Tesla V100-PCIE-32GB`, `NVIDIA  RTX A4500`, `NVIDIA  A30`, `NVIDIA GeForce RTX 3080TI`, `Tesla T4`, `NVIDIA RTX A30`
- **id** (string) - Optional - A unique string identifying a Serverless endpoint.
- **idleTimeout** (integer) - Optional - The number of seconds a worker on a Serverless endpoint can be running without taking a job before the worker is scaled down.
- **instanceIds** (array of strings) - Optional - For CPU Serverless endpoints, a list of instance IDs that can be attached to a Serverless endpoint.
- **name** (string) - Optional - A user-defined name for a Serverless endpoint. The name does not need to be unique.
- **networkVolumeId** (string) - Optional - The unique string identifying the network volume to attach to the Serverless endpoint.
- **networkVolumeIds** (array of strings) - Optional - A list of network volume IDs attached to the Serverless endpoint. Allows multiple network volumes to be used with multi-region endpoints.
- **scalerType** (string) - Optional - The method used to scale up workers on a Serverless endpoint. 
  - Possible values: `QUEUE_DELAY`, `REQUEST_COUNT`
- **scalerValue** (integer) - Optional - If the endpoint scalerType is QUEUE_DELAY, the number of seconds a request can remain in queue before a new worker is scaled up. If the endpoint scalerType is REQUEST_COUNT, the number of workers is calculated based on the number of requests in the endpoint's queue.

### Request Example
```json
{
  "max_request_time_ms": 30000,
  "gpuCount": 1,
  "gpuTypeIds": ["NVIDIA RTX 4090"],
  "idleTimeout": 10,
  "name": "my-serverless-endpoint",
  "networkVolumeIds": ["vol-123abc", "vol-456def"],
  "scalerType": "QUEUE_DELAY",
  "scalerValue": 5
}
```

### Response

#### Success Response (200)

- **id** (string) - A unique string identifying the Serverless endpoint.
- **name** (string) - The user-defined name for the Serverless endpoint.
- **gpuCount** (integer) - The number of GPUs attached to each worker.
- **gpuTypeIds** (array of strings) - The list of GPU types attached to the endpoint.
- **max_request_time_ms** (integer) - The maximum time in milliseconds a request can run.
- **idleTimeout** (integer) - The idle timeout in seconds for workers.
- **networkVolumeIds** (array of strings) - The list of network volume IDs attached to the endpoint.
- **scalerType** (string) - The scaling method used for the endpoint.
- **scalerValue** (integer) - The value associated with the scaling method.

#### Response Example
```json
{
  "id": "jpnw0v75y3qoql",
  "name": "my-serverless-endpoint",
  "gpuCount": 1,
  "gpuTypeIds": ["NVIDIA RTX 4090"],
  "max_request_time_ms": 30000,
  "idleTimeout": 10,
  "networkVolumeIds": ["vol-123abc", "vol-456def"],
  "scalerType": "QUEUE_DELAY",
  "scalerValue": 5
}
```
```

--------------------------------

### User Machines Filter API

Source: https://docs.runpod.io/references/graphql-spec

Defines filters for user machines, specifying a start and end range.

```APIDOC
## UserMachinesFilter

### Description
Filters for user machines based on a start and end integer value.

### Fields
- **start** (Int) - The starting point for the filter.
- **end** (Int) - The ending point for the filter.

### Example
```json
{"start": 123, "end": 123}
```
```

--------------------------------

### TypeScript: Image Generation with Runpod and Vercel AI SDK

Source: https://docs.runpod.io/hub/public-endpoints

Example TypeScript code for image generation using Runpod's Flux Dev model via the Vercel AI SDK. It employs the `experimental_generateImage` function and specifies the model using `runpod.imageModel()`, along with the prompt and desired aspect ratio.

```typescript
import { runpod } from '@runpod/ai-sdk-provider';
import { experimental_generateImage as generateImage } from 'ai';

const { image } = await generateImage({
  model: runpod.imageModel('flux/flux-dev'),
  prompt: 'A serene mountain landscape at sunset',
  aspectRatio: '4:3',
});
```

--------------------------------

### MachineEarning Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents earning data for a machine on a specific date. It includes the machine's name, ID, the date of earnings, and breakdowns of earnings from host total, GPU, and disk. This is crucial for financial tracking.

```json
{
  "name": "abc123",
  "date": "2007-12-03T10:15:30Z",
  "machineId": "xyz789",
  "hostTotalEarnings": 123.45,
  "hostGpuEarnings": 123.45,
  "hostDiskEarnings": 123.45
}
```

--------------------------------

### Example PodRegistry Data Structure

Source: https://docs.runpod.io/references/graphql-spec

This JSON object demonstrates the structure for PodRegistry, including authentication credentials such as auth token, password, URL, user, and username. It's used for configuring access to container registries.

```json
{
  "auth": "abc123",
  "pass": "abc123",
  "url": "abc123",
  "user": "xyz789",
  "username": "xyz789"
}
```

--------------------------------

### GET /endpoints/{endpointId}

Source: https://docs.runpod.io/api-reference/endpoints/GET/endpoints/endpointId

Retrieves details for a specific serverless endpoint by its ID. It can optionally include information about the template used and the workers running on the endpoint.

```APIDOC
## GET /endpoints/{endpointId}

### Description
Returns a single endpoint by its ID. You can optionally include template and worker information.

### Method
GET

### Endpoint
/endpoints/{endpointId}

#### Path Parameters
- **endpointId** (string) - Required - ID of endpoint to return.

#### Query Parameters
- **includeTemplate** (boolean) - Optional - Include information about the template used to create the endpoint. Defaults to false.
- **includeWorkers** (boolean) - Optional - Include information about the workers running on the endpoint. Defaults to false.

### Request Example
```
GET https://rest.runpod.io/v1/endpoints/your-endpoint-id?includeTemplate=true&includeWorkers=true
```

### Response
#### Success Response (200)
- **allowedCudaVersions** (array[string]) - A list of acceptable CUDA versions for the workers on a Serverless endpoint.
- **computeType** (string) - The type of compute used by workers on a Serverless endpoint (CPU or GPU).
- **createdAt** (string) - The UTC timestamp when a Serverless endpoint was created.
- **dataCenterIds** (array[string]) - A list of Runpod data center IDs where workers can be located.
- **env** (object) - Environment variables for the endpoint.
- **executionTimeoutMs** (integer) - The maximum time in milliseconds that a worker can run.

#### Response Example
```json
{
  "id": "your-endpoint-id",
  "name": "My Endpoint",
  "templateId": "your-template-id",
  "desiredReplicas": 1,
  "minReplicas": 0,
  "maxReplicas": 1,
  "ports": [
    {
      "port": 8000,
      "protocol": "tcp",
      "isPublic": true
    }
  ],
  "gpuType": {
    "id": "NVIDIA-RTX-3090"
  },
  "networkVolumeId": null,
  "networkVolumeMountPath": null,
  "state": "RUNNING",
  "health": "HEALTHY",
  "templateVersion": "1.0.0",
  "createdAt": "2024-07-12T19:14:40.144Z",
  "lastError": null,
  "lastErrorTimestamp": null,
  "lastHealthCheckTimestamp": "2024-07-12T19:15:00.000Z",
  "lastUpdated": "2024-07-12T19:14:40.144Z",
  "ready": true,
  "readyTimestamp": "2024-07-12T19:14:45.000Z",
  "desiredState": "RUNNING",
  "isServerless": true,
  "billing": {
    "startTime": "2024-07-12T19:14:40.144Z",
    "stopTime": null,
    "cost": 0.0000000000,
    "currency": "USD"
  },
  "allowedCudaVersions": [
    "12.1"
  ],
  "computeType": "GPU",
  "dataCenterIds": [
    "EU-RO-1"
  ],
  "env": {
    "MODEL_LOAD_TIMEOUT_SECONDS": "3600"
  },
  "executionTimeoutMs": 600000
}
```

#### Error Response (400, 404)
- **400**: Invalid ID supplied.
- **404**: Endpoint not found.
```

--------------------------------

### Example PodRuntimeContainer Data Structure

Source: https://docs.runpod.io/references/graphql-spec

This JSON object details the CPU and memory utilization percentages for a pod's container. It helps in monitoring the resource consumption of the running container.

```json
{"cpuPercent": 123, "memoryPercent": 987}
```

--------------------------------

### Accessing Environment Variables in Python, Node.js, and Bash

Source: https://docs.runpod.io/pods/templates/environment-variables

Demonstrates how to access environment variables in Python using `os.environ`, in Node.js using `process.env`, and in Bash scripts using parameter expansion. These examples show how to retrieve variable values and provide default values if a variable is not set.

```python
import os

model_name = os.environ.get('MODEL_NAME', 'default-model')
api_key = os.environ['API_KEY']  # Raises error if not found
```

```javascript
const modelName = process.env.MODEL_NAME || 'default-model';
const apiKey = process.env.API_KEY;
```

```bash
#!/bin/bash
MODEL_NAME=${MODEL_NAME:-"default-model"}
echo "Using model: $MODEL_NAME"
```

--------------------------------

### UserStorageBilling Object Example

Source: https://docs.runpod.io/references/graphql-spec

Represents billing details for storage usage, including various types of disk space and network storage. Used for tracking and analyzing storage costs.

```json
{
  "time": "2007-12-03T10:15:30Z",
  "amount": 987.65,
  "diskSpaceBilledGB": 123,
  "networkStorageDiskSpaceBilledGB": 123,
  "networkStorageAmount": 123.45,
  "gpuDiskSpaceBilledGB": 123,
  "gpuStorageAmount": 123.45,
  "cpuDiskSpaceBilledGB": 123,
  "cpuStorageAmount": 123.45,
  "slsDiskSpaceBilledGB": 123,
  "slsStorageAmount": 123.45
}
```

--------------------------------

### GET /billing/pods

Source: https://docs.runpod.io/api-reference/billing/GET/billing/pods

Retrieve billing information about your Pods. This endpoint allows you to fetch historical billing data for your Runpod pods, with options to filter and group the results.

```APIDOC
## GET /billing/pods

### Description
Retrieve billing information about your Pods. This endpoint allows you to fetch historical billing data for your Runpod pods, with options to filter and group the results.

### Method
GET

### Endpoint
/billing/pods

### Parameters
#### Query Parameters
- **bucketSize** (string) - Optional - The length of each billing time bucket. The billing time bucket is the time range over which each billing record is aggregated. Enum: hour, day, week, month, year. Default: day
- **endTime** (string) - Optional - The end date of the billing period to retrieve. Format: date-time. Example: '2023-01-31T23:59:59Z'
- **gpuTypeId** (string) - Optional - Filter to Pods with the provided GPU type attached. Example: NVIDIA GeForce RTX 4090
- **grouping** (string) - Optional - Group the billing records by the provided field. Enum: podId, gpuTypeId. Default: gpuTypeId
- **podId** (string) - Optional - Filter to a specific Pod. Example: xedezhzb9la3ye
- **startTime** (string) - Optional - The start date of the billing period to retrieve. Format: date-time. Example: '2023-01-01T00:00:00Z'

### Response
#### Success Response (200)
- **amount** (number) - The billing amount.

#### Response Example
```json
[
  {
    "amount": 10.50
  }
]
```
```

--------------------------------

### Run Runpod Storage REST API Server

Source: https://docs.runpod.io/community-solutions/runpod-network-volume-storage-tool

Starts a self-hosted REST API server that acts as a proxy to Runpod's storage API. This allows integration with other applications via HTTP requests.

```bash
uv run runpod-storage-server --host 0.0.0.0 --port 8000
```

--------------------------------

### Choosing Custom Framework Base Image

Source: https://docs.runpod.io/serverless/workers/create-dockerfile

Build upon specialized base images tailored for specific frameworks like PyTorch, which may include pre-installed libraries and CUDA support.

```dockerfile
FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
```

--------------------------------

### GET /containerregistryauth/{containerRegistryAuthId}

Source: https://docs.runpod.io/api-reference/container-registry-auths/GET/containerregistryauth/containerRegistryAuthId

Retrieves a specific container registry authentication configuration by its unique ID. This is useful for accessing details of stored credentials for private container registries.

```APIDOC
## GET /containerregistryauth/{containerRegistryAuthId}

### Description
Returns a single container registry auth.

### Method
GET

### Endpoint
/containerregistryauth/{containerRegistryAuthId}

### Parameters
#### Path Parameters
- **containerRegistryAuthId** (string) - Required - ID of container registry auth to return.

### Request Example
```json
{
  "example": ""
}
```

### Response
#### Success Response (200)
- **id** (string) - A unique string identifying a container registry authentication.
- **name** (string) - A user-defined name for a container registry authentication. The name must be unique.

#### Response Example
```json
{
  "id": "clzdaifot0001l90809257ynb",
  "name": "my creds"
}
```
```

--------------------------------

### UserMachinesFilter Object Example

Source: https://docs.runpod.io/references/graphql-spec

Defines filters for querying user machines, specifying a time range. Used to retrieve machine data within a specific period.

```json
{"start": 123, "end": 123}
```

--------------------------------

### POST /pod

Source: https://docs.runpod.io/api-reference/pods/POST/pods

Creates a new Pod on RunPod. This endpoint allows for detailed configuration of the Pod, including GPU type, image, network settings, and storage.

```APIDOC
## POST /pod

### Description
Creates a new Pod on RunPod. This endpoint allows for detailed configuration of the Pod, including GPU type, image, network settings, and storage.

### Method
POST

### Endpoint
/pod

### Parameters
#### Query Parameters
- **gpuTypePriority** (string) - Optional - If the created Pod is a GPU Pod, set to availability to respond to current GPU type availability. Set to custom to always try to rent GPU types in the order specified in gpuTypeIds.
- **templateId** (string) - Optional - If the Pod is created with a template, the unique string identifying that template.

#### Request Body
- **name** (string) - Optional - A user-defined name for the created Pod. The name does not need to be unique. (maxLength: 191)
- **imageName** (string) - Required - The image tag for the container run on the created Pod.
- **gpuAvailable** (array) - Optional - A list of Runpod GPU types which can be attached to the created Pod. The order of the list determines the order to rent GPU types.
- **ports** (array) - Optional - A list of ports exposed on the created Pod. Each port is formatted as [port number]/[protocol]. Protocol can be either http or tcp. (default: "8888/http,22/tcp")
- **volumeInGb** (integer) - Optional - The amount of disk space, in gigabytes (GB), to allocate on the Pod volume for the created Pod. The data on the Pod volume is persisted across Pod restarts. To persist data so that future Pods can access it, create a network volume and set networkVolumeId to attach it to the Pod. (default: 20)
- **volumeMountPath** (string) - Optional - The mount path for the Pod volume. (default: "/workspace")
- **networkVolumeId** (string) - Optional - The unique string identifying the network volume to attach to the created Pod. If attached, a network volume replaces the Pod network volume.
- **minDiskBandwidthMBps** (number) - Optional - The minimum disk bandwidth, in megabytes per second (MBps), for the created Pod.
- **minDownloadMbps** (number) - Optional - The minimum download speed, in megabits per second (Mbps), for the created Pod.
- **minUploadMbps** (number) - Optional - The minimum upload speed, in megabits per second (Mbps), for the created Pod.
- **minRAMPerGPU** (integer) - Optional - If the created Pod is a GPU Pod, the minimum amount of RAM, in gigabytes (GB), allocated to the created Pod for each GPU attached to the Pod. (default: 8)
- **minVCPUPerGPU** (integer) - Optional - If the created Pod is a GPU Pod, the minimum number of virtual CPUs allocated to the created Pod for each GPU attached to the Pod. (default: 2)
- **vcpuCount** (integer) - Optional - If the created Pod is a CPU Pod, the number of vCPUs allocated to the Pod. (default: 2)
- **interruptible** (boolean) - Optional - Set to true to create an interruptible or spot Pod. An interruptible Pod can be rented at a lower cost but can be stopped at any time to free up resources for another Pod. A reserved Pod is rented at a higher cost but runs until it exits or is manually stopped. (default: false)
- **locked** (boolean) - Optional - Set to true to lock a Pod. Locking a Pod disables stopping or resetting the Pod. (default: false)
- **supportPublicIp** (boolean) - Optional - If the created Pod is on Community Cloud, set to true if you need the Pod to expose a public IP address. If null, the Pod might not have a public IP address. On Secure Cloud, the Pod will always have a public IP address.

### Request Example
```json
{
  "name": "my-gpu-pod",
  "imageName": "runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04",
  "gpuAvailable": [
    "NVIDIA A5000 Ada",
    "Tesla V100-PCIE-32GB"
  ],
  "ports": [
    "8888/http",
    "22/tcp"
  ],
  "volumeInGb": 100,
  "interruptible": true,
  "supportPublicIp": true
}
```

### Response
#### Success Response (200)
- **podId** (string) - The unique identifier for the created Pod.
- **networkId** (string) - The network identifier for the created Pod.
- **ports** (array) - A list of ports exposed on the created Pod.

#### Response Example
```json
{
  "podId": "65d0f0a2f4d3a80012345678",
  "networkId": "net-abcdef1234567890",
  "ports": [
    "8888/http",
    "22/tcp"
  ]
}
```
```

--------------------------------

### Get Job Status

Source: https://docs.runpod.io/sdks/go/endpoints

Retrieve the status of a specific job run by its ID. This is useful for monitoring the progress of your tasks.

```APIDOC
## GET /runpod/status

### Description
Retrieves the status of a specific job run using its unique identifier.

### Method
GET

### Endpoint
/runpod/status

### Parameters
#### Query Parameters
- **id** (string) - Required - The unique identifier of the run to get the status for.

### Request Example
```bash
GET /runpod/status?id=5efff030-686c-4179-85bb-31b9bf97b944-u1
```

### Response
#### Success Response (200)
- **id** (string) - The unique identifier of the run.
- **status** (string) - The current status of the run (e.g., IN_PROGRESS, COMPLETED, FAILED).
- **delayTime** (integer) - The delay time in seconds.
- **started** (boolean) - Indicates if the run has started.
- **completed** (boolean) - Indicates if the run has completed.
- **succeeded** (boolean) - Indicates if the run has succeeded.

#### Response Example
```json
{
  "delayTime": 18,
  "id": "792b1497-b2c8-4c95-90bf-4e2a6a2a37ff-u1",
  "status": "IN_PROGRESS",
  "started": true,
  "completed": false,
  "succeeded": false
}
```
```

--------------------------------

### MachineBalance Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents the balance information for a machine, detailing its earnings from disk and GPU usage, as well as the total earnings. This is useful for tracking machine profitability.

```json
{
  "hostDiskEarnings": 987.65,
  "hostGpuEarnings": 123.45,
  "hostTotalEarnings": 987.65
}
```

--------------------------------

### UserRunpodEndpointBilling Object Example

Source: https://docs.runpod.io/references/graphql-spec

Represents billing information for Runpod endpoints, including the time of billing and the amount charged. Used for tracking endpoint usage costs.

```json
{
  "time": "2007-12-03T10:15:30Z",
  "amount": 987.65
}
```

--------------------------------

### Validate Critical Environment Variables on Startup (Python)

Source: https://docs.runpod.io/serverless/development/environment-variables

This Python code implements a best practice for validating critical environment variables at the start of the handler. It defines a list of required variables and checks if they are present in the environment. If any are missing, it raises a `ValueError` with a clear message listing the missing variables.

```python
import os
import runpod

# Validate environment variables on startup
required_vars = ["API_KEY", "SERVICE_URL"]
missing_vars = [var for var in required_vars if not os.environ.get(var)]

if missing_vars:
    raise ValueError(f"Missing required environment variables: {', '.join(missing_vars)}")

def handler(event):
    # Your handler logic here
    pass

runpod.serverless.start({"handler": handler})
```

--------------------------------

### POST /graphql - Create Serverless Template

Source: https://docs.runpod.io/sdks/graphql/manage-pod-templates

This endpoint allows you to create a new serverless template. For serverless templates, `volumeInGb` must be set to 0 as they do not have persistent storage.

```APIDOC
## POST /graphql - Create Serverless Template

### Description
Creates a new serverless template. Ensure `volumeInGb` is set to 0 for serverless templates.

### Method
POST

### Endpoint
`https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}`

### Parameters
#### Query Parameters
- **api_key** (string) - Required - Your RunPod API key.

#### Request Body
- **query** (string) - Required - The GraphQL mutation for saving a template.
  - **input** (object) - Required - Input object for saving the template.
    - **containerDiskInGb** (integer) - Required - Size of the container disk in GB.
    - **dockerArgs** (string) - Required - Arguments to pass to the Docker container.
    - **env** (array of objects) - Optional - Environment variables for the container.
      - **key** (string) - Required - The environment variable key.
      - **value** (string) - Required - The environment variable value.
    - **imageName** (string) - Required - The Docker image name and tag.
    - **isServerless** (boolean) - Required - Set to `true` for serverless templates.
    - **name** (string) - Required - The name of the template.
    - **readme** (string) - Optional - A README description for the template.
    - **volumeInGb** (integer) - Required - Size of the persistent volume in GB. Must be 0 for serverless templates.

### Request Example
```json
{
  "query": "mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \"python handler.py\", env: [ { key: \"key1\", value: \"value1\" }, { key: \"key2\", value: \"value2\" } ], imageName: \"runpod/serverless-hello-world:latest\", isServerless: true, name: \"Generated Serverless Template\", readme: \"## Hello, World!\", volumeInGb: 0 }) { containerDiskInGb dockerArgs env { key value } id imageName isServerless name readme } }"
}
```

### Response
#### Success Response (200)
- **data.saveTemplate** (object) - The details of the saved template.
  - **containerDiskInGb** (integer) - Size of the container disk in GB.
  - **dockerArgs** (string) - Arguments passed to the Docker container.
  - **env** (array of objects) - Environment variables.
    - **key** (string) - Environment variable key.
    - **value** (string) - Environment variable value.
  - **id** (string) - The unique identifier for the template.
  - **imageName** (string) - The Docker image used.
  - **isServerless** (boolean) - Indicates if the template is serverless.
  - **name** (string) - The name of the template.
  - **readme** (string) - The README description.

#### Response Example
```json
{
  "data": {
    "saveTemplate": {
      "containerDiskInGb": 5,
      "dockerArgs": "python handler.py",
      "env": [
        {
          "key": "key1",
          "value": "value1"
        },
        {
          "key": "key2",
          "value": "value2"
        }
      ],
      "id": "xkhgg72fuo",
      "imageName": "runpod/serverless-hello-world:latest",
      "isServerless": true,
      "name": "Generated Serverless Template",
      "readme": "## Hello, World!"
    }
  }
}
```
```

--------------------------------

### Configure AWS CLI for Runpod S3 API

Source: https://docs.runpod.io/serverless/storage/s3-api

This snippet demonstrates how to configure the AWS CLI to use the Runpod S3-compatible API. It involves installing the AWS CLI, running the configure command, and providing the necessary credentials (access key ID and secret access key) obtained from the Runpod console. This configuration allows users to interact with their network volumes using the AWS CLI.

```bash
aws configure
```

--------------------------------

### WebhookRequestStatus Object Example

Source: https://docs.runpod.io/references/graphql-spec

Represents the status of webhook requests, including the time of the request and a list of response codes. Used for monitoring webhook communication.

```json
{
  "time": "2007-12-03T10:15:30Z",
  "responses": [987]
}
```

--------------------------------

### List SSH Keys with RunPod CLI

Source: https://docs.runpod.io/runpodctl/reference/runpodctl-ssh-list-keys

Displays all SSH keys associated with your RunPod account. This command requires the runpodctl CLI to be installed and configured.

```sh
runpodctl ssh list-keys
```

--------------------------------

### MachineBenchmark Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object provides details about a machine's benchmark results. It includes any errors encountered, the timestamp of the benchmark, the number of GPUs used, and associated pod information. This helps in evaluating machine performance.

```json
{
  "errors": "xyz789",
  "benchmarkedAt": "2007-12-03T10:15:30Z",
  "gpuCount": 987,
  "pod": BenchmarkPod
}
```

--------------------------------

### Set up SSH Password on Destination Pod (Bash)

Source: https://docs.runpod.io/community-solutions/ssh-password-migration/overview

This script sets up an SSH password on a destination Runpod instance, enabling secure remote access for data migration. It requires terminal access and the ability to download files. The output provides connection details needed for the transfer.

```bash
wget https://raw.githubusercontent.com/justinwlin/Runpod-SSH-Password/main/passwordrunpod.sh && chmod +x passwordrunpod.sh && ./passwordrunpod.sh
```

--------------------------------

### OpenAI API Compatibility - Model Naming

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

Explains the importance of the `MODEL_NAME` environment variable for identifying deployed models in OpenAI-compatible API requests.

```APIDOC
## OpenAI API Compatibility - Model Naming

The `MODEL_NAME` environment variable is essential for all OpenAI-compatible API requests. This variable corresponds to either:

1. The [Hugging Face model](https://huggingface.co/models) you've deployed (e.g., `mistralai/Mistral-7B-Instruct-v0.2`).
2. A custom name if you've set `OPENAI_SERVED_MODEL_NAME_OVERRIDE` as an environment variable.

This model name is used in chat and text completion API requests to identify which model should process your request.
```

--------------------------------

### Configure RunPod Local API Server

Source: https://docs.runpod.io/serverless/development/local-testing

Combines multiple flags to customize the local RunPod API server environment. This example sets the port to 8080, uses 4 concurrent workers, sets the log level to DEBUG, and enables the debugger.

```sh
python handler.py --rp_serve_api \
    --rp_api_port 8080 \
    --rp_api_concurrency 4 \
    --rp_log_level DEBUG \
    --rp_debugger
```

--------------------------------

### OpenAI API Compatibility - Supported Endpoints

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

Lists the core OpenAI API endpoints supported by vLLM workers on Runpod, including their descriptions and support status.

```APIDOC
## OpenAI API Compatibility - Supported Endpoints

vLLM workers support these core OpenAI API endpoints:

| Endpoint            | Description                     | Status          |
| ------------------- | ------------------------------- | --------------- |
| `/chat/completions` | Generate chat model completions | Fully supported |
| `/completions`      | Generate text completions       | Fully supported |
| `/models`           | List available models           | Supported       |
```

--------------------------------

### Build Docker Image for Runpod

Source: https://docs.runpod.io/serverless/workers/deploy

Builds a Docker image compatible with Runpod's infrastructure. Requires Docker installed and a Dockerfile. The `--platform linux/amd64` flag is crucial for compatibility, especially on ARM-based systems.

```sh
docker build --platform linux/amd64 \
    -t DOCKER_USERNAME/WORKER_NAME:VERSION .
```

--------------------------------

### RunPod User CPU Cloud Billing Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON structure provides billing details for CPU cloud resources. It includes the CPU flavor ID, timestamp, duration billed in seconds, and the amount charged. This is useful for tracking CPU-specific costs.

```json
{
  "cpuFlavorId": "xyz789",
  "time": "2007-12-03T10:15:30Z",
  "timeBilledSeconds": 123,
  "amount": 987.65
}
```

--------------------------------

### Push Docker Image to Registry

Source: https://docs.runpod.io/tutorials/serverless/generate-sdxl-turbo

Pushes the locally built Docker image to a container registry. Ensure you replace '<username>/<repo>:<tag>' with the same name and tag used during the build process. This makes the image accessible for deployment on Runpod.

```bash
docker push <username>/<repo>:<tag>
```

--------------------------------

### Deploy Mistral Model with vLLM CLI

Source: https://docs.runpod.io/serverless/vllm/configuration

This bash script demonstrates how to launch a Mistral model using the vLLM CLI. It specifies model name, tokenizer mode, config format, load format, and enables auto tool choice with a Mistral tool call parser. Ensure vLLM is installed and accessible in your environment.

```bash
vllm serve mistralai/Ministral-8B-Instruct-2410 \
  --tokenizer_mode mistral \
  --config_format mistral \
  --load_format mistral \
  --enable-auto-tool-choice \
  --tool-call-parser mistral
```

--------------------------------

### Stop Runpod Agent (Bash)

Source: https://docs.runpod.io/hosting/burn-testing

This command stops the Runpod agent service using systemctl. It is a prerequisite for running burn tests.

```bash
sudo systemctl stop runpod
```

--------------------------------

### RunPod API podStop Mutation Example

Source: https://docs.runpod.io/references/graphql-spec

This GraphQL mutation demonstrates how to stop a pod using the `podStop` function. It requires a `PodStopInput` object and returns detailed pod information upon successful execution.

```graphql
mutation podStop($input: PodStopInput!) {
  podStop(input: $input) {
    lowestBidPriceToResume
    aiApiId
    apiKey
    clusterIp
    clusterIdx
    clusterCidr
    clusterRole
    consumerUserId
    containerDiskInGb
    containerRegistryAuthId
    costMultiplier
    costPerHr
    createdAt
    adjustedCostPerHr
    desiredStatus
    dockerArgs
    dockerId
    env
    gpuCount
    gpuPowerLimitPercent
    gpus {
      ...GpuFragment
    }
    id
    imageName
    lastStatusChange
    locked
    machineId
    memoryInGb
    name
    podType
    port
    ports
    registry {
      ...PodRegistryFragment
    }
    templateId
    uptimeSeconds
    vcpuCount
    version
    volumeEncrypted
    volumeInGb
    volumeKey
    volumeMountPath
    lastStartedAt
    cpuFlavorId
    machineType
    slsVersion
    networkVolumeId
    testPod
    ideAiApiId
    hubReleaseId
    hubRelease {
      ...HubReleaseFragment
    }
    modelVersions {
      ...ModelVersionPodAssignmentFragment
    }
    cpuFlavor {
      ...CpuFlavorFragment
    }
    runtime {
      ...PodRuntimeFragment
    }
    machine {
      ...PodMachineInfoFragment
    }
    latestTelemetry {
      ...PodTelemetryFragment
    }
    endpoint {
      ...EndpointFragment
    }
    networkVolume {
      ...NetworkVolumeFragment
    }
    savingsPlans {
      ...SavingsPlanFragment
    }
    clusterId
    ipAddress {
      ...IPAddressFragment
    }
    models
  }
}
```

```json
{
  "input": PodStopInput
}
```

--------------------------------

### Export Environment Variables for Runpod

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

Exports environment variables, filtering for those starting with RUNPOD_, PATH, or '=', and saves them to /etc/rp_environment. It also sources this file in ~/.bashrc for persistence.

```bash
export_env_vars() {
    echo "Exporting environment variables..."
    printenv | grep -E '^RUNPOD_|^PATH=|^_=' | awk -F = '{ print "export " $1 "=\"" $2 "\"" }' >> /etc/rp_environment
    echo 'source /etc/rp_environment' >> ~/.bashrc
}

```

--------------------------------

### Build Docker Image Locally

Source: https://docs.runpod.io/pods/templates/create-custom-template

Builds a Docker image from the current project directory. The `--platform linux/amd64` flag is crucial for compatibility with Runpod's infrastructure, especially when building on macOS or ARM systems. This process includes downloading base images, installing dependencies, and copying application files.

```bash
docker build --platform linux/amd64 -t my-custom-template:latest .
```

--------------------------------

### Get Pod by ID

Source: https://docs.runpod.io/sdks/graphql/manage-pods

Retrieves details for a specific pod using its unique identifier. This endpoint is useful for fetching the most up-to-date status and configuration of a single pod.

```APIDOC
## POST /graphql

### Description
Retrieves details for a specific pod using its unique identifier. This endpoint is useful for fetching the most up-to-date status and configuration of a single pod.

### Method
POST

### Endpoint
https://api.runpod.io/graphql

### Parameters
#### Query Parameters
- **api_key** (string) - Required - Your RunPod API key.

#### Request Body
- **query** (string) - Required - The GraphQL query string to fetch a specific pod by its ID.
  - **input** (object) - Input for the pod query.
    - **podId** (string) - Required - The unique identifier of the pod to retrieve.

### Request Example
```json
{
  "query": "query Pod { pod(input: {podId: \"ldl1dxirsim64n\"}) { id name runtime { uptimeInSeconds ports { ip isIpPublic privatePort publicPort type } gpus { id gpuUtilPercent memoryUtilPercent } container { cpuPercent memoryPercent } } } }"
}
```

### Response
#### Success Response (200)
- **data** (object) - Contains the result of the GraphQL query.
  - **pod** (object or null) - Details of the requested pod.
    - **id** (string) - The unique identifier for the pod.
    - **name** (string) - The name of the pod.
    - **runtime** (object)
      - **uptimeInSeconds** (integer) - The duration the pod has been running in seconds.
      - **ports** (array of objects or null) - Network ports configured for the pod.
        - **ip** (string) - The IP address of the port.
        - **isIpPublic** (boolean) - Indicates if the IP address is public.
        - **privatePort** (integer) - The private port number.
        - **publicPort** (integer) - The public port number.
        - **type** (string) - The type of the port (e.g., http).
      - **gpus** (array of objects) - GPU utilization details.
        - **id** (string) - The unique identifier for the GPU.
        - **gpuUtilPercent** (integer) - The percentage of GPU utilization.
        - **memoryUtilPercent** (integer) - The percentage of GPU memory utilization.
      - **container** (object)
        - **cpuPercent** (integer) - The percentage of CPU utilization.
        - **memoryPercent** (integer) - The percentage of memory utilization.

#### Response Example
```json
{
  "data": {
    "pod": {
      "id": "ldl1dxirsim64n",
      "name": "Runpod Pytorch",
      "runtime": {
        "uptimeInSeconds": 11,
        "ports": [
          {
            "ip": "100.65.0.101",
            "isIpPublic": false,
            "privatePort": 8888,
            "publicPort": 60141,
            "type": "http"
          }
        ],
        "gpus": [
          {
            "id": "GPU-e0488b7e-6932-795b-a125-4472c16ea72c",
            "gpuUtilPercent": 0,
            "memoryUtilPercent": 0
          }
        ],
        "container": {
          "cpuPercent": 0,
          "memoryPercent": 0
        }
      }
    }
  }
}
```
```

--------------------------------

### WAN 2.2 I2V 720p

Source: https://docs.runpod.io/hub/public-endpoint-reference

Open-source AI image-to-video generation using a diffusion transformer architecture at 720p.

```APIDOC
## POST /v2/wan-2-2-i2v-720/

### Description
Generates video from an input image using a diffusion transformer architecture at 720p resolution.

### Method
POST

### Endpoint
`https://api.runpod.ai/v2/wan-2-2-i2v-720/`

### Parameters
#### Query Parameters
- **input_image** (string) - Required - A URL to the input image.
- **prompt** (string) - Optional - A text prompt to guide the video generation.
- **duration** (integer) - Optional - The desired duration of the video in seconds.

### Request Example
```json
{
  "input_image": "https://example.com/images/input.jpg",
  "prompt": "The image animates with a cinematic feel",
  "duration": 5
}
```

### Response
#### Success Response (200)
- **video_url** (string) - The URL to the generated 720p video.

#### Response Example
```json
{
  "video_url": "https://storage.runpod.ai/videos/wan2.2_i2v_output.mp4"
}
```
```

--------------------------------

### Purge Queue

Source: https://docs.runpod.io/sdks/python/endpoints

This section describes how to purge all jobs from a queue using the `purge_queue()` function. It includes details on the `timeout` parameter and provides a Python code example.

```APIDOC
## Purge Queue

You can purge all jobs from a queue by using the `purge_queue()` function. You can provide the `timeout` parameter to specify how long to wait for the server to respond before purging the queue.

`purge_queue()` doesn't affect Jobs in progress.

### Method
POST

### Endpoint
`/endpoints/{endpoint_id}/queue/purge`

### Parameters
#### Query Parameters
- **timeout** (integer) - Optional - The time in seconds to wait for the server to respond before purging the queue.

### Request Example
```python
import runpod
import os

runpod.api_key = os.getenv("RUNPOD_API_KEY")

endpoint = runpod.Endpoint("YOUR_ENDPOINT_ID")

endpoint.purge_queue(timeout=3)
```

### Response
#### Success Response (200)
- **message** (string) - A confirmation message indicating the queue has been purged.

#### Response Example
```json
{
  "message": "Queue purged successfully."
}
```
```

--------------------------------

### Navigate to Repository Directory

Source: https://docs.runpod.io/tutorials/serverless/generate-sdxl-turbo

Changes the current directory to the root of the cloned 'worker-sdxl-turbo' repository. This is necessary to execute subsequent build commands within the correct context.

```bash
cd worker-sdxl-turbo
```

--------------------------------

### Initialize OpenAI Client for RunPod vLLM

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

This snippet shows how to initialize the OpenAI client to communicate with a RunPod vLLM endpoint. It requires your RunPod API key and the specific endpoint ID. Ensure you replace placeholder values with your actual credentials and model name.

```javascript
import { OpenAI } from "openai";

// Replace ENDPOINT_ID and RUNPOD_API_KEY with your actual values
const openai = new OpenAI({
  apiKey: "RUNPOD_API_KEY", 
  baseURL: "https://api.runpod.ai/v2/ENDPOINT_ID/openai/v1"
});

// Replace MODEL_NAME with your actual model name
const response = await openai.chat.completions.create({
  model: "MODEL_NAME",
  messages: [
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "Hello!" }
  ]
});
```

--------------------------------

### POST /chat/completions

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

This endpoint allows you to generate chat completions using a specified model. You can control various aspects of the generation process, such as temperature, top_p, and stop sequences.

```APIDOC
## POST /chat/completions

### Description
This endpoint generates chat completions based on a list of messages. It supports various parameters to control the generation process, including model selection, sampling methods, and output formatting.

### Method
POST

### Endpoint
/chat/completions

### Parameters
#### Request Body
- **messages** (list[dict[str, str]]) - Required - List of messages with `role` and `content` keys. The model's chat template will be applied automatically.
- **model** (string) - Required - The model repo that you've deployed on your Runpod Serverless endpoint.
- **temperature** (float) - Optional - Controls the randomness of sampling. Lower values make it more deterministic, higher values make it more random. Zero means greedy sampling. (Default: 0.7)
- **top_p** (float) - Optional - Controls the cumulative probability of top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens. (Default: 1.0)
- **n** (int) - Optional - Number of output sequences to return for the given prompt. (Default: 1)
- **max_tokens** (int) - Optional - Maximum number of tokens to generate per output sequence.
- **seed** (int) - Optional - Random seed to use for the generation.
- **stop** (string or list[str]) - Optional - String(s) that stop generation when produced. The returned output will not contain the stop strings. (Default: [])
- **stream** (bool) - Optional - Whether to stream the response. (Default: false)
- **presence_penalty** (float) - Optional - Penalizes new tokens based on whether they appear in the generated text so far. Values > 0 encourage new tokens, values < 0 encourage repetition. (Default: 0.0)
- **frequency_penalty** (float) - Optional - Penalizes new tokens based on their frequency in the generated text so far. Values > 0 encourage new tokens, values < 0 encourage repetition. (Default: 0.0)
- **logit_bias** (dict[str, float]) - Optional - Unsupported by vLLM.
- **user** (string) - Optional - Unsupported by vLLM.

### Request Example
```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Who won the world series in 2020?"}
  ],
  "model": "your-model-repo",
  "temperature": 0.7,
  "max_tokens": 100
}
```

### Response
#### Success Response (200)
- **choices** (list[dict]) - A list of generated completion choices.
  - **message** (dict) - The message content.
    - **role** (string) - The role of the author of the message.
    - **content** (string) - The content of the message.
- **created** (int) - Unix timestamp (in seconds) of when the response was generated.
- **id** (string) - A unique identifier for the response.
- **model** (string) - The model used for the completion.
- **object** (string) - The type of object returned, usually "chat.completion".
- **usage** (dict) - Usage statistics for the request.
  - **completion_tokens** (int) - Number of tokens in the generated completion.
  - **prompt_tokens** (int) - Number of tokens in the prompt.
  - **total_tokens** (int) - Total number of tokens used.

#### Response Example
```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "The Los Angeles Dodgers won the World Series in 2020."
      },
      "finish_reason": "stop",
      "index": 0
    }
  ],
  "created": 1677652288,
  "id": "chatcmpl-123",
  "model": "your-model-repo",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 10,
    "prompt_tokens": 20,
    "total_tokens": 30
  }
}
```
```

--------------------------------

### GET /templates/{templateId}

Source: https://docs.runpod.io/api-reference/templates/GET/templates/templateId

Retrieves a specific template by its ID. This endpoint allows filtering to include templates bound to serverless endpoints, community-made public templates, or official Runpod templates.

```APIDOC
## GET /templates/{templateId}

### Description
Retrieves a specific template by its ID. This endpoint allows filtering to include templates bound to serverless endpoints, community-made public templates, or official Runpod templates.

### Method
GET

### Endpoint
/templates/{templateId}

### Parameters
#### Path Parameters
- **templateId** (string) - Required - ID of template to return.

#### Query Parameters
- **includeEndpointBoundTemplates** (boolean) - Optional - Include templates bound to Serverless endpoints in the response. (default: false)
- **includePublicTemplates** (boolean) - Optional - Include community-made public templates in the response. (default: false)
- **includeRunpodTemplates** (boolean) - Optional - Include official Runpod templates in the response. (default: false)

### Request Example
```json
{
  "example": "GET /v1/templates/30zmvf89kd?includePublicTemplates=true"
}
```

### Response
#### Success Response (200)
- **id** (string) - A unique string identifying a template.
- **imageName** (string) - The image tag for the container run on Pods or workers created from a template.
- **name** (string) - The name of the template.
- **category** (string) - The category of the template. The category can be used to filter templates in the Runpod UI. Current categories are NVIDIA, AMD, and CPU.
- **containerDiskInGb** (integer) - The amount of disk space, in gigabytes (GB), to allocate on the container disk for a Pod or worker.
- **dockerEntrypoint** (array) - If specified, overrides the ENTRYPOINT for the Docker image run on a Pod or worker. If [], uses the ENTRYPOINT defined in the image.
- **dockerStartCmd** (array) - If specified, overrides the start CMD for the Docker image run on a Pod or worker. If [], uses the start CMD defined in the image.
- **env** (object) - Environment variables to set for the container.
- **isPublic** (boolean) - Set to true if a template is public and can be used by any Runpod user. Set to false if a template is private and can only be used by the creator.
- **isRunpod** (boolean) - If true, a template is an official template managed by Runpod.
- **isServerless** (boolean) - If true, a template is configured for use with Runpod Serverless.
- **ports** (array) - An array of port mappings for the container.
- **repoName** (string) - The name of the repository for the template.
- **templateType** (string) - The type of the template (e.g., 'image', 'script').
- **userId** (string) - The ID of the user who created the template.
- **volumeInGb** (integer) - The amount of persistent storage, in gigabytes (GB), to allocate for the template.
- **templateVersion** (string) - The version of the template.
- **gpuType** (string) - The type of GPU required for the template.
- **networkVolumeId** (string) - The ID of the network volume to attach to the template.
- **containerRegistryAuthId** (string) - The ID of the container registry authentication to use.
- **earned** (number) - The amount of Runpod credits earned by the creator of a template by all Pods or workers created from the template.

#### Response Example
```json
{
  "example": {
    "id": "30zmvf89kd",
    "imageName": "runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04",
    "name": "PyTorch 2.1 CUDA 11.8",
    "category": "NVIDIA",
    "containerDiskInGb": 50,
    "dockerEntrypoint": [],
    "dockerStartCmd": [],
    "env": {},
    "isPublic": false,
    "isRunpod": true,
    "isServerless": false,
    "ports": [
      {
        "containerPort": 8888,
        "name": "default",
        "transport": "http"
      }
    ],
    "repoName": "runpod",
    "templateType": "image",
    "userId": "64d1a1a1a1a1a1a1a1a1a1a1",
    "volumeInGb": 100,
    "templateVersion": "1.0.0",
    "gpuType": "NVIDIA",
    "networkVolumeId": null,
    "containerRegistryAuthId": null,
    "earned": 100
  }
}
```

#### Error Response (400)
- **description**: Invalid ID supplied.

#### Error Response (404)
- **description**: Template not found.
```

--------------------------------

### Example PodResumeInput Structure

Source: https://docs.runpod.io/references/graphql-spec

This JSON object defines the input for resuming a pod. It requires the pod ID and optionally allows specifying the GPU count, whether to sync the machine, and the compute type.

```json
{
  "podId": "xyz789",
  "gpuCount": 987,
  "syncMachine": false,
  "computeType": "CPU"
}
```

--------------------------------

### RunPod API Job Submission Response

Source: https://docs.runpod.io/tutorials/serverless/comfyui

This is an example JSON response from the RunPod API after submitting a job. It contains a unique job ID and the initial status of the job, which is typically 'IN_QUEUE' upon submission.

```json
{
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "status": "IN_QUEUE"
}
```

--------------------------------

### Dockerfile for Custom Pod Template (Dockerfile)

Source: https://docs.runpod.io/pods/templates/create-custom-template

This Dockerfile extends a Runpod PyTorch base image, sets environment variables for unbuffered Python output, installs system and Python dependencies, and copies application files to the working directory.

```dockerfile
# Use Runpod PyTorch base image
FROM runpod/pytorch:1.0.2-cu1281-torch280-ubuntu2404

# Set environment variables
# This ensures Python output is immediately visible in logs
ENV PYTHONUNBUFFERED=1

# Set the working directory
WORKDIR /app

# Install system dependencies if needed
RUN apt-get update --yes && \
    DEBIAN_FRONTEND=noninteractive apt-get install --yes --no-install-recommends \
        wget \
        curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements file
COPY requirements.txt /app/

# Install Python dependencies
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Copy application files
COPY . /app
```

--------------------------------

### Check Job Status for Asynchronous Request (cURL)

Source: https://docs.runpod.io/hub/public-endpoints

Example of how to check the status of an asynchronous job using the `/status` endpoint in cURL. This requires the job ID obtained from the initial asynchronous request.

```bash
curl -X GET "https://api.runpod.ai/v2/black-forest-labs-flux-1-dev/status/{job-id}" \
  -H "Authorization: Bearer RUNPOD_API_KEY"
```

--------------------------------

### Repository Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents the structure of a repository as defined in the RunPod.io API. It includes fields for identification, listing status, repository details, and performance metrics.

```json
{
  "id": 4,
  "listed": false,
  "repoId": "abc123",
  "title": "xyz789",
  "description": "abc123",
  "repoName": "abc123",
  "repoOwner": "abc123",
  "repoOwnerAvatarUrl": "abc123",
  "createdAt": "2007-12-03T10:15:30Z",
  "updatedAt": "2007-12-03T10:15:30Z",
  "lastScannedAt": "2007-12-03T10:15:30Z",
  "views": 987,
  "stars": 123,
  "deploys": 123,
  "openIssues": 987,
  "watchers": 987,
  "language": "abc123",
  "tags": ["xyz789"],
  "category": "xyz789",
  "type": "abc123",
  "rejectNote": "abc123",
  "releases": ["HubRelease"],
  "listedRelease": "HubRelease"
}
```

--------------------------------

### AdminUserReferrals Type Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object demonstrates the structure of the `AdminUserReferrals` type. It includes fields related to user referrals, such as IDs, emails, sign-up methods, referral codes, and earnings.

```json
{
  "id": "abc123",
  "email": "xyz789",
  "signUpMethod": "GOOGLE",
  "createdAt": "2007-12-03T10:15:30Z",
  "signedTermsOfService": false,
  "referralEmailUsage": ReferralEmailUsage,
  "referralCode": "xyz789",
  "affiliateKey": "xyz789",
  "clientLifetimeSpend": 123.45,
  "activatedAt": "2007-12-03T10:15:30Z",
  "earnings": AffiliateProgramEarnings,
  "referredUsers": [AdminUserReferrals],
  "referralBonus": 987.65,
  "referredById": "xyz789",
  "referredByEmail": "xyz789"
}
```

--------------------------------

### SharedApiKey Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents a SharedApiKey, including its ID, associated secret ID, creation timestamp, name, and description.

```json
{
  "id": "xyz789",
  "secretId": "xyz789",
  "createdAt": "2007-12-03T10:15:30Z",
  "name": "abc123",
  "description": "abc123"
}
```

--------------------------------

### JSON Output Example from Serverless TTS Simulator

Source: https://docs.runpod.io/tutorials/sdks/python/101/generator

This JSON output represents the expected result when running the serverless TTS simulator with the provided test input. It shows incremental status updates for each processed audio chunk and a final completion message.

```json
{"status": "processing", "chunk": "Audio chunk 1: This is a test"}
{"status": "processing", "chunk": "Audio chunk 2: of the Runpod"}
{"status": "processing", "chunk": "Audio chunk 3: text to speech"}
{"status": "processing", "chunk": "Audio chunk 4: simulator It processes"}
{"status": "processing", "chunk": "Audio chunk 5: text in chunks"}
{"status": "processing", "chunk": "Audio chunk 6: and simulates audio"}
{"status": "processing", "chunk": "Audio chunk 7: generation"}
{"status": "completed", "message": "Text-to-speech conversion completed"}

```

--------------------------------

### Example API Response Structure

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents a typical API response, often used for pagination. It includes an end cursor for fetching subsequent pages and a boolean indicating if more pages are available.

```json
{
  "endCursor": "2007-12-03T10:15:30Z",
  "hasNextPage": true
}
```

--------------------------------

### Build and Push Docker Image (Bash)

Source: https://docs.runpod.io/tutorials/pods/build-docker-images

Executes the Bazel command to build the Docker image defined in the `BUILD.bazel` file and push it to your configured Docker Hub repository. This command initiates the image creation and upload process.

```bash
bazel run //:push_custom_image
```

--------------------------------

### Clone Axolotl Repository

Source: https://docs.runpod.io/instant-clusters/axolotl

Clones the Axolotl repository into the Pod's main directory. This is a prerequisite for setting up Axolotl on each pod within the cluster.

```shell
git clone https://github.com/axolotl-ai-cloud/axolotl
```

--------------------------------

### Wan 2.2 T2V 720p Input Configuration (JSON)

Source: https://docs.runpod.io/hub/public-endpoint-reference

This JSON object defines the input parameters for generating a 720p video using the Wan 2.2 T2V model. It includes a text prompt, inference steps, guidance scale, negative prompt, video size, duration, and other optional settings for controlling the video generation process.

```json
{
  "input": {
    "prompt": "A serene morning in an ancient forest, golden sunlight filtering through tall pine trees, creating dancing light patterns on the moss-covered ground",
    "num_inference_steps": 30,
    "guidance": 5,
    "negative_prompt": "",
    "size": "1280*720",
    "duration": 5,
    "flow_shift": 5,
    "seed": -1,
    "enable_prompt_optimization": false,
    "enable_safety_checker": true
  }
}
```

--------------------------------

### Define Python Serverless Function Logic

Source: https://docs.runpod.io/tutorials/sdks/python/101/hello

Defines the core logic for a serverless function. This example function checks if a provided integer is even. It includes input validation to ensure the input is an integer.

```python
def is_even(job):
    job_input = job["input"]
    the_number = job_input["number"]

    if not isinstance(the_number, int):
        return {"error": "Please provide an integer."}

    return the_number % 2 == 0
```

--------------------------------

### Migrate OpenAI API Calls to Runpod Serverless Endpoints (JavaScript)

Source: https://docs.runpod.io/tutorials/migrations/openai/overview

This JavaScript snippet shows how to initialize the OpenAI client for use with Runpod Serverless Endpoints. It involves setting the `baseURL` to the Runpod host and the `apiKey` to your Runpod API key. The example includes a call to the chat completions endpoint.

```javascript
import OpenAI from 'openai'

const openai = new OpenAI({
  baseURL: process.env.RUNPOD_HOST,
  apiKey: process.env.RUNPOD_API_KEY,
})

const chatCompletion = await openai.chat.completions.create({
   model: "openchat/openchat-3.5-0106",
   messages: [{'role': 'user', 'content': 'Why is Runpod the best platform?'}],

});

```

--------------------------------

### Asynchronous API Request to Runpod Public Endpoint (cURL)

Source: https://docs.runpod.io/hub/public-endpoints

Example of making an asynchronous API request to a Runpod Public Endpoint using cURL. This method is useful for longer-running tasks, returning a job ID that can be used to check the status later.

```bash
curl -X POST "https://api.runpod.ai/v2/black-forest-labs-flux-1-dev/run" \
  -H "Authorization: Bearer RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "prompt": "A futuristic cityscape with flying cars",
      "width": 1024,
      "height": 1024,
      "num_inference_steps": 50,
      "guidance": 8.0
    }
  }'
```

--------------------------------

### RunPod User GPU Cloud Billing Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON structure details billing information for GPU cloud resources. It includes the GPU type ID, timestamp, duration billed in seconds, and the amount charged. This is essential for monitoring GPU expenditure.

```json
{
  "gpuTypeId": "xyz789",
  "time": "2007-12-03T10:15:30Z",
  "timeBilledSeconds": 123,
  "amount": 987.65
}
```

--------------------------------

### Create Serverless Endpoint via REST API

Source: https://docs.runpod.io/serverless/endpoints/overview

This snippet demonstrates how to create a Serverless endpoint using a POST request to the Runpod REST API. It requires an API key for authentication and specifies various configuration parameters such as GPU type, compute resources, scaling settings, and name. Ensure you replace 'RUNPOD_API_KEY' with your actual API key.

```bash
curl --request POST \
  --url https://rest.runpod.io/v1/endpoints \
  --header 'Authorization: Bearer RUNPOD_API_KEY' \
  --header 'Content-Type: application/json' \
  --data '{ \
  "allowedCudaVersions": [ \
    "12.8" \
  ], \
  "computeType": "GPU", \
  "cpuFlavorIds": [ \
    "cpu3c" \
  ], \
  "dataCenterIds": [ \
    "EU-RO-1", \
    "CA-MTL-1" \
  ], \
  "executionTimeoutMs": 600000, \
  "flashboot": true, \
  "gpuCount": 1, \
  "gpuTypeIds": [ \
    "NVIDIA GeForce RTX 4090" \
  ], \
  "idleTimeout": 5, \
  "name": "my-endpoint", \
  "scalerType": "QUEUE_DELAY", \
  "scalerValue": 4, \
  "templateId": "30zmvf89kd", \
  "vcpuCount": 2, \
  "workersMax": 3, \
  "workersMin": 0 \
}'
```

--------------------------------

### Queue-Based Serverless Worker Implementation (Python)

Source: https://docs.runpod.io/serverless/load-balancing/overview

This Python code demonstrates the structure of a traditional queue-based Serverless worker for RunPod. It defines a handler function that processes job inputs and returns a result. The worker is started using `runpod.serverless.start()`. This approach is suitable for asynchronous tasks and guarantees request processing through RunPod's queueing system.

```python
import runpod

def handler(job):
    """Handler function that will be used to process jobs."""
    job_input = job["input"]
    prompt = job_input.get("prompt", "Hello world")

    # Process the request
    result = f"Generated text for: {prompt}"

    return {"generated_text": result}

runpod.serverless.start({"handler": handler})
```

--------------------------------

### TeamInvite Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object illustrates the structure of a TeamInvite, detailing its ID, associated team, role, and timestamps for creation and expiration. It's used for managing invitations to join teams.

```json
{
  "id": 4,
  "team": "TeamInviteTeam",
  "role": "owner",
  "createdAt": "2007-12-03T10:15:30Z",
  "expiresAt": "2007-12-03T10:15:30Z"
}
```

--------------------------------

### TeamOwner Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents a team owner, containing their ID, email, and authentication ID. It's a core identifier for team ownership.

```json
{
  "id": "abc123",
  "email": "xyz789",
  "authId": "abc123"
}
```

--------------------------------

### SSH Connection Command Structure

Source: https://docs.runpod.io/pods/configuration/use-ssh

This outlines the general structure of the SSH command for connecting to a Runpod Pod. It details the components: username, Pod IP address, SSH port, and the path to the private SSH key.

```sh
ssh root@[POD_IP_ADDRESS] -p [SSH_PORT] -i [PATH_TO_SSH_KEY]
```

--------------------------------

### TeamInviteOwner Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents the owner of a team invitation, containing only their email address. It's a minimal representation used within the invitation context.

```json
{
  "email": "abc123"
}
```

--------------------------------

### Generate Focused Text with Adjusted Parameters

Source: https://docs.runpod.io/tutorials/pods/run-your-first

This example shows how to modify generation parameters to achieve more focused and deterministic output. It uses the same chat messages but adjusts `temperature` to 0.3 and `top_p` to 0.9, while keeping `max_new_tokens` at 150.

```python
# More focused and deterministic output
outputs = pipe(messages, max_new_tokens=150, do_sample=True, temperature=0.3, top_p=0.9)
print(outputs[0]["generated_text"][-1]['content'])
```

--------------------------------

### Get Runpod Endpoints

Source: https://docs.runpod.io/sdks/python/apis

Retrieves a list of all available endpoint configurations within Runpod. This function requires the 'runpod' library and an API key set as an environment variable 'RUNPOD_API_KEY'. It returns a list of endpoint configurations.

```python
import runpod
import os

runpod.api_key = os.getenv("RUNPOD_API_KEY")

# Fetching all available endpoints
endpoints = runpod.get_endpoints()

# Displaying the list of endpoints
print(endpoints)
```

--------------------------------

### Configure RunPod CLI API Key

Source: https://docs.runpod.io/pods/manage-pods

Sets the API key for the Runpod CLI, which is required for managing Pods and other resources. Ensure you have installed the Runpod CLI and have your API key ready.

```sh
runpodctl config --apiKey RUNPOD_API_API_KEY
```

--------------------------------

### Generate Text with Ollama via HTTP API (cURL)

Source: https://docs.runpod.io/tutorials/pods/run-ollama

This example shows how to make an HTTP POST request to the Ollama API's `/api/generate` endpoint using cURL to generate text. You need to specify the desired model and provide a prompt. Replace `{POD_ID}` with your Runpod Pod ID.

```bash
curl -X POST https://{POD_ID}-11434.proxy.runpod.net/api/generate -d '{
  "model": "mistral",
  "prompt":"Here is a story about llamas eating grass"
 }'
```

--------------------------------

### Pod Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents a Pod within the Runpod.io system. It includes details about its configuration, resources, pricing, and status. It serves as a comprehensive representation of a single compute instance.

```json
{
  "pods": [Pod],
  "id": "xyz789",
  "userId": "xyz789",
  "name": "abc123",
  "registered": false,
  "podCloudEnabled": true,
  "clusterEnabled": false,
  "slsEnabled": false,
  "priority": 123,
  "minPodGpuCount": 123,
  "gpuTypeId": "xyz789",
  "gpuType": GpuType,
  "gpuPowerLimitPercentageRented": 123,
  "gpuPowerLimitPercentageSelf": 123,
  "cpuCount": 123,
  "cpuTypeId": "xyz789",
  "cpuType": CpuType,
  "dataCenter": DataCenter,
  "moboName": "xyz789",
  "hidden": false,
  "hostPricePerGpu": 987.65,
  "hostMinBidPerGpu": 987.65,
  "defaultImageName": "xyz789",
  "defaultStartScript": "xyz789",
  "defaultArguments": "abc123",
  "defaultDiskInGb": 987,
  "defaultPort": 43612,
  "defaultEnv": ["abc123"],
  "location": "xyz789",
  "listed": true,
  "verified": false,
  "machineId": "abc123",
  "diskReserved": 123,
  "diskTotal": 987,
  "diskMBps": 123,
  "downloadMbps": 123,
  "gpuReserved": 987,
  "totalGpuAllocated": 987,
  "gpuTotal": 123,
  "pcieLink": 123,
  "pcieLinkWidth": 987,
  "memoryReserved": 123,
  "memoryTotal": 987,
  "uploadMbps": 123,
  "vcpuReserved": 123,
  "vcpuTotal": 987,
  "installCert": "xyz789",
  "uptimePercentListedOneWeek": 987.65,
  "uptimePercentListedFourWeek": 987.65,
  "uptimePercentListedTwelveWeek": 123.45,
  "uptimePercentAbsoluteTwelveWeek": 123.45,
  "margin": 987.65,
  "gpuCloudPrice": 123.45,
  "supportPublicIp": true,
  "secureCloud": false,
  "ownedByMe": false,
  "idleJobTemplateId": "xyz789",
  "idleJobTemplate": PodTemplate,
  "maintenanceStart": "2007-12-03T10:15:30Z",
  "maintenanceEnd": "2007-12-03T10:15:30Z",
  "maintenanceNote": "abc123",
  "maintenanceMode": true,
  "note": "abc123",
  "machineBalance": MachineBalance,
  "machineSystem": MachineSystem,
  "dataCenterId": "xyz789",
  "machineType": "xyz789",
  "lastBenchmark": MachineBenchmark,
  "upcomingMaintenances": [MachineMaintenance],
  "nextMaintenance": MachineMaintenance,
  "nodeGroupId": "xyz789",
  "lastSyncAt": "2007-12-03T10:15:30Z",
  "markedForDeletion": true,
  "pinCpus": true,
  "modelRepos": [ModelRepo],
  "poolState": "GENERALLY_AVAILABLE",
  "poolId": "xyz789",
  "backgroundPodTelemetry": PodTelemetry,
  "latestTelemetry": MachineTelemetry,
  "uptime": MachineUptime,
  "runpodIp": "xyz789",
  "publicIp": "abc123"
}
```

--------------------------------

### TeamInviteTeam Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object describes the team associated with an invitation, including its ID, name, and owner's details. It provides essential information about the team being invited to.

```json
{
  "id": 4,
  "name": "abc123",
  "owner": "TeamInviteOwner"
}
```

--------------------------------

### GET /pods/{podId}

Source: https://docs.runpod.io/api-reference/pods/GET/pods/podId

Retrieves details of a specific Pod by its ID. It supports various query parameters to include additional information such as machine details, network volumes, savings plans, templates, and workers.

```APIDOC
## GET /pods/{podId}

### Description
Returns a single Pod by its ID. You can include additional information about the machine, network volume, savings plans, template, and workers associated with the Pod.

### Method
GET

### Endpoint
/pods/{podId}

### Parameters
#### Path Parameters
- **podId** (string) - Required - ID of Pod to return.

#### Query Parameters
- **includeMachine** (boolean) - Optional - Include information about the machine the Pod is running on. Defaults to false.
- **includeNetworkVolume** (boolean) - Optional - Include information about the network volume attached to the returned Pod, if any. Defaults to false.
- **includeSavingsPlans** (boolean) - Optional - Include information about the savings plans applied to the Pod. Defaults to false.
- **includeTemplate** (boolean) - Optional - Include information about the template the Pod uses, if any. Defaults to false.
- **includeWorkers** (boolean) - Optional - Set to true to also list Pods which are Serverless workers. Defaults to false.

### Response
#### Success Response (200)
- **adjustedCostPerHr** (number) - The effective cost in Runpod credits per hour of running a Pod, adjusted by active Savings Plans.
- **aiApiId** (string) - Synonym for endpointId (legacy name).
- **consumerUserId** (string) - A unique string identifying the Runpod user who rents a Pod.
- **containerDiskInGb** (integer) - The amount of disk space, in gigabytes (GB), to allocate on the container disk for a Pod.
- **containerRegistryAuthId** (string) - If a Pod is created with a container registry auth, the unique string identifying that container registry auth.
- **costPerHr** (number) - The cost in Runpod credits per hour of running a Pod.
- **cpuFlavorId** (string) - If the Pod is a CPU Pod, the unique string identifying the CPU flavor the Pod is running on.
- **desiredStatus** (string) - The current expected status of a Pod. Enum: RUNNING, EXITED, TERMINATED.
- **dockerEntrypoint** (array of strings) - If specified, overrides the ENTRYPOINT for the Docker image run on the Pod.

#### Response Example
```json
{
  "adjustedCostPerHr": 0.69,
  "aiApiId": null,
  "consumerUserId": "user_2PyTJrLzeuwfZilRZ7JhCQDuSqo",
  "containerDiskInGb": 50,
  "containerRegistryAuthId": "clzdaifot0001l90809257ynb",
  "costPerHr": "0.74",
  "cpuFlavorId": "cpu3c",
  "desiredStatus": "RUNNING",
  "dockerEntrypoint": [
    "/bin/bash",
    "-c",
    "while true; do echo hello; sleep 10;done"
  ]
}
```

#### Error Response (400)
Invalid ID supplied.

#### Error Response (404)
Pod not found.
```

--------------------------------

### Example PodRuntime Data Structure

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents the runtime information for a pod, including details about its container, GPU usage, exposed ports, and uptime in seconds. It provides insights into the pod's operational status.

```json
{
  "container": PodRuntimeContainer,
  "gpus": [PodRuntimeGpus],
  "ports": [PodRuntimePorts],
  "uptimeInSeconds": 123
}
```

--------------------------------

### POST /cancel

Source: https://docs.runpod.io/serverless/endpoints/operations

Stops in-progress jobs, removes queued jobs before they start, and returns immediately with the canceled status. Replace YOUR_JOB_ID with the actual job ID.

```APIDOC
## POST /cancel

### Description
Stop jobs that are no longer needed or taking too long to complete. This operation stops in-progress jobs, removes queued jobs before they start, and returns immediately with the canceled status.

### Method
POST

### Endpoint
`https://api.runpod.ai/v2/$ENDPOINT_ID/cancel/YOUR_JOB_ID`

### Parameters
#### Path Parameters
- **ENDPOINT_ID** (string) - Required - The ID of the endpoint.
- **YOUR_JOB_ID** (string) - Required - The ID of the job to cancel.

#### Query Parameters
None

#### Request Body
None

### Request Example
```bash
curl --request POST \
  --url https://api.runpod.ai/v2/$ENDPOINT_ID/cancel/YOUR_JOB_ID \
  -H "authorization: $RUNPOD_API_KEY"
```

### Response
#### Success Response (200)
- **id** (string) - The ID of the job.
- **status** (string) - The status of the cancel operation (e.g., "CANCELLED").

#### Response Example
```json
{
  "id": "724907fe-7bcc-4e42-998d-52cb93e1421f-u1",
  "status": "CANCELLED"
}
```
```

--------------------------------

### Transfer Directory Itself with rsync

Source: https://docs.runpod.io/pods/storage/transfer-files

This rsync command transfers a directory and its contents to a specified location on a RunPod instance. Unlike the previous example, the absence of a trailing slash on the source directory means the directory itself will be created at the destination. This is useful for moving entire project folders.

```bash
rsync -avz -e "ssh -p 43201" ~/documents/example_dir root@194.26.196.6:/root/
```

--------------------------------

### SkyPilot Configuration for Runpod

Source: https://docs.runpod.io/integrations/skypilot

A YAML configuration file for SkyPilot to define a job running on Runpod. It specifies the cloud provider, working directory, setup commands, and run commands.

```yaml
resources:
  cloud: runpod

# Working directory (optional) containing the project codebase.
# Its contents are synced to ~/sky_workdir/ on the cluster.
workdir: .

# Setup commands (optional).
# Typical use: pip install -r requirements.txt
# Invoked under the workdir (i.e., can use its files).
setup: |
  echo "Running setup."

# Run commands.
# Typical use: make use of resources, such as running training.
# Invoked under the workdir (i.e., can use its files).
run: |
  echo "Hello, SkyPilot!"
  conda env list

```

--------------------------------

### Example PodRentInterruptableInput Structure

Source: https://docs.runpod.io/references/graphql-spec

This JSON object outlines the input parameters for renting an interruptible pod. It includes configurations for GPU bid, cloud type, disk size, container registry, country code, environment variables, GPU count and type, image name, resource minimums, port settings, and termination conditions.

```json
{
  "bidPerGpu": 123.45,
  "cloudType": "SECURE",
  "containerDiskInGb": 987,
  "containerRegistryAuthId": "abc123",
  "countryCode": "xyz789",
  "dockerArgs": "abc123",
  "env": [EnvironmentVariableInput],
  "gpuCount": 987,
  "gpuTypeId": "abc123",
  "imageName": "xyz789",
  "minDisk": 987,
  "minDownload": 987,
  "minMemoryInGb": 123,
  "minUpload": 123,
  "minVcpuCount": 987,
  "name": "abc123",
  "networkVolumeId": "xyz789",
  "port": 43612,
  "ports": "xyz789",
  "startJupyter": false,
  "startSsh": false,
  "stopAfter": "abc123",
  "supportPublicIp": false,
  "templateId": "xyz789",
  "terminateAfter": "xyz789",
  "volumeInGb": 987,
  "volumeKey": "xyz789",
  "volumeMountPath": "xyz789",
  "dataCenterId": "abc123",
  "cudaVersion": "xyz789",
  "allowedCudaVersions": ["xyz789"],
  "minCudaVersion": "xyz789"
}
```

--------------------------------

### GET /status/{jobId}

Source: https://docs.runpod.io/serverless/endpoints/operations

Checks the current state, execution statistics, and results of a previously submitted job. Allows configuring time-to-live (TTL) for job results.

```APIDOC
## GET /status/{jobId}

### Description
Checks the current state, execution statistics, and results of a previously submitted job. Allows configuring time-to-live (TTL) for job results.

### Method
GET

### Endpoint
`https://api.runpod.ai/v2/$ENDPOINT_ID/status/YOUR_JOB_ID`

### Parameters
#### Path Parameters
- **jobId** (string) - Required - The ID of the job to check.

#### Query Parameters
- **ttl** (integer) - Optional - Time-to-live for the job results in seconds.

#### Request Body
None

### Request Example
```bash
curl --request GET \
     --url https://api.runpod.ai/v2/$ENDPOINT_ID/status/YOUR_JOB_ID \
     -H "authorization: $RUNPOD_API_KEY"
```

### Response
#### Success Response (200)
- **status** (string) - The current status of the job (e.g., "IN_QUEUE", "PROCESSING", "COMPLETED", "FAILED").
- **execution_stats** (object) - Statistics about job execution.
- **result** (any) - The output of the job if completed.

#### Response Example
```json
{
  "status": "COMPLETED",
  "execution_stats": {
    "queue_time": 10,
    "total_time": 120
  },
  "result": {
    "output": "Job completed successfully."
  }
}
```
```

--------------------------------

### Wan 2.1 T2V Input Parameters (JSON)

Source: https://docs.runpod.io/hub/public-endpoint-reference

This JSON object defines the input parameters for the Wan 2.1 T2V model, used for generating 720p videos from text prompts. It includes essential fields like 'prompt', 'negative_prompt', 'size', and various control parameters for inference steps, guidance, duration, and more. The 'seed' parameter allows for reproducible results.

```json
{
  "input": {
    "prompt": "Steady rain falls on a bustling Tokyo street at night, neon signs casting vibrant pink and blue light that reflects and ripples across the wet black pavement",
    "num_inference_steps": 30,
    "guidance": 5,
    "negative_prompt": "",
    "size": "1280*720",
    "duration": 5,
    "flow_shift": 5,
    "seed": -1,
    "enable_prompt_optimization": false,
    "enable_safety_checker": true
  }
}
```

--------------------------------

### GET /billing/endpoints

Source: https://docs.runpod.io/api-reference/billing/GET/billing/endpoints

Retrieve billing information about your Serverless endpoints. This endpoint allows you to fetch billing history aggregated over specified time buckets and filtered by data center, endpoint ID, GPU type, and time range.

```APIDOC
## GET /billing/endpoints

### Description
Retrieve billing information about your Serverless endpoints. This endpoint allows you to fetch billing history aggregated over specified time buckets and filtered by data center, endpoint ID, GPU type, and time range.

### Method
GET

### Endpoint
/billing/endpoints

### Parameters
#### Query Parameters
- **bucketSize** (string) - Optional - The length of each billing time bucket. Allowed values: hour, day, week, month, year. Defaults to 'day'.
- **dataCenterId** (array) - Optional - Filter to endpoints located in any of the provided Runpod data centers. Defaults to all available data centers.
- **endpointId** (string) - Optional - Filter to a specific endpoint.
- **endTime** (string) - Optional - The end date of the billing period to retrieve. Format: date-time (e.g., '2023-01-31T23:59:59Z').
- **gpuTypeId** (array) - Optional - Filter by specific GPU types.

### Request Example
```json
{
  "example": "No request body for GET request"
}
```

### Response
#### Success Response (200)
- **billingData** (array) - Array of billing records, each containing details like endpoint ID, GPU type, data center, and cost information.

#### Response Example
```json
{
  "example": "[\n  {\n    \"endpointId\": \"jpnw0v75y3qoql\",\n    \"gpuType\": \"NVIDIA A40\",\n    \"dataCenter\": \"EU-RO-1\",\n    \"cost\": 0.5,\n    \"startTime\": \"2023-01-01T00:00:00Z\",\n    \"endTime\": \"2023-01-01T23:59:59Z\"\n  }\n]"
}
```
```

--------------------------------

### ApiKey Type Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents the `ApiKey` type, showcasing fields such as ID, permissions, creation date, last usage date, name, policies, and activation status.

```json
{
  "id": "String",
  "permissions": "String!",
  "createdAt": "DateTime!",
  "lastUsed": "DateTime",
  "name": "String",
  "policies": "JSON",
  "isActive": "Boolean",
  "isActiveHumanReadableString": "String",
  "isLegacy": "Boolean"
}
```

--------------------------------

### Clone cog-worker Repository (Bash)

Source: https://docs.runpod.io/tutorials/migrations/cog/overview

Clones the cog-worker repository from GitHub and navigates into the directory. This repository contains essential scripts and configuration files for the migration process. It requires Git to be installed.

```bash
git clone https://github.com/runpod-workers/cog-worker.git
cd cog-worker/
```

--------------------------------

### Map Secrets to Environment Variables

Source: https://docs.runpod.io/pods/templates/secrets

This example demonstrates how to map secrets stored in RunPod to environment variables within your Pod templates. This allows your applications to access sensitive information securely without hardcoding it.

```shell
API_KEY={{ RUNPOD_SECRET_openai_key }}
DATABASE_URL={{ RUNPOD_SECRET_db_connection }}
```

--------------------------------

### Setting Environment Variables in Runpod

Source: https://docs.runpod.io/pods/references/environment-variables

Example of setting environment variables for a Runpod Pod. These variables can be accessed by your application to configure its behavior, such as specifying model names or API endpoints.

```bash
# Set a model name that your application can read
MODEL_NAME=llama-2-7b-chat
API_ENDPOINT=https://api.example.com/v1
MAX_BATCH_SIZE=32

```

--------------------------------

### RunPod User Billing Input Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents the input for fetching user billing data. It allows specifying the granularity of the billing data, such as 'MINUTELY'. This is used when querying billing information via the API.

```json
{
  "granularity": "MINUTELY"
}
```

--------------------------------

### Get Docker Image SHA for Versioning

Source: https://docs.runpod.io/serverless/workers/deploy

Retrieves the SHA256 digest of a Docker image after it has been pushed. This SHA tag is recommended for absolute reproducibility in production deployments.

```sh
# Get the SHA after pushing
docker inspect --format='{{index .RepoDigests 0}}' DOCKER_USERNAME/WORKER_NAME:VERSION
```

--------------------------------

### Upload Model to Hugging Face Repository

Source: https://docs.runpod.io/tutorials/pods/fine-tune-llm-axolotl

Uploads the merged model files to the specified Hugging Face model repository. 'your_model_name' should match the repository name, and 'path_to_your_model' is the local path to your model files.

```bash
huggingface-cli upload your_model_name path_to_your_model
```

--------------------------------

### Run vLLM Chat Completions Endpoint

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

Sends a POST request to the vLLM chat completions endpoint to generate responses in a conversational format. This example shows how to structure the 'messages' array for a chat interaction. It requires the endpoint ID and an API key.

```bash
curl -X POST "https://ENDPOINT_ID.api.runpod.ai/v1/chat/completions" \
     -H 'Authorization: Bearer RUNPOD_API_KEY' \
     -H 'Content-Type: application/json' \
     -d '{
       "messages": [
         {"role": "user", "content": "Tell me a short story"}
       ],
       "max_tokens": 100,
       "temperature": 0.8
     }'
```

--------------------------------

### RunPod IO Pod Stop Response Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents the response received after stopping a pod. It includes details about the pod's state, cost, and configuration.

```json
{
  "data": {
    "podStop": {
      "lowestBidPriceToResume": 123.45,
      "aiApiId": "xyz789",
      "apiKey": "xyz789",
      "clusterIp": "abc123",
      "clusterIdx": 987,
      "clusterCidr": "xyz789",
      "clusterRole": "SLURM_CONTROLLER",
      "consumerUserId": "abc123",
      "containerDiskInGb": 987,
      "containerRegistryAuthId": "abc123",
      "costMultiplier": 987.65,
      "costPerHr": 123.45,
      "createdAt": "2007-12-03T10:15:30Z",
      "adjustedCostPerHr": 987.65,
      "desiredStatus": "CREATED",
      "dockerArgs": "xyz789",
      "dockerId": "xyz789",
      "env": ["abc123"],
      "gpuCount": 987,
      "gpuPowerLimitPercent": 987,
      "gpus": [Gpu],
      "id": "abc123",
      "imageName": "xyz789",
      "lastStatusChange": "xyz789",
      "locked": true,
      "machineId": "xyz789",
      "memoryInGb": 987.65,
      "name": "abc123",
      "podType": "INTERRUPTABLE",
      "port": 43612,
      "ports": "xyz789",
      "registry": PodRegistry,
      "templateId": "abc123",
      "uptimeSeconds": 987,
      "vcpuCount": 123.45,
      "version": 123,
      "volumeEncrypted": true,
      "volumeInGb": 123.45,
      "volumeKey": "abc123",
      "volumeMountPath": "abc123",
      "lastStartedAt": "2007-12-03T10:15:30Z",
      "cpuFlavorId": "xyz789",
      "machineType": "xyz789",
      "slsVersion": 987,
      "networkVolumeId": "xyz789",
      "testPod": false,
      "ideAiApiId": "abc123",
      "hubReleaseId": "xyz789",
      "hubRelease": HubRelease,
      "modelVersions": [ModelVersionPodAssignment],
      "cpuFlavor": CpuFlavor,
      "runtime": PodRuntime,
      "machine": PodMachineInfo,
      "latestTelemetry": PodTelemetry,
      "endpoint": Endpoint,
      "networkVolume": NetworkVolume,
      "savingsPlans": [SavingsPlan],
      "clusterId": "xyz789",
      "ipAddress": IPAddress,
      "models": ["xyz789"]
    }
  }
}
```

--------------------------------

### Nano Banana Edit Image Combination

Source: https://docs.runpod.io/hub/public-endpoint-reference

Nano Banana Edit combines multiple source images based on a text prompt. It requires an array of image URLs and supports safety checks. The prompt guides the combination process.

```json
{
  "input": {
    "prompt": "Combine these four source images into a single realistic 3D character figure scene",
    "images": [
      "https://image.runpod.ai/uploads/0bz_xzhuLq/a2166199-5bd5-496b-b9ab-a8bae3f73bdc.jpg",
      "https://image.runpod.ai/uploads/Yw86rhY6xi/2ff8435f-f416-4096-9a4d-2f8c838b2d53.jpg"
    ],
    "enable_safety_checker": true
  }
}
```

--------------------------------

### Local Testing Output Analysis

Source: https://docs.runpod.io/tutorials/sdks/python/get-started/running-locally

This output log provides insights into the local execution of a Runpod serverless function. It confirms the server start, input processing, handler execution, and final job status, aiding in debugging and verification.

```bash
-- Starting Serverless Worker |  Version 1.6.2 --
INFO   | Using test_input.json as job input.
DEBUG  | Retrieved local job: {'input': {'name': 'World'}, 'id': 'local_test'}
INFO   | local_test | Started.
DEBUG  | local_test | Handler output: Hello World!
DEBUG  | local_test | run_job return: {'output': 'Hello World!'}
INFO   | Job local_test completed successfully.
INFO   | Job result: {'output': 'Hello World!'}
INFO   | Local testing complete, exiting.
```

--------------------------------

### Retrieve Job Results using cURL

Source: https://docs.runpod.io/tutorials/serverless/run-your-first

This command-line snippet demonstrates how to fetch the status and results of a Runpod job using cURL. It sends a GET request to the job status endpoint with necessary authentication headers and pipes the JSON output to a file named 'output.json'. Ensure you replace 'ENDPOINT_ID', 'JOB_ID', and 'YOUR_API_KEY' with your specific values.

```bash
curl https://api.runpod.ai/v2/ENDPOINT_ID/status/JOB_ID \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' | jq . > output.json
```

--------------------------------

### Python API Request to Runpod Flux Dev

Source: https://docs.runpod.io/hub/public-endpoints

Example Python code to make a POST request to the Runpod API's `/run` endpoint for image generation using the Flux Dev model. It includes setting up headers and JSON data for the prompt, image format, and generation parameters. Requires the 'requests' library.

```python
import requests

headers = {"Content-Type": "application/json", "Authorization": "Bearer RUNPOD_API_KEY"}

data = {
    "input": {
        "prompt": "A serene mountain landscape at sunset",
        "image_format": "png",
        "num_inference_steps": 25,
        "guidance": 7,
        "seed": 50,
        "width": 1024,
        "height": 1024,
    }
}

response = requests.post(
    "https://api.runpod.ai/v2/black-forest-labs-flux-1-dev/run",
    headers=headers,
    json=data,
)
```

--------------------------------

### Background Pod Telemetry Input (JSON)

Source: https://docs.runpod.io/references/graphql-spec

Example input for background pod telemetry, requiring machine ID and GPU index. This data is used to monitor specific GPU instances within a machine.

```json
{"machineId": "abc123", "gpuIndex": 987}
```

--------------------------------

### GitHub Actions Workflow for Testing and Deployment

Source: https://docs.runpod.io/serverless/workers/github-integration

This GitHub Actions workflow automates the process of building and pushing a Docker image, running tests, and deploying to Runpod. It requires a Docker username, worker name, and a Runpod API key stored as a GitHub secret. The workflow triggers on push or pull request events to the main branch.

```yaml
name: Test and Deploy

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test-and-deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: [DOCKER_USERNAME]/[WORKER_NAME]:${{ github.sha }}
        
    - name: Run Tests
      uses: runpod/runpod-test-runner@v1
      with:
        image-tag: [DOCKER_USERNAME]/[WORKER_NAME]:${{ github.sha }}
        runpod-api-key: ${{ secrets.RUNPOD_API_KEY }} # Add your API key to a GitHub secret
        test-filename: .github/tests.json
        request-timeout: 300
```

--------------------------------

### Build and Push Docker Image for RunPod

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

Commands to build a Docker image for RunPod and push it to a container registry like Docker Hub. The `--platform linux/amd64` flag ensures compatibility with RunPod's infrastructure. Ensure you are logged into your Docker registry.

```bash
docker build --platform linux/amd64 --tag YOUR_USERNAME/dual-mode-worker .
docker push YOUR_USERNAME/dual-mode-worker:latest
```

--------------------------------

### RunPod User Cluster Billing Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON structure details billing information for cluster usage. It includes timestamps, billed amounts, duration in seconds, cluster ID, and GPU type ID. This is helpful for detailed cost analysis of cluster resources.

```json
{
  "time": "2007-12-03T10:15:30Z",
  "amount": 123.45,
  "timeBilledSeconds": 987,
  "clusterId": "xyz789",
  "gpuTypeId": "xyz789"
}
```

--------------------------------

### Generate Image with Qwen Image Model

Source: https://docs.runpod.io/hub/public-endpoint-reference

This snippet demonstrates how to generate an image using the Qwen Image model. It requires a prompt and allows for negative prompts, size, seed, and safety checker configuration. The output is a JSON object representing the generation request.

```json
{
  "input": {
    "prompt": "A fashion-forward woman sitting at cobblestone street in Paris",
    "negative_prompt": "",
    "size": "1328*1328",
    "seed": -1,
    "enable_safety_checker": true
  }
}
```

--------------------------------

### GET /v2/{ENDPOINT_ID}/status/{JOB_ID} - Monitor Job Progress

Source: https://docs.runpod.io/tutorials/serverless/run-your-first

Retrieves the current status of an image generation job using its unique job ID. This endpoint is used to track the progress of submitted jobs.

```APIDOC
## GET /v2/{ENDPOINT_ID}/status/{JOB_ID}

### Description
Retrieves the current status of an image generation job using its unique job ID. This endpoint is used to track the progress of submitted jobs.

### Method
GET

### Endpoint
`https://api.runpod.ai/v2/{ENDPOINT_ID}/status/{JOB_ID}`

### Parameters
#### Path Parameters
- **ENDPOINT_ID** (string) - Required - The unique identifier of your deployed Serverless endpoint.
- **JOB_ID** (string) - Required - The unique identifier of the job to check.

#### Query Parameters
None

#### Request Body
None

### Request Example
```bash
curl https://api.runpod.ai/v2/ENDPOINT_ID/status/JOB_ID \
-H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY'
```

### Response
#### Success Response (200)
- **delayTime** (integer) - The time the job waited in the queue before processing, in milliseconds.
- **id** (string) - The job ID.
- **input** (object) - The input parameters used for the job.
- **status** (string) - The current status of the job (e.g., "IN_PROGRESS", "COMPLETED", "FAILED").

#### Response Example
```json
{
  "delayTime": 2624,
  "id": "c80ffee4-f315-4e25-a146-0f3d98cf024b",
  "input": {
    "prompt": "A cute fluffy white dog in the style of a Pixar animation 3D drawing."
  },
  "status": "IN_PROGRESS"
}
```
```

--------------------------------

### GET /health

Source: https://docs.runpod.io/serverless/endpoints/operations

Retrieves a quick overview of your endpoint’s operational status, including worker availability, job queue status, potential bottlenecks, and scaling requirements.

```APIDOC
## GET /health

### Description
Retrieves a quick overview of your endpoint’s operational status, including worker availability, job queue status, potential bottlenecks, and scaling requirements.

### Method
GET

### Endpoint
`https://api.runpod.ai/v2/$ENDPOINT_ID/health`

### Parameters

#### Query Parameters
None

#### Request Body
None

### Request Example
```curl
curl --request GET \
     --url https://api.runpod.ai/v2/$ENDPOINT_ID/health \
     -H "authorization: $RUNPOD_API_KEY"
```

### Response
#### Success Response (200)
Returns a JSON response with the current status of the endpoint, including job statistics and worker status.
- **jobs** (object) - Contains statistics about the jobs.
  - **completed** (integer) - Number of completed jobs.
  - **failed** (integer) - Number of failed jobs.
  - **inProgress** (integer) - Number of jobs currently in progress.
  - **inQueue** (integer) - Number of jobs waiting in the queue.
  - **retried** (integer) - Number of retried jobs.
- **workers** (object) - Contains statistics about the workers.
  - **idle** (integer) - Number of idle workers.
  - **running** (integer) - Number of running workers.

#### Response Example
```json
{
  "jobs": {
    "completed": 1,
    "failed": 5,
    "inProgress": 0,
    "inQueue": 2,
    "retried": 0
  },
  "workers": {
    "idle": 0,
    "running": 0
  }
}
```
```

--------------------------------

### Local Test Input JSON for Stable Diffusion

Source: https://docs.runpod.io/tutorials/sdks/python/102/stable-diffusion-text-to-image

This JSON file provides a sample input structure for testing the Stable Diffusion serverless worker locally. It includes a 'prompt' key within an 'input' object, which is expected by the handler function.

```json
{
  "input": {
    "prompt": "A serene landscape with mountains and a lake at sunset"
  }
}
```

--------------------------------

### Get Run Status with Go SDK

Source: https://docs.runpod.io/sdks/go/endpoints

Retrieves the status of a specific RunPod job using its ID. Requires the RUNPOD_API_KEY and RUNPOD_BASE_URL environment variables. Outputs the job status in JSON format.

```go
package main

import (
	"encoding/json"
	"fmt"
	"log"
	"os"

	"github.com/runpod/go-sdk/pkg/sdk"
	"github.com/runpod/go-sdk/pkg/sdk/config"
	rpEndpoint "github.com/runpod/go-sdk/pkg/sdk/endpoint"
)

func main() {

	apiKey := os.Getenv("RUNPOD_API_KEY")
	baseURL := os.Getenv("RUNPOD_BASE_URL")

	endpoint, err := rpEndpoint.New(
		&config.Config{ApiKey: &apiKey},
		&rpEndpoint.Option{EndpointId: &baseURL},
	)
	if err != nil {
		log.Fatalf("Failed to create endpoint: %v", err)
	}
	input := rpEndpoint.StatusInput{
		Id: sdk.String("5efff030-686c-4179-85bb-31b9bf97b944-u1"),
	}
	output, err := endpoint.Status(&input)
	if err != nil {
		panic(err)
	}
	dt, _ := json.Marshal(output)
	fmt.Printf("output:%s\n", dt)
}
```

--------------------------------

### vLLM Worker Test Request Output

Source: https://docs.runpod.io/serverless/vllm/get-started

This JSON object shows the typical output received after sending a test request to a vLLM worker. It includes details like processing time, status, and the generated output, which in this example is a placeholder 'CHAT_RESPONSE'.

```json
{
  "delayTime": 638,
  "executionTime": 3344,
  "id": "f0706ead-c5ec-4689-937c-e21d5fbbca47-u1",
  "output": [
    {
      "choices": [
        {
          "tokens": ["CHAT_RESPONSE"]
        }
      ],
      "usage": {
        "input": 3,
        "output": 100
      }
    }
  ],
  "status": "COMPLETED",
  "workerId": "0e7o8fgmm9xgty"
}
```

--------------------------------

### Example Structured JSON Log Output

Source: https://docs.runpod.io/serverless/development/write-logs

Demonstrates the expected JSON output format for structured logs generated by the Python code. Each log entry is a JSON object containing fields like 'level', 'message', and custom fields such as 'request_id' and 'execution_time_ms'.

```json
{"level": "INFO", "message": "Processing request", "request_id": "abc123", "input_keys": ["prompt", "max_length"]}
{"level": "INFO", "message": "Request completed", "request_id": "abc123", "execution_time_ms": 123}
```

--------------------------------

### Create Serverless Template using cURL and GraphQL

Source: https://docs.runpod.io/sdks/graphql/manage-pod-templates

This snippet demonstrates how to create a serverless template using the RunPod API. It requires specifying template details such as container disk size, Docker arguments, environment variables, image name, and a name for the template. For serverless templates, `volumeInGb` must be 0. The output shows the details of the newly created template.

```bash
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \"python handler.py\", env: [ { key: \"key1\", value: \"value1\" }, { key: \"key2\", value: \"value2\" } ], imageName: \"runpod/serverless-hello-world:latest\", isServerless: true, name: \"Generated Serverless Template\", readme: \"## Hello, World!\", volumeInGb: 0 }) { containerDiskInGb dockerArgs env { key value } id imageName isServerless name readme } }"}'
```

```graphql
mutation {
  saveTemplate(input: {
    containerDiskInGb: 5,
    dockerArgs: "python handler.py",
    env: [
      {
        key: "key1",
        value: "value1"
      },
      {
        key: "key2",
        value: "value2"
      }
    ],
    imageName: "runpod/serverless-hello-world:latest",
    isServerless: true,
    name: "Generated Serverless Template",
    readme: "## Hello, World!",
    volumeInGb: 0
  }) {
    containerDiskInGb
    dockerArgs
    env {
      key
      value
    }
    id
    imageName
    isServerless
    name
    readme
  }
}
```

```json
{
  "data": {
    "saveTemplate": {
      "containerDiskInGb": 5,
      "dockerArgs": "python handler.py",
      "env": [
        {
          "key": "key1",
          "value": "value1"
        },
        {
          "key": "key2",
          "value": "value2"
        }
      ],
      "id": "xkhgg72fuo",
      "imageName": "runpod/serverless-hello-world:latest",
      "isServerless": true,
      "name": "Generated Serverless Template",
      "readme": "## Hello, World!"
    }
  }
}
```

--------------------------------

### Initialize vLLM Engine

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

Initializes the vLLM asynchronous engine. It retrieves the model name and configuration parameters from environment variables. The function handles potential exceptions during initialization and sets a global flag to indicate engine readiness. Dependencies include `os`, `AsyncLLMEngine`, and `AsyncEngineArgs`.

```python
import os
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.engine.arg_utils import AsyncEngineArgs

# Global variables
engine: Optional[AsyncLLMEngine] = None
engine_ready = False

async def create_engine():
    """Initialize the vLLM engine"""
    global engine, engine_ready
    
    try:
        # Get model name from environment variable
        model_name = os.getenv("MODEL_NAME", "microsoft/DialoGPT-medium")
        
        # Configure engine arguments
        engine_args = AsyncEngineArgs(
            model=model_name,
            tensor_parallel_size=int(os.getenv("TENSOR_PARALLEL_SIZE", "1")),
            dtype=os.getenv("DTYPE", "auto"),
            trust_remote_code=os.getenv("TRUST_REMOTE_CODE", "true").lower() == "true",
            max_model_len=int(os.getenv("MAX_MODEL_LEN")) if os.getenv("MAX_MODEL_LEN") else None,
            gpu_memory_utilization=float(os.getenv("GPU_MEMORY_UTILIZATION", "0.9")),
            enforce_eager=os.getenv("ENFORCE_EAGER", "false").lower() == "true",
        )
        
        # Create the engine
        engine = AsyncLLMEngine.from_engine_args(engine_args)
        engine_ready = True
        logger.info(f"vLLM engine initialized successfully with model: {model_name}")
        
    except Exception as e:
        logger.error(f"Failed to initialize vLLM engine: {str(e)}")
        engine_ready = False
        raise
```

--------------------------------

### Monitor Job Progress with cURL

Source: https://docs.runpod.io/tutorials/serverless/comfyui

This command-line snippet uses `curl` to send a GET request to the RunPod AI API's status endpoint. It requires an endpoint ID, job ID, and an API key for authentication. The response indicates the job's current status, such as 'IN_PROGRESS'.

```bash
curl https://api.runpod.ai/v2/ENDPOINT_ID/status/JOB_ID \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY'
```

--------------------------------

### Create Concurrent Handler with Python Asyncio

Source: https://docs.runpod.io/serverless/workers/concurrent-handler

Defines an asynchronous handler function 'process_request' that simulates processing jobs with delays using asyncio. It also includes a placeholder 'adjust_concurrency' function and starts the Runpod serverless endpoint.

```python
import runpod
import asyncio
import random

# Global variable to simulate a varying request rate
request_rate = 0

async def process_request(job):
    # This function processes incoming requests concurrently.
    #
    # Args:
    #     job (dict): Contains the input data and request metadata
    #
    # Returns:
    #     str: The processed result
 
    
    # Extract input data
    job_input = job["input"]
    delay = job_input.get("delay", 1)
    
    # Simulate an asynchronous task (like a database query or API call)
    await asyncio.sleep(delay)
    
    return f"Processed: {job_input}"

# Placeholder code for a dynamic concurrency adjustment function 
def adjust_concurrency(current_concurrency):
    return 50

def update_request_rate():
    """Simulates changes in the request rate to mimic real-world scenarios."""
    global request_rate
    request_rate = random.randint(20, 100)

# Start the Serverless function when the script is run
if __name__ == "__main__":
    runpod.serverless.start({
        "handler": process_request,
        "concurrency_modifier": adjust_concurrency
    })
```

--------------------------------

### Rent Pod with CUDA Version Filter (cURL, GraphQL)

Source: https://docs.runpod.io/sdks/graphql/manage-pods

Rent an interruptible pod, specifying allowed CUDA versions for GPU compatibility. This example uses cURL to send a GraphQL mutation to the RunPod API. It requires an API key and defines pod specifications like GPU type, image, and environment variables.

```bash
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{ \
    "query": "mutation { podRentInterruptable( input: { bidPerGpu: 0.2, cloudType: SECURE, gpuCount: 1, volumeInGb: 40, containerDiskInGb: 40, minVcpuCount: 2, minMemoryInGb: 15, gpuTypeId: \"NVIDIA RTX A6000\", name: \"Runpod Pytorch\", imageName: \"runpod/pytorch\", dockerArgs: \"\", ports: \"8888/http\", volumeMountPath: \"/workspace\", env: [{ key: \"JUPYTER_PASSWORD\", value: \"vunw9ybnzqwpia2795p2\" }], allowedCudaVersions: [\"12.0\", \"12.1\", \"12.2\", \"12.3\" ] } ) { id imageName env machineId machine { podHostId } } }" \
  }'
```

```GraphQL
mutation {
  podRentInterruptable(input: {
    bidPerGpu: 0.2,
    cloudType: SECURE,
    gpuCount: 1,
    volumeInGb: 40,
    containerDiskInGb: 40,
    minVcpuCount: 2,
    minMemoryInGb: 15,
    gpuTypeId: "NVIDIA RTX A6000",
    name: "Runpod Pytorch",
    imageName: "runpod/pytorch",
    dockerArgs: "",
    ports: "8888/http",
    volumeMountPath: "/workspace",
    env: [{ key: "JUPYTER_PASSWORD", value: "vunw9ybnzqwpia2795p2" }],
    allowedCudaVersions: ["12.0", "12.1", "12.2", "12.3"]
  }) {
    id
    imageName
    env
    machineId
    machine {
      podHostId
    }
  }
}
```

--------------------------------

### WAN 2.1 T2V 720p

Source: https://docs.runpod.io/hub/public-endpoint-reference

Open-source AI text-to-video generation using a diffusion transformer architecture at 720p.

```APIDOC
## POST /v2/wan-2-1-t2v-720/

### Description
Generates video from a text prompt using a diffusion transformer architecture at 720p resolution.

### Method
POST

### Endpoint
`https://api.runpod.ai/v2/wan-2-1-t2v-720/`

### Parameters
#### Query Parameters
- **prompt** (string) - Required - The text prompt to generate video from.
- **duration** (integer) - Optional - The desired duration of the video in seconds.

### Request Example
```json
{
  "prompt": "A bustling alien marketplace with strange creatures",
  "duration": 5
}
```

### Response
#### Success Response (200)
- **video_url** (string) - The URL to the generated 720p video.

#### Response Example
```json
{
  "video_url": "https://storage.runpod.ai/videos/wan2.1_t2v_output.mp4"
}
```
```

--------------------------------

### User Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents a user, including their pods, background pods, and paginated pod connections. It also contains fields for machines, machine summaries, user identifiers, container registry credentials, spending information, quotas, referral earnings, terms of service agreement, spend limits, payment details, and multi-factor authentication status.

```json
{
  "pods": ["Pod"],
  "backgroundPods": ["Pod"],
  "podsConnection": "PodsConnection",
  "machines": ["Machine"],
  "hiddenMachines": ["Machine"],
  "machinesSummary": ["MachineSummary"],
  "id": "String",
  "authId": "String",
  "email": "String",
  "containerRegistryCreds": ["ContainerRegistryAuth"],
  "currentSpendPerHr": 0.0,
  "machineQuota": 0,
  "referralEarned": 0.0,
  "signedTermsOfService": true,
  "spendLimit": 0,
  "stripeSavedPaymentId": "String",
  "stripeSavedPaymentLast4": "String",
  "templateEarned": 0.0,
  "multiFactorEnabled": true,
  "machineEarnings": ["MachineEarning"],
  "machineEarningsCustomRange": ["MachineEarning"],
  "networkStorageEarnings": ["NetworkStorageEarning"]
}
```

--------------------------------

### Transfer Files with runpodctl CLI

Source: https://docs.runpod.io/pods/storage/transfer-files

Use the runpodctl CLI for quick, occasional file transfers between your local machine and RunPod Pods. This method utilizes secure one-time codes and requires no prior setup on the Pod as it's pre-installed. It's ideal for small to medium-sized files.

```bash
runpodctl send YOUR_FILE
```

```bash
runpodctl receive 8338-galileo-collect-fidel
```

--------------------------------

### RunPod User Billing Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON structure outlines the billing details for a RunPod user. It categorizes costs by GPU cloud, CPU cloud, serverless usage, cluster usage, endpoint usage, storage, and provides a summary. This is useful for analyzing spending patterns.

```json
{
  "gpuCloud": [UserGpuCloudBilling],
  "cpuCloud": [UserCpuCloudBilling],
  "serverless": [UserServerlessBilling],
  "cluster": [UserClusterBilling],
  "runpodEndpoint": [UserRunpodEndpointBilling],
  "storage": [UserStorageBilling],
  "summary": [UserSummaryBilling]
}
```

--------------------------------

### TeamScopes Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents the scopes assigned to a team member, currently containing a 'role' field. It's used to define permissions and access levels within a team.

```json
{
  "role": "xyz789"
}
```

--------------------------------

### Wan 2.2 I2V 720p Image-to-Video Generation

Source: https://docs.runpod.io/hub/public-endpoint-reference

This endpoint generates a 720p video from a given image and a text prompt describing the desired video content and motion.

```APIDOC
## POST /websites/runpod_io/wan2.2i2v720p

### Description
Generates a 720p video from a static image using the Wan 2.2 I2V model. The video content and motion are guided by a text prompt.

### Method
POST

### Endpoint
/websites/runpod_io/wan2.2i2v720p

### Parameters
#### Request Body
- **prompt** (string) - Required - Text description of the desired video motion and content.
- **image** (string) - Required - URL of the input image to animate.
- **negative_prompt** (string) - Optional - Elements to exclude from the generated video. Defaults to "".
- **size** (string) - Optional - Video resolution in format "width*height". Defaults to "1280*720".
- **num_inference_steps** (integer) - Optional - Number of denoising steps. Defaults to 30. Range: 1-50.
- **guidance** (float) - Optional - How closely to follow the prompt. Defaults to 5. Range: 0.0-10.0.
- **duration** (integer) - Optional - Video duration in seconds. Defaults to 5.
- **flow_shift** (integer) - Optional - Controls the motion flow in the generated video. Defaults to 5.
- **seed** (integer) - Optional - Provide a seed for reproducible results. The default value (-1) will generate a random seed.
- **enable_prompt_optimization** (boolean) - Optional - Whether to automatically optimize the prompt. Defaults to false.
- **enable_safety_checker** (boolean) - Optional - Whether to run safety checks on the output. Defaults to true.

### Request Example
```json
{
  "input": {
    "prompt": "cinematic shot: slow-tracking camera glides parallel to a giant white origami boat as it gently drifts down a jade-green river",
    "image": "https://image.runpod.ai/asset/alibaba/wan-2-2-i2v-720.png",
    "num_inference_steps": 30,
    "guidance": 5,
    "negative_prompt": "",
    "size": "1280*720",
    "duration": 5,
    "flow_shift": 5,
    "seed": -1,
    "enable_prompt_optimization": false,
    "enable_safety_checker": true
  }
}
```

### Response
#### Success Response (200)
- **video_url** (string) - URL of the generated video.

#### Response Example
```json
{
  "video_url": "https://example.com/generated_video.mp4"
}
```
```

--------------------------------

### RunPod User Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON structure represents a comprehensive user profile on RunPod. It includes details about pods, machines, billing, payment methods, and user preferences. This is a foundational data model for user-related operations.

```json
{
  "pods": [Pod],
  "backgroundPods": [Pod],
  "podsConnection": PodsConnection,
  "machines": [Machine],
  "hiddenMachines": [Machine],
  "machinesSummary": [MachineSummary],
  "id": "abc123",
  "authId": "abc123",
  "email": "xyz789",
  "containerRegistryCreds": [ContainerRegistryAuth],
  "currentSpendPerHr": 987.65,
  "machineQuota": 123,
  "referralEarned": 123.45,
  "signedTermsOfService": false,
  "spendLimit": 987,
  "stripeSavedPaymentId": "xyz789",
  "stripeSavedPaymentLast4": "xyz789",
  "templateEarned": 123.45,
  "multiFactorEnabled": false,
  "machineEarnings": [MachineEarning],
  "machineEarningsCustomRange": [MachineEarning],
  "networkStorageEarnings": [NetworkStorageEarning],
  "underBalance": true,
  "minBalance": 123.45,
  "stripeAutoReloadAmount": 123,
  "stripeAutoPaymentThreshold": 123,
  "spendDetails": SpendDetails,
  "maxServerlessConcurrency": 123,
  "clientLifetimeSpend": 123.45,
  "referralId": "abc123",
  "datacenters": [DataCenter],
  "githubAccountInfo": GithubAccountInfo,
  "discordAccountInfo": DiscordAccountInfo,
  "isAutoPayEnabled": false,
  "enableBackupPayments": true,
  "onboardingInfo": OnboardingInfo,
  "affiliateKey": "abc123",
  "hasActivated": true,
  "canRefer": false,
  "referralUsageId": "abc123",
  "activeMigrations": [PodMigration],
  "notifyCcList": "abc123",
  "userPreferences": [UserPreference],
  "clientBalance": 123.45,
  "hostBalance": 123.45,
  "hostStripeLinked": true,
  "stripeAccountId": "abc123",
  "stripeReloadHistory": [StripeReloadTransaction],
  "payoutHistory": [PayoutHistory],
  "dailyCharges": [ClientCreditCharge],
  "referral": UserReferral,
  "pubKey": "xyz789",
  "information": UserInformation,
  "notifyPodsStale": true,
  "notifyPodsGeneral": true,
  "notifyLowBalance": true,
  "creditAlertThreshold": 123.45,
  "notifyOther": true,
  "podTemplates": [PodTemplate],
  "creditCodes": [CreditCode],
  "endpoint": Endpoint,
  "endpoints": [Endpoint],
  "networkVolumes": [NetworkVolume],
  "teams": [Team],
  "ownedTeams": [Team],
  "team": Team,
  "teamMembership": TeamMembership,
  "teamScopes": TeamScopes,
  "isTeam": true,
  "savingsPlans": [SavingsPlan],
  "savingsPlan": SavingsPlan,
  "serverlessDiscount": Discount,
  "serverlessDiscounts": [Discount],
  "billing": UserBilling,
  "secrets": [Secret],
  "impersonations": [Impersonation],
  "activeImpersonation": Impersonation,
  "nodeGroups": [NodeGroup],
  "cluster": Cluster,
  "clusters": [Cluster],
  "apiKeys": [ApiKey],
  "sharedApiKeys": [SharedApiKey],
  "paymentMethods": [PaymentMethod],
  "paymentMethodsByPriority": [PaymentMethod],
  "defaultPaymentMethod": PaymentMethod,
  "reservations": [UserReservation]
}
```

--------------------------------

### POST /v1/chat/completions

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

Provides OpenAI-compatible chat completions. It takes a list of messages and sampling parameters, then returns a generated text response.

```APIDOC
## POST /v1/chat/completions

### Description
This endpoint provides OpenAI-compatible chat completions. It takes a list of messages and sampling parameters, then returns a generated text response.

### Method
POST

### Endpoint
/v1/chat/completions

### Parameters
#### Request Body
- **messages** (array[object]) - Required - An array of message objects, each with a 'role' (system, user, or assistant) and 'content'.
- **max_tokens** (integer) - Optional - The maximum number of tokens to generate in the completion.
- **temperature** (number) - Optional - Controls randomness. Lower values make the output more deterministic.
- **top_p** (number) - Optional - Nucleus sampling parameter. Controls diversity via probability mass.
- **stop** (array[string]) - Optional - A list of strings that will cause the generation to stop.

### Request Example
```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Tell me a joke."} 
  ],
  "max_tokens": 50,
  "temperature": 0.7
}
```

### Response
#### Success Response (200)
- **id** (string) - Unique identifier for the completion.
- **object** (string) - The type of object, usually 'chat.completion'.
- **model** (string) - The name of the model used.
- **choices** (array[object]) - A list of completion choices.
  - **index** (integer) - The index of the choice.
  - **message** (object) - The generated message.
    - **role** (string) - The role of the message sender (e.g., 'assistant').
    - **content** (string) - The generated text content.
  - **finish_reason** (string) - The reason the generation stopped (e.g., 'stop', 'length').
- **usage** (object) - Token usage statistics.
  - **prompt_tokens** (integer) - Number of tokens in the prompt.
  - **completion_tokens** (integer) - Number of tokens in the completion.
  - **total_tokens** (integer) - Total tokens used.

#### Response Example
```json
{
  "id": "req_abc123",
  "object": "chat.completion",
  "model": "microsoft/DialoGPT-medium",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Why don't scientists trust atoms? Because they make up everything!"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 12,
    "total_tokens": 27
  }
}
```

#### Error Response (e.g., 500)
- **detail** (object) - Contains error information.
  - **error_code** (string) - A code indicating the type of error.
  - **error_message** (string) - A human-readable error message.
  - **request_id** (string) - The ID associated with the failed request.
```

--------------------------------

### Login to Hugging Face CLI

Source: https://docs.runpod.io/fine-tune

Logs the user into the Hugging Face CLI, enabling authentication for subsequent commands. This is a prerequisite for uploading models. No specific inputs are required beyond user interaction for authentication.

```shell
huggingface-cli login
```

--------------------------------

### Team Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents a Team entity, including its ID, name, owner information, memberships, and available roles. It serves as a blueprint for team-related data within the RunPod IO system.

```json
{
  "id": "abc123",
  "name": "xyz789",
  "owner": "TeamOwner",
  "memberships": ["TeamMembership"],
  "membership": "TeamMembership",
  "members": ["TeamMembership"],
  "invites": ["TeamInvite"],
  "isOwner": true,
  "availableRoles": ["xyz789"]
}
```

--------------------------------

### Generate Image with Qwen Image and LoRA

Source: https://docs.runpod.io/hub/public-endpoint-reference

This snippet shows how to generate an image using Qwen Image with LoRA support for customization. It includes the prompt, LoRA configurations (path and scale), size, seed, and safety checker. This allows for fine-tuned image generation based on specific LoRA models.

```json
{
  "input": {
    "prompt": "Real life Anime in a cozy kitchen",
    "loras": [
      {
        "path": "https://huggingface.co/flymy-ai/qwen-image-anime-irl-lora/resolve/main/flymy_anime_irl.safetensors",
        "scale": 1
      }
    ],
    "size": "1024*1024",
    "seed": -1,
    "enable_safety_checker": true
  }
}
```

--------------------------------

### GET /billing/networkvolumes

Source: https://docs.runpod.io/api-reference/billing/GET/billing/networkvolumes

Retrieve billing information about your network volumes. This endpoint allows you to fetch historical billing data for your Runpod network volumes, aggregated over specified time intervals.

```APIDOC
## GET /billing/networkvolumes

### Description
Retrieve billing information about your network volumes. This endpoint allows you to fetch historical billing data for your Runpod network volumes, aggregated over specified time intervals.

### Method
GET

### Endpoint
/billing/networkvolumes

### Parameters
#### Query Parameters
- **bucketSize** (string) - Optional - The length of each billing time bucket. The billing time bucket is the time range over which each billing record is aggregated. Allowed values: `hour`, `day`, `week`, `month`, `year`. Defaults to `day`.
- **endTime** (string) - Optional - The end date of the billing period to retrieve. Format: `date-time` (e.g., '2023-01-31T23:59:59Z').
- **startTime** (string) - Optional - The start date of the billing period to retrieve. Format: `date-time` (e.g., '2023-01-01T00:00:00Z').

### Request Example
```json
{
  "example": "GET /billing/networkvolumes?bucketSize=month&startTime=2023-01-01T00:00:00Z&endTime=2023-01-31T23:59:59Z"
}
```

### Response
#### Success Response (200)
- **amount** (number) - The amount charged for the group for the billing period, in USD.
- **diskSpaceBilledGb** (integer) - The amount of disk space billed for the billing period, in gigabytes (GB). Does not apply to all resource types.
- **highPerformanceStorageAmount** (number) - The amount charged for high performance storage for the billing period, in USD.
- **highPerformanceStorageDiskSpaceBilledGb** (integer) - The amount of high performance storage disk space billed for the billing period, in gigabytes (GB).
- **time** (string) - The start of the period for which the billing record applies. Format: `date-time`.

#### Response Example
```json
{
  "example": [
    {
      "amount": 100.5,
      "diskSpaceBilledGb": 50,
      "highPerformanceStorageAmount": 100.5,
      "highPerformanceStorageDiskSpaceBilledGb": 50,
      "time": "2023-01-01T00:00:00Z"
    }
  ]
}
```
```

--------------------------------

### Local Testing Commands for Runpod Serverless Function (Bash)

Source: https://docs.runpod.io/tutorials/sdks/python/101/aggregate

These bash commands demonstrate how to test the Python serverless function locally using command-line arguments. They show how to provide JSON input for both sentiment analysis and object detection tasks, allowing for verification of the function's behavior before deployment.

```bash
python your_script.py --test_input '
{
  "input": {
    "task_type": "sentiment",
    "items": [
      "I love this product!",
      "The service was terrible.",
      "It was okay, nothing special."
    ]
  }
}'

```

```bash
python your_script.py --test_input '
{
  "input": {
    "task_type": "object_detection",
    "items": [
      "image1.jpg",
      "image2.jpg",
      "image3.jpg"
    ]
  }
}'

```

--------------------------------

### Run vLLM Health Check Endpoint

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

Executes a GET request to the health check endpoint of a vLLM load balancer. This is useful for verifying if the endpoint is responsive. It requires the endpoint ID and an API key for authentication.

```bash
curl -X GET "https://ENDPOINT_ID.api.runpod.ai/ping" \
    -H 'Authorization: Bearer RUNPOD_API_KEY' \
    -H "Content-Type: application/json" \

```

--------------------------------

### Get Object with AWS S3API CLI

Source: https://docs.runpod.io/serverless/storage/s3-api

Downloads an object from a Runpod network volume using the `aws s3api get-object` command. This method requires specifying the bucket (NETWORK_VOLUME_ID), key (REMOTE_FILE), region, endpoint URL, and the local path to save the file.

```bash
aws s3api get-object --bucket NETWORK_VOLUME_ID \
    --key REMOTE_FILE \
    --region DATACENTER \
    --endpoint-url https://s3api-DATACENTER.runpod.io/ \
    LOCAL_FILE
```

--------------------------------

### RunPod Cancel Operation Response

Source: https://docs.runpod.io/serverless/endpoints/operations

This is an example of the JSON response received after a successful cancel operation on a RunPod job. It includes the job ID and its new status, which should be 'CANCELLED'.

```json
{
  "id": "724907fe-7bcc-4e42-998d-52cb93e1421f-u1",
  "status": "CANCELLED"
}

```

--------------------------------

### Access Symmetrical TCP Ports in Python Application

Source: https://docs.runpod.io/pods/configuration/expose-ports

This Python code example shows how to access an assigned symmetrical TCP port from environment variables and use it to configure a web application. It defaults to port 8000 if the environment variable is not set.

```python
import os

# Get the assigned port or use a default
port = os.environ.get('RUNPOD_TCP_PORT_70000', '8000')
app.run(host='0.0.0.0', port=int(port))
```

--------------------------------

### TeamMembership Data Structure Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object represents a user's membership within a team, including the membership ID, associated user and team, scopes, and timestamps for creation and updates. It defines the relationship between a user and a team.

```json
{
  "id": 4,
  "member": "User",
  "team": "Team",
  "scopes": {},
  "createdAt": "2007-12-03T10:15:30Z",
  "updatedAt": "2007-12-03T10:15:30Z"
}
```

--------------------------------

### Configure Runpod Go SDK with Environment Variables

Source: https://docs.runpod.io/sdks/go/endpoints

This Go code snippet demonstrates how to retrieve the Runpod API key and base URL from environment variables. It includes checks to ensure these variables are set before proceeding, which is crucial for authenticating SDK calls.

```go
package main

import (
	"log"
	"os"

	"github.com/runpod/go-sdk/pkg/sdk"
	"github.com.runpod/go-sdk/pkg/sdk/config"
	rpEndpoint "github.com/runpod/go-sdk/pkg/sdk/endpoint"
)

func main() {
	// Retrieve the API key and base URL from environment variables
	apiKey := os.Getenv("RUNPOD_API_KEY")
	baseURL := os.Getenv("RUNPOD_BASE_URL")

	// Check if environment variables are set
	if apiKey == "" {
		log.Fatalf("Environment variable RUNPOD_API_KEY is not set")
	}
	if baseURL == "" {
		log.Fatalf("Environment variable RUNPOD_BASE_URL is not set")
	}


    // Use the endpoint object
    // ...
}
```

--------------------------------

### Perform Inference with Fine-tuned Model using Axolotl

Source: https://docs.runpod.io/tutorials/pods/fine-tune-llm-axolotl

This command runs inference on your fine-tuned LLM. It requires the training configuration file and the directory where the LoRA model weights are saved. This allows you to interact with and test the model's performance.

```bash
accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml --lora_model_dir="./lora-out"
```

--------------------------------

### WAN 2.2 T2V 720p

Source: https://docs.runpod.io/hub/public-endpoint-reference

Open-source AI text-to-video generation using a diffusion transformer architecture at 720p.

```APIDOC
## POST /v2/wan-2-2-t2v-720/

### Description
Generates video from a text prompt using a diffusion transformer architecture at 720p resolution.

### Method
POST

### Endpoint
`https://api.runpod.ai/v2/wan-2-2-t2v-720/`

### Parameters
#### Query Parameters
- **prompt** (string) - Required - The text prompt to generate video from.
- **duration** (integer) - Optional - The desired duration of the video in seconds.

### Request Example
```json
{
  "prompt": "A serene forest scene with sunlight filtering through the trees",
  "duration": 5
}
```

### Response
#### Success Response (200)
- **video_url** (string) - The URL to the generated 720p video.

#### Response Example
```json
{
  "video_url": "https://storage.runpod.ai/videos/wan2.2_t2v_output.mp4"
}
```
```

--------------------------------

### List Available Models (Python)

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

This Python snippet demonstrates how to retrieve a list of all available models on a RunPod endpoint. It uses the `client.models.list()` method and then extracts the model IDs into a Python list for further processing or display. This is essential for selecting the correct model for API requests.

```python
models_response = client.models.list()
list_of_models = [model.id for model in models_response]
print(list_of_models)
```

--------------------------------

### Load Stable Diffusion Model for Inference

Source: https://docs.runpod.io/tutorials/sdks/python/102/stable-diffusion-text-to-image

Defines a function to load the Stable Diffusion model (v1.5) from Hugging Face and move it to the CUDA-enabled GPU. Loading the model once when the worker starts optimizes performance for subsequent requests.

```python
def load_model():
    model_id = "runwayml/stable-diffusion-v1-5"
    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
    pipe = pipe.to("cuda")
    return pipe
```

--------------------------------

### Send HTTP POST Request to Local Server with curl

Source: https://docs.runpod.io/tutorials/sdks/python/101/local-server-testing

This snippet demonstrates how to send an HTTP POST request to a local Runpod server using the `curl` command-line tool. It specifies the request method, URL, content type, and the JSON payload for testing the serverless function. This is useful for simulating production calls during local development.

```bash
curl -X POST http://localhost:8000/run \
     -H "Content-Type: application/json" \
     -d '{"input": {"text": "Hello, Runpod!"}}'
```

--------------------------------

### Text Completion Request (Python)

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

This Python code shows how to make a non-streaming text completion request to the RunPod API. It utilizes the `client.completions.create` method with parameters like `model`, `prompt`, `temperature`, and `max_tokens`. The response is then printed, displaying the completed text.

```python
# Text completion request
response = client.completions.create(
    model=MODEL_NAME,
    prompt="Write a poem about artificial intelligence:",
    temperature=0.7,
    max_tokens=150
)

# Print the response
print(response.choices[0].text)
```

--------------------------------

### Generate SSH Key Pair (Shell)

Source: https://docs.runpod.io/pods/configuration/use-ssh

Generates an SSH key pair using the ed25519 algorithm. This command is run on your local terminal and creates a public and private key file. Ensure you replace the placeholder email with your actual email address.

```sh
ssh-keygen -t ed25519 -C "YOUR_EMAIL@DOMAIN.COM"
```

--------------------------------

### Stop Pod (cURL, GraphQL)

Source: https://docs.runpod.io/sdks/graphql/manage-pods

Stop a running pod by providing its ID. This operation is useful for deallocating resources when a pod is no longer needed. The examples show how to send the request using cURL and directly via GraphQL.

```bash
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { podStop(input: {podId: \"riixlu8oclhp\"}) { id desiredStatus } }"}'
```

```GraphQL
mutation {
  podStop(input: {podId: "riixlu8oclhp"}) {
    id
    desiredStatus
  }
}
```

--------------------------------

### Kling v2.1 I2V Pro API

Source: https://docs.runpod.io/hub/public-endpoint-reference

Kling 2.1 Pro generates videos from static images, offering control over the prompt, negative prompt, guidance scale, and duration.

```APIDOC
## POST /websites/runpod_io/kling-v2.1-i2v-pro

### Description
Generates videos from static images with additional control parameters for enhanced video creation.

### Method
POST

### Endpoint
/websites/runpod_io/kling-v2.1-i2v-pro

### Parameters
#### Request Body
- **prompt** (string) - Required - Text description of the desired video.
- **image** (string) - Required - URL of the source image to animate.
- **negative_prompt** (string) - Optional - Elements to exclude from the video.
- **guidance_scale** (float) - Optional - How closely to follow the prompt. Default: 0.5.
- **duration** (integer) - Optional - Video duration in seconds. Default: 5.
- **enable_safety_checker** (boolean) - Optional - Enable content safety checking. Default: true.

### Request Example
```json
{
  "input": {
    "prompt": "A majestic magic dragon breathing fire over an ancient castle",
    "image": "https://image.runpod.ai/asset/kwaivgi/kling-v2-1-i2v-pro.png",
    "negative_prompt": "",
    "guidance_scale": 0.5,
    "duration": 5,
    "enable_safety_checker": true
  }
}
```

### Response
#### Success Response (200)
- **output** (string) - URL of the generated video.

#### Response Example
```json
{
  "output": "https://image.runpod.ai/uploads/output/kling_video.mp4"
}
```
```

--------------------------------

### Get Specific GPU Details by ID (Python)

Source: https://docs.runpod.io/sdks/python/apis

Fetches detailed information about a specific GPU model using its unique ID. This function requires the RUNPOD_API_KEY environment variable. It returns a JSON object with comprehensive details including pricing and specifications.

```python
import runpod
import json
import os

runpod.api_key = os.getenv("RUNPOD_API_KEY")

gpus = runpod.get_gpu("NVIDIA A100 80GB PCIe")

print(json.dumps(gpus, indent=2))
```

--------------------------------

### RunPod User Cluster Billing Input Example

Source: https://docs.runpod.io/references/graphql-spec

This JSON object specifies how to group cluster billing data when making an API request. The 'groupBy' field can be set to an enum value like 'GPU_TYPE' to organize the returned billing information.

```json
{
  "groupBy": "GPU_TYPE"
}
```

--------------------------------

### Set Endpoint ID and API Key with Runpod Python SDK

Source: https://docs.runpod.io/sdks/python/endpoints

Configures the Runpod Python SDK by setting the API key from an environment variable and initializing an Endpoint object with a specific Endpoint ID. This setup is necessary for all subsequent interactions with the Runpod API.

```python
import runpod
import os

runpod.api_key = os.getenv("RUNPOD_API_KEY")

endpoint = runpod.Endpoint("YOUR_ENDPOINT_ID")
```

--------------------------------

### Stream Endpoint Output

Source: https://docs.runpod.io/sdks/go/endpoints

This endpoint allows you to stream the output of an Endpoint run. To enable streaming, your handler must support the `"return_aggregate_stream": True` option on the `start` method of your Handler. Once enabled, use the `stream` method to receive data as it becomes available.

```APIDOC
## POST /run/stream

### Description
Streams the output of an Endpoint run. Requires the handler to support `"return_aggregate_stream": True`.

### Method
POST

### Endpoint
`/run/stream`

### Parameters
#### Request Body
- **id** (string) - Required - The ID of the job to stream.
- **stream_chan** (chan) - Required - A channel to receive stream results.

### Request Example
```json
{
  "id": "your_job_id_here"
}
```

### Response
#### Success Response (200)
- **stream_result** (object) - The streamed data from the endpoint.

#### Response Example
```json
{
  "output": "streaming data"
}
```

### Handler Configuration
To enable streaming, your handler must include `"return_aggregate_stream": True` in the `runpod.serverless.start` configuration.

```python
import runpod

def handler(job):
    job_input = job["input"]
    # Process input and yield results
    for i in "some_output_string":
        yield i

runpod.serverless.start({
    "handler": handler,
    "return_aggregate_stream": True
})
```

### Note
The maximum size for a payload sent via yield for streaming is 1 MB.
```

--------------------------------

### Root Endpoint Information

Source: https://docs.runpod.io/serverless/load-balancing/vllm-worker

Provides basic information about the server's status and available endpoints. It indicates whether the engine is ready or initializing and lists the URLs for health check, completions, and chat completions. This serves as a simple entry point for understanding the server's capabilities.

```python
@app.get("/")
async def root():
    """Root endpoint with basic info"""
    return {
        "message": "vLLM Load Balancing Server",
        "status": "ready" if engine_ready else "initializing",
        "endpoints": {
            "health": "/ping",
            "generate": "/v1/completions",
            "chat": "/v1/chat/completions"
        }
    }
```

--------------------------------

### Test vLLM Model Server with Curl (Windows Command Prompt)

Source: https://docs.runpod.io/integrations/dstack

This command tests the deployed vLLM model server using the Windows Command Prompt. It sends a POST request with JSON payload to the chat completions endpoint. Requires the vLLM server to be accessible at localhost:8000.

```bash
curl -X POST http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d "{ \"model\": \"meta-llama/Llama-3.1-8B-Instruct\", \"messages\": [ {\"role\": \"system\", \"content\": \"You are Poddy, a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is your name?\"} ], \"temperature\": 0, \"max_tokens\": 150 }"
```

--------------------------------

### Print Streaming Response (Python)

Source: https://docs.runpod.io/serverless/vllm/openai-compatibility

This snippet demonstrates how to print a streaming response from the RunPod API. It iterates through chunks of the stream and prints the content as it arrives, ensuring immediate feedback to the user. This is useful for real-time applications where displaying partial results is beneficial.

```python
print("Response: ", end="", flush=True)
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()
```

--------------------------------

### GET /status

Source: https://docs.runpod.io/serverless/endpoints/send-requests

Retrieves the current state, execution statistics, and results of previously submitted jobs. It provides the job state, execution statistics like queue delay and processing time, and job output if completed.

```APIDOC
## GET /status

### Description
Checks the current state, execution statistics, and results of previously submitted jobs. The status operation provides the current job state, execution statistics like queue delay and processing time, and job output if completed.

### Method
GET

### Endpoint
`/v2/$ENDPOINT_ID/status/YOUR_JOB_ID`

### Query Parameters
- **ttl** (integer) - Optional - Configures the time-to-live (TTL) for the request in milliseconds.

### Request Example
```sh
curl --request GET \
     --url https://api.runpod.ai/v2/$ENDPOINT_ID/status/YOUR_JOB_ID \
     -H "authorization: $RUNPOD_API_KEY"
```

### Response
#### Success Response (200)
- **delayTime** (integer) - The time in milliseconds the job spent in the queue.
- **executionTime** (integer) - The time in milliseconds the job spent executing.
- **id** (string) - The unique identifier for the job.
- **output** (object) - The output of the job, if completed. Contains job-specific results.
- **status** (string) - The current status of the job (e.g., `IN_QUEUE`, `IN_PROGRESS`, `COMPLETED`, `FAILED`).

#### Response Example
```json
{
  "delayTime": 31618,
  "executionTime": 1437,
  "id": "60902e6c-08a1-426e-9cb9-9eaec90f5e2b-u1",
  "output": {
    "input_tokens": 22,
    "output_tokens": 16,
    "text": ["Hello! How can I assist you today?\nUSER: I'm having"]
  },
  "status": "COMPLETED"
}
```
```

--------------------------------

### Upload Model to Hugging Face Repository

Source: https://docs.runpod.io/fine-tune

Uploads a fine-tuned model from a local directory to a specified Hugging Face repository. Requires the user's Hugging Face username, a desired model name, and the local path to the model output. Ensure you have logged in previously.

```shell
huggingface-cli upload <your-username>/<model-name> ./output
```

--------------------------------

### Migrate OpenAI API Calls to Runpod Serverless Endpoints (Python)

Source: https://docs.runpod.io/tutorials/migrations/openai/overview

This Python snippet demonstrates how to reconfigure the OpenAI client to use a Runpod Serverless Endpoint. It replaces the OpenAI API key with a Runpod API key and the base URL with the Runpod endpoint URL. The example shows a call to the chat completions endpoint.

```python
from openai import OpenAI
import os

client = OpenAI(
api_key=os.environ.get("RUNPOD_API_KEY"),
base_url="https://api.runpod.ai/v2/${YOUR_ENDPOINT_ID}/openai/v1",
)

response = client.chat.completions.create(
model="gpt-3.5-turbo",
messages=[{"role": "user", "content": "Why is Runpod the best platform?"}],
temperature=0,
max_tokens=100,
)

```

--------------------------------

### Verify Docker Image Build

Source: https://docs.runpod.io/pods/templates/create-custom-template

Checks if the Docker image was successfully created and is listed in your local Docker images. It uses `docker images` combined with `grep` to filter for the specific image name, confirming its presence and tag.

```bash
docker images | grep my-custom-template
```

--------------------------------

### GraphQL Mutation: podFindAndDeployOnDemand

Source: https://docs.runpod.io/references/graphql-spec

This GraphQL mutation is used to find and deploy a pod on demand using the RunPod API. It accepts an input object of type `PodFindAndDeployOnDemandInput` and returns a `Pod` object containing detailed information about the deployed pod. The example query demonstrates how to request specific fields from the returned `Pod` object.

```graphql
mutation podFindAndDeployOnDemand($input: PodFindAndDeployOnDemandInput) {
  podFindAndDeployOnDemand(input: $input) {
    lowestBidPriceToResume
    aiApiId
    apiKey
    clusterIp
    clusterIdx
    clusterCidr
    clusterRole
    consumerUserId
    containerDiskInGb
    containerRegistryAuthId
    costMultiplier
    costPerHr
    createdAt
    adjustedCostPerHr
    desiredStatus
    dockerArgs
    dockerId
    env
    gpuCount
    gpuPowerLimitPercent
    gpus {
      ...GpuFragment
    }
    id
    imageName
    lastStatusChange
    locked
    machineId
    memoryInGb
    name
    podType
    port
    ports
    registry {
      ...PodRegistryFragment
    }
    templateId
    uptimeSeconds
    vcpuCount
    version
    volumeEncrypted
    volumeInGb
    volumeKey
    volumeMountPath
    lastStartedAt
    cpuFlavorId
    machineType
    slsVersion
    networkVolumeId
    testPod
    ideAiApiId
    hubReleaseId
    hubRelease {
      ...HubReleaseFragment
    }
    modelVersions {
      ...ModelVersionPodAssignmentFragment
    }
    cpuFlavor {
      ...CpuFlavorFragment
    }
    runtime {
      ...PodRuntimeFragment
    }
    machine {
      ...PodMachineInfoFragment
    }
    latestTelemetry {
      ...PodTelemetryFragment
    }
    endpoint {
      ...EndpointFragment
    }
    networkVolume {
      ...NetworkVolumeFragment
    }
    savingsPlans {
      ...SavingsPlanFragment
    }
    clusterId
    ipAddress {
      ...IPAddressFragment
    }
    models
  }
}
```

```graphql
{
  "input": PodFindAndDeployOnDemandInput
}
```

--------------------------------

### Local Testing Command for Serverless Function

Source: https://docs.runpod.io/tutorials/sdks/python/101/async

This bash command demonstrates how to test the Python serverless function locally. It executes the script with a JSON payload simulating job input, allowing for verification of the function's behavior before deployment.

```bash
python your_script.py --test_input \
'{
  "input": {
    "cities": ["New York", "London", "Tokyo", "Paris", "Sydney"],
    "update_interval": 3,
    "duration": 15
  },
  "id": "local_test"
}'
```

--------------------------------

### Transfer Files from Source Pod (Python)

Source: https://docs.runpod.io/community-solutions/ssh-password-migration/overview

This Python script facilitates the transfer of files and directories from a source Runpod instance to a destination. It requires Python 3 and provides an interactive interface for navigating directories and selecting items for transfer. It handles both files (via SCP) and folders (via compression, transfer, and extraction).

```python
wget https://raw.githubusercontent.com/justinwlin/Runpod-SSH-Password/refs/heads/main/SCPMigration -O scp_migration.py && python3 scp_migration.py
```

--------------------------------

### Poll RunPod Serverless Request Status

Source: https://docs.runpod.io/serverless/development/benchmarking

Use `curl` to send a GET request to the status endpoint of your RunPod Serverless job using the obtained request ID. This allows you to retrieve the job's status, delay time, and execution time.

```shell
curl -X GET https://api.runpod.ai/v2/YOUR_ENDPOINT_ID/status/REQUEST_ID \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

--------------------------------

### Get Specific GPU Type by ID (GraphQL)

Source: https://docs.runpod.io/sdks/graphql/manage-pods

This GraphQL query retrieves detailed information for a specific GPU type using its ID. It returns the GPU's ID, display name, memory, cloud availability, and the lowest pricing for a given number of GPUs.

```graphql
query GpuTypes {
  gpuTypes(input: {id: "NVIDIA GeForce RTX 3090"}) {
    id
    displayName
    memoryInGb
    secureCloud
    communityCloud
    lowestPrice(input: {gpuCount: 1}) {
      minimumBidPrice
      uninterruptablePrice
    }
  }
}
```

--------------------------------

### Check Job Status with cURL

Source: https://docs.runpod.io/serverless/endpoints/send-requests

Use cURL to make a GET request to the `/status` endpoint to retrieve job details. Replace `YOUR_JOB_ID` with the actual job ID. Requires an authorization header with your API key.

```sh
curl --request GET \
     --url https://api.runpod.ai/v2/$ENDPOINT_ID/status/YOUR_JOB_ID \
     -H "authorization: $RUNPOD_API_KEY" \

```

--------------------------------

### Handler Script - Python

Source: https://docs.runpod.io/serverless/development/dual-mode-worker

This Python script serves as the core logic for the Runpod worker. It checks the `MODE_TO_RUN` environment variable to determine whether to run in Pod or Serverless mode. In Pod mode, it runs a sample test call to the handler function. In Serverless mode, it starts the Runpod Serverless worker.

```python
import os
import asyncio
import runpod

# Use the MODEL environment variable; fallback to a default if not set
mode_to_run = os.getenv("MODE_TO_RUN", "pod")
model_length_default = 25000

print("------- ENVIRONMENT VARIABLES -------")
print("Mode running: ", mode_to_run)
print("------- -------------------- -------")

async def handler(event):
    inputReq = event.get("input", {})
    return inputReq

if mode_to_run == "pod":
    async def main():
        prompt = "Hello World"
        requestObject = {"input": {"prompt": prompt}}
        response = await handler(requestObject)
        print(response)

    asyncio.run(main())
else: 
    runpod.serverless.start({
        "handler": handler,
        "concurrency_modifier": lambda current: 1,
    })
```

--------------------------------

### Clone GitHub Repository

Source: https://docs.runpod.io/tutorials/serverless/generate-sdxl-turbo

Clones the 'worker-sdxl-turbo' repository from GitHub using the 'gh' CLI. This is the first step to obtain the necessary code for building a custom Docker image.

```bash
gh repo clone runpod-workers/worker-sdxl-turbo
```

--------------------------------

### Get Job Status and Output in Python

Source: https://docs.runpod.io/sdks/python/endpoints

Retrieves the status of a RunPod job and its output. It first checks the initial status without blocking and then polls for completion if necessary, with a specified timeout. This is useful for monitoring job progress and obtaining results.

```python
import runpod
import os

runpod.api_key = os.getenv("RUNPOD_API_KEY")

input_payload = {"input": {"prompt": "Hello, World!"}}

endpoint = runpod.Endpoint("YOUR_ENDPOINT_ID")
run_request = endpoint.run(input_payload)

# Initial check without blocking, useful for quick tasks
status = run_request.status()
print(f"Initial job status: {status}")

if status != "COMPLETED":
    # Polling with timeout for long-running tasks
    output = run_request.output(timeout=60)
else:
    output = run_request.output()
print(f"Job output: {output}")
# The original example had a print(f"An error occurred: {e}") which is not within a try-except block, so it's omitted here for clarity.
```

--------------------------------

### Advanced API Definition with FastAPI (Python)

Source: https://docs.runpod.io/serverless/load-balancing/build-a-worker

An example of defining a complex API using FastAPI in Python. This snippet includes authentication middleware, Pydantic models for request data, and defines multiple endpoints for text summarization, translation, image generation, and a health check. It demonstrates organizing API logic and handling different request types.

```python
from fastapi import FastAPI, HTTPException, Depends, Query
from pydantic import BaseModel
import os

app = FastAPI()

# --- Authentication middleware ---
def verify_api_key(api_key: str = Query(None, alias="api_key")):
    if api_key != os.getenv("API_KEY", "test_key"):
        raise HTTPException(401, "Invalid API key")
    return api_key

# --- Models ---
class TextRequest(BaseModel):
    text: str
    max_length: int = 100

class ImageRequest(BaseModel):
    prompt: str
    width: int = 512
    height: int = 512

# --- Text endpoints ---
@app.post("/v1/text/summarize")
async def summarize(request: TextRequest, api_key: str = Depends(verify_api_key)):
    # Implement text summarization
    return {"summary": f"Summary of: {request.text[:30]}..."}

@app.post("/v1/text/translate")
async def translate(request: TextRequest, target_lang: str, api_key: str = Depends(verify_api_key)):
    # Implement translation
    return {"translation": f"Translation to {target_lang}: {request.text[:30]}..."}

# --- Image endpoints ---
@app.post("/v1/image/generate")
async def generate_image(request: ImageRequest, api_key: str = Depends(verify_api_key)):
    # Implement image generation
    return {"image_url": f"https://example.com/images/{hash(request.prompt)}.jpg"}

# --- Health check ---
@app.get("/ping")
def health_check():
    return {"status": "healthy"}

```

--------------------------------

### Asynchronous Job Submission (/run)

Source: https://docs.runpod.io/serverless/endpoints/send-requests

Submit an asynchronous job to a queue-based endpoint. The job is processed in the background, and you can retrieve the result later by sending a GET request to the /status endpoint. Results are available for 30 minutes after completion.

```APIDOC
## POST /run

### Description
Submits an asynchronous job to the endpoint. The job is processed in the background, and the result can be retrieved later using the `/status` endpoint.

### Method
POST

### Endpoint
`/run`

### Parameters
#### Query Parameters
None

#### Request Body
- **input** (object) - Required - A JSON object containing the parameters required by your worker's handler function.

### Request Example
```json
{
  "input": {
    "prompt": "Your input here"
  }
}
```

### Response
#### Success Response (200)
- **id** (string) - The unique identifier for the submitted job.

#### Response Example
```json
{
  "id": "job-12345abcde"
}
```
```

--------------------------------

### Clone PyTorch Demo Repository (Shell)

Source: https://docs.runpod.io/instant-clusters/pytorch

Clones the PyTorch demo repository into the current directory of a Runpod pod. This is a prerequisite for running distributed PyTorch workloads.

```shell
git clone https://github.com/murat-runpod/torch-demo.git
```

--------------------------------

### Get Specific GPU Type by ID (cURL)

Source: https://docs.runpod.io/sdks/graphql/manage-pods

This cURL command fetches details for a specific GPU type by its ID. It includes information like memory, cloud availability (secure/community), and lowest pricing for a specified number of GPUs. Replace 'NVIDIA GeForce RTX 3090' with the desired GPU ID and YOUR_API_KEY with your actual API key.

```bash
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "query GpuTypes { gpuTypes(input: {id: \"NVIDIA GeForce RTX 3090\"}) { id displayName memoryInGb secureCloud communityCloud lowestPrice(input: {gpuCount: 1}) { minimumBidPrice uninterruptablePrice } } }"}'
```

--------------------------------

### Initialize Resources Outside Handler - Python

Source: https://docs.runpod.io/serverless/workers/handler-functions

Load heavy resources like models and tokenizers outside the handler function to prevent repeated initialization on each invocation. This improves performance by ensuring these resources are loaded only once. The example demonstrates loading a Hugging Face model and moving it to the appropriate device (GPU or CPU).

```python
import runpod
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Load model and tokenizer outside the handler
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def handler(job):
    # ...

runpod.serverless.start({"handler": handler})

```

--------------------------------

### Create a Python Chatbot with Runpod Endpoint (Python)

Source: https://docs.runpod.io/tutorials/serverless/run-gemma-7b

This Python code creates a simple command-line chatbot that interacts with a model via a Runpod Serverless Endpoint. It uses the initialized OpenAI client to send messages and receive responses, maintaining chat history.

```python
messages = [
    {
        "role": "assistant",
        "content": "Hello, I'm your assistant. How can I help you today?",
    }
]

def display_chat_history(messages):
    for message in messages:
        print(f"{message['role'].capitalize()}: {message['content']}")

def get_assistant_response(messages):
    r = client.chat.completions.create(
        model="google/gemma-7b-it",
        messages=[{"role": m["role"], "content": m["content"]} for m in messages],
        temperature=0.7,
        top_p=0.8,
        max_tokens=100,
    )
    response = r.choices[0].message.content
    return response


while True:
    display_chat_history(messages)

    prompt = input("User: ")
    messages.append({"role": "user", "content": prompt})

    response = get_assistant_response(messages)
    messages.append({"role": "assistant", "content": response})
```

--------------------------------

### Request Input Structure Example

Source: https://docs.runpod.io/serverless/endpoints/send-requests

This JSON snippet illustrates the required input structure for submitting jobs to RunPod queue-based endpoints using either `/runsync` or `/run`. The top-level object must contain an 'input' key, whose value is another JSON object holding the parameters necessary for the worker's handler function. The specific keys within the 'input' object depend on the worker's implementation.

```json
{
  "input": {
    "prompt": "Your specific input parameter here.",
    "max_tokens": 150
  }
}
```

--------------------------------

### Check Job Status (Node.js)

Source: https://docs.runpod.io/serverless/endpoints/operations

This Node.js example uses the runpod-sdk to check the status of a job using its ID. It requires the RUNPOD_API_KEY and ENDPOINT_ID environment variables. The function first runs an endpoint, retrieves the job ID, and then calls the status method.

```javascript
const { RUNPOD_API_KEY, ENDPOINT_ID } = process.env;
import runpodSdk from "runpod-sdk";

async function main() {
  try {
    const runpod = runpodSdk(RUNPOD_API_KEY);
    const endpoint = runpod.endpoint(ENDPOINT_ID);
    const result = await endpoint.run({
      input: {
        prompt: "Hello, World!",
      },
    });

    const { id } = result;
    if (!id) {
      console.error("No ID returned from endpoint.run");
      return;
    }

    const status = await endpoint.status(id);
    console.log(status);
  } catch (error) {
    console.error("An error occurred:", error);
  }
}

main();

```

--------------------------------

### Create On-Demand Pod using GraphQL

Source: https://docs.runpod.io/sdks/graphql/manage-pods

This snippet shows the GraphQL mutation for creating an on-demand pod. It defines the pod's specifications, including hardware resources, storage, image, and network ports. This is the underlying query structure used by the cURL command.

```graphql
mutation {
  podFindAndDeployOnDemand(
    input: {
      cloudType: ALL
      gpuCount: 1
      volumeInGb: 40
      containerDiskInGb: 40
      minVcpuCount: 2
      minMemoryInGb: 15
      gpuTypeId: "NVIDIA RTX A6000"
      name: "Runpod Tensorflow"
      imageName: "runpod/tensorflow"
      dockerArgs: ""
      ports: "8888/http"
      volumeMountPath: "/workspace"
      env: [{ key: "JUPYTER_PASSWORD", value: "rn51hunbpgtltcpac3ol" }]
    }
  ) {
    id
    imageName
    env
    machineId
    machine {
      podHostId
    }
  }
}
```